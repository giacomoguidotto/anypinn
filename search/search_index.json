{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AnyPINN","text":"<p>Work in Progress \u2014 This project is under active development and APIs may change. If you run into any issues, please open an issue on GitHub.</p> <p>A modular Python library for solving differential equations with Physics-Informed Neural Networks.</p> <p>AnyPINN lets you go from zero to a running PINN experiment in seconds, or give you the full control to define custom physics, constraints, and training loops. You decide how deep to go.</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>The fastest way to start is the bootstrap CLI. It scaffolds a complete, runnable project interactively. Run it with uvx (ships with <code>uv</code>):</p> <pre><code>uvx anypinn create my-project\n</code></pre> <p>or with pipx:</p> <pre><code>pipx run anypinn create my-project\n</code></pre> <pre><code>? Choose a starting point:\n  &gt; SIR Epidemic Model\n    ...\n    Custom ODE\n    Blank project\n\n? Select training data source:\n  &gt; Generate synthetic data\n    Load from CSV\n\n? Include Lightning training wrapper? (Y/n)\n\nCreating my-project/\n  \u2713  pyproject.toml   project metadata &amp; dependencies\n  \u2713  ode.py           your ODE definition\n  \u2713  config.py        hyperparameters with sensible defaults\n  \u2713  train.py         ready-to-run training script\n  \u2713  data/            data directory\n\n  Done! Run:  cd my-project &amp;&amp; uv sync &amp;&amp; uv run train.py\n</code></pre> <p>All prompts are also available as flags to skip the interactive flow:</p> <pre><code>anypinn create my-project \\\n  --template sir \\\n  --data synthetic \\\n  --lightning\n</code></pre> Flag Values Description <code>--help, -h</code> \u2014 Show help and exit <code>--list-templates, -l</code> \u2014 Print all templates with descriptions and exit <code>--template, -t</code> built-in template name, <code>custom</code>, or <code>blank</code> Starting template <code>--data, -d</code> <code>synthetic</code>, <code>csv</code> Training data source <code>--lightning, -L</code> \u2014 Include PyTorch Lightning wrapper <code>--no-lightning, -NL</code> \u2014 Exclude PyTorch Lightning wrapper"},{"location":"#who-is-this-for","title":"\ud83d\udc65 Who Is This For?","text":"<p>AnyPINN is built around progressive complexity. Start simple, go deeper only when you need to.</p> User Goal How Experimenter Run a known problem, tweak parameters, see results Pick a built-in template, change config, press start Researcher Define new physics or custom constraints Subclass <code>Constraint</code> and <code>Problem</code>, use the provided training engine Framework builder Custom training loops, novel architectures Use <code>anypinn.core</code> directly \u2014 zero Lightning required"},{"location":"#examples","title":"\ud83d\udca1 Examples","text":"<p>The <code>examples/</code> directory has ready-made, self-contained scripts covering epidemic models, oscillators, predator-prey dynamics, and more \u2014 from a minimal ~80-line core-only script to full Lightning stacks. They're a great source of inspiration when defining your own problem.</p>"},{"location":"#defining-your-own-problem","title":"\ud83d\udd2c Defining Your Own Problem","text":"<p>If you want to go beyond the built-in templates, here is the full workflow for defining a custom ODE inverse problem.</p>"},{"location":"#1-define-the-ode","title":"1: Define the ODE","text":"<p>Implement a function matching the <code>ODECallable</code> protocol:</p> <pre><code>from torch import Tensor\nfrom anypinn.core import ArgsRegistry\n\ndef my_ode(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"Return dy/dx given current state y and position x.\"\"\"\n    k = args[\"k\"](x)        # learnable or fixed parameter\n    return -k * y           # simple exponential decay\n</code></pre>"},{"location":"#2-configure-hyperparameters","title":"2: Configure hyperparameters","text":"<pre><code>from dataclasses import dataclass\nfrom anypinn.problems import ODEHyperparameters\n\n@dataclass(frozen=True, kw_only=True)\nclass MyHyperparameters(ODEHyperparameters):\n    pde_weight: float = 1.0\n    ic_weight: float = 10.0\n    data_weight: float = 5.0\n</code></pre>"},{"location":"#3-build-the-problem","title":"3: Build the problem","text":"<pre><code>from anypinn.problems import ODEInverseProblem, ODEProperties\n\nprops = ODEProperties(ode=my_ode, args={\"k\": param}, y0=y0)\nproblem = ODEInverseProblem(\n    ode_props=props,\n    fields={\"u\": field},\n    params={\"k\": param},\n    hp=hp,\n)\n</code></pre>"},{"location":"#4-train","title":"4: Train","text":"<pre><code>import pytorch_lightning as pl\nfrom anypinn.lightning import PINNModule\n\n# With Lightning (batteries included)\nmodule = PINNModule(problem, hp)\ntrainer = pl.Trainer(max_epochs=50_000)\ntrainer.fit(module, datamodule=dm)\n\n# Or with your own training loop (core only, no Lightning)\noptimizer = torch.optim.Adam(problem.parameters(), lr=1e-3)\nfor batch in dataloader:\n    optimizer.zero_grad()\n    loss = problem.training_loss(batch, log=my_log_fn)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>AnyPINN is split into four layers with a strict dependency direction \u2014 outer layers depend on inner ones, never the reverse.</p> <pre><code>graph TD\n    EXP[\"Your Experiment / Generated Project\"]\n\n    EXP --&gt; CAT\n    EXP --&gt; LIT\n\n    subgraph CAT[\"anypinn.catalog\"]\n        direction LR\n        CA1[SIR / SEIR]\n        CA2[DampedOscillator]\n        CA3[LotkaVolterra]\n    end\n\n    subgraph LIT[\"anypinn.lightning (optional)\"]\n        direction LR\n        L1[PINNModule]\n        L2[Callbacks]\n        L3[PINNDataModule]\n    end\n\n    subgraph PROB[\"anypinn.problems\"]\n        direction LR\n        P1[ResidualsConstraint]\n        P2[ICConstraint]\n        P3[DataConstraint]\n        P4[ODEInverseProblem]\n    end\n\n    subgraph CORE[\"anypinn.core (standalone \u00b7 pure PyTorch)\"]\n        direction LR\n        C1[Problem \u00b7 Constraint]\n        C2[Field \u00b7 Parameter]\n        C3[Config \u00b7 Context]\n    end\n\n    CAT --&gt;|depends on| PROB\n    CAT --&gt;|depends on| CORE\n    LIT --&gt;|depends on| CORE\n    PROB --&gt;|depends on| CORE</code></pre>"},{"location":"#anypinncore-the-math-layer","title":"<code>anypinn.core</code> \u2014 The Math Layer","text":"<p>Pure PyTorch. Defines what a PINN problem is, with no opinions about training.</p> <ul> <li><code>Problem</code> \u2014 Aggregates constraints, fields, and parameters. Provides <code>training_loss()</code> and <code>predict()</code>.</li> <li><code>Constraint</code> (ABC) \u2014 A single loss term. Subclass it to express any physics equation, boundary condition, or data-matching objective.</li> <li><code>Field</code> \u2014 MLP mapping input coordinates to state variables (e.g., <code>t \u2192 [S, I, R]</code>).</li> <li><code>Parameter</code> \u2014 Learnable scalar or function-valued parameter (e.g., <code>\u03b2</code> in SIR).</li> <li><code>InferredContext</code> \u2014 Runtime domain bounds and validation references, extracted from data and injected into constraints automatically.</li> </ul>"},{"location":"#anypinnlightning-the-training-engine-optional","title":"<code>anypinn.lightning</code> \u2014 The Training Engine (optional)","text":"<p>A thin wrapper plugging a <code>Problem</code> into PyTorch Lightning:</p> <ul> <li><code>PINNModule</code> \u2014 <code>LightningModule</code> wrapping any <code>Problem</code>. Handles optimizer setup, context injection, and prediction.</li> <li><code>PINNDataModule</code> \u2014 Abstract data module managing loading, config-driven collocation sampling, and context creation. Collocation strategy is selected via <code>TrainingDataConfig.collocation_sampler</code> (<code>\"random\"</code>, <code>\"uniform\"</code>, <code>\"latin_hypercube\"</code>, <code>\"log_uniform_1d\"</code>, or <code>\"adaptive\"</code>).</li> <li>Callbacks \u2014 SMMA-based early stopping, formatted progress bars, data scaling, prediction writers.</li> </ul>"},{"location":"#anypinnproblems-ode-building-blocks","title":"<code>anypinn.problems</code> \u2014 ODE Building Blocks","text":"<p>Ready-made constraints for ODE inverse problems:</p> <ul> <li><code>ResidualsConstraint</code> \u2014 <code>\u2016dy/dt \u2212 f(t, y)\u2016\u00b2</code> via autograd</li> <li><code>ICConstraint</code> \u2014 <code>\u2016y(t\u2080) \u2212 y\u2080\u2016\u00b2</code></li> <li><code>DataConstraint</code> \u2014 <code>\u2016prediction \u2212 observed data\u2016\u00b2</code></li> <li><code>ODEInverseProblem</code> \u2014 Composes all three with configurable weights</li> </ul>"},{"location":"#anypinncatalog-problem-specific-building-blocks","title":"<code>anypinn.catalog</code> \u2014 Problem-Specific Building Blocks","text":"<p>Drop-in ODE functions and <code>DataModule</code>s for specific systems. See <code>anypinn/catalog/</code> for the full list.</p>"},{"location":"#tooling","title":"\ud83d\udee0\ufe0f Tooling","text":"Tool Purpose uv Dependency management just Task automation Ruff Linting and formatting pytest Testing ty Type checking <p>All common tasks (test, lint, format, type-check, docs) are available via <code>just</code>.</p> <p>devenv users: devenv redirects <code>uv sync</code> installs to <code>.devenv/state/venv</code> instead of the standard <code>.venv</code>, so ty cannot auto-discover it. Create a gitignored <code>ty.toml</code> at the project root with: <pre><code>[environment]\npython-version = \"3.13\"\npython = \"./.devenv/state/venv\"\nroot = [\"./src\"]\n</code></pre> (<code>ty.toml</code> takes full precedence over <code>pyproject.toml</code>, so all three settings are required.)</p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>See CONTRIBUTING.md for setup instructions, code style guidelines, and the pull request workflow.</p>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at @giacomoguidotto. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>Welcome! Contributions are warmly appreciated \u2014 bug reports, new problem templates, constraint types, documentation improvements, and more.</p> <p>By participating in this project you agree to abide by the Code of Conduct.</p>"},{"location":"CONTRIBUTING/#setup","title":"\ud83d\udee0\ufe0f Setup","text":"<p>Prerequisites: Python 3.11+, uv.</p> <pre><code>git clone https://github.com/your-org/anypinn\ncd anypinn\nuv sync\n</code></pre> <p>devenv users: devenv redirects <code>uv sync</code> installs to <code>.devenv/state/venv</code> instead of the standard <code>.venv</code>, so ty cannot auto-discover it. Create a gitignored <code>ty.toml</code> at the project root with: <pre><code>[environment]\npython-version = \"3.13\"\npython = \"./.devenv/state/venv\"\nroot = [\"./src\"]\n</code></pre> (<code>ty.toml</code> takes full precedence over <code>pyproject.toml</code>, so all three settings are required.)</p> <p>All common tasks are driven by <code>just</code>:</p> <pre><code>just test           # Run tests with coverage\njust lint           # Check code style\njust fmt            # Format code (isort + ruff)\njust lint-fix       # Auto-fix linting issues\njust check          # Type checking (ty)\njust docs-serve     # Serve docs locally\njust ci             # lint + check + test (full CI suite)\n</code></pre>"},{"location":"CONTRIBUTING/#workflow","title":"\ud83d\udd01 Workflow","text":"<ol> <li> <p>Fork the repository and create a branch for your change:    <pre><code>git checkout -b feat/my-feature\n</code></pre></p> </li> <li> <p>Make your changes, then verify everything passes:    <pre><code>just ci\n</code></pre></p> </li> <li> <p>Commit following Conventional Commits:    <pre><code>git commit -m \"feat: add Brusselator ODE template\"\n</code></pre></p> </li> </ol> Prefix Effect <code>fix:</code> Patch release (0.0.X) <code>feat:</code> Minor release (0.X.0) <code>feat!:</code> / <code>BREAKING CHANGE:</code> Major release (X.0.0) <ol> <li>Push and open a pull request.</li> </ol>"},{"location":"CONTRIBUTING/#code-style","title":"\u270d\ufe0f Code Style","text":"<ul> <li>Line length: 99</li> <li>Ruff linter with rules: F, E, I, N, UP, RUF, B, C4, ISC, PIE, PT, PTH, SIM, TID</li> <li>Absolute imports only \u2014 no relative imports</li> <li>All config dataclasses: <code>@dataclass(frozen=True, kw_only=True)</code></li> </ul>"},{"location":"CONTRIBUTING/#architecture-guidelines","title":"\ud83c\udfd7\ufe0f Architecture Guidelines","text":"<ul> <li>Keep the layer separation: <code>anypinn.core</code> stays pure PyTorch, Lightning stays optional.</li> <li><code>anypinn.core</code> must not import from <code>anypinn.lightning</code>, <code>anypinn.problems</code>, or <code>anypinn.catalog</code>.</li> <li>If you change the architecture or data flow, update both <code>CLAUDE.md</code> and <code>README.md</code>.</li> </ul>"},{"location":"rationale/","title":"Design Rationale &amp; Future Work","text":"<p>This document addresses two questions: why AnyPINN exists given the breadth of existing PINN libraries, and what remains to be built for the library to reach its full intended scope.</p>"},{"location":"rationale/#1-positioning-against-existing-libraries","title":"1. Positioning Against Existing Libraries","text":"<p>The Physics-Informed Neural Network ecosystem already contains a dozen libraries spanning two major frameworks. The question is therefore not \"does a gap exist?\" but \"which tradeoffs does each library make, and which tradeoffs are the right ones for which users?\".</p> <p>The libraries considered here include PyTorch-based tools (NeuroDiffEq, IDRLNet, NVIDIA Modulus, PINA) and TensorFlow-based tools (DeepXDE, TensorDiffEq, SciANN, PyDEns, Elvet, NVIDIA SimNet).</p>"},{"location":"rationale/#11-the-framework-ecosystem-problem","title":"1.1 The Framework Ecosystem Problem","text":"<p>More than half of the existing libraries \u2014 DeepXDE (in its original form), TensorDiffEq, SciANN, PyDEns, Elvet, and NVIDIA SimNet \u2014 are TensorFlow-based. This is a meaningful architectural liability. PyTorch has become the de-facto standard for ML research: its share of papers, tooling, and downstream ecosystem (Lightning, Hugging Face, einops, triton, <code>torch.compile</code>) dwarfs TensorFlow's in the research space. Directing a researcher to a TensorFlow-based PINN library in 2025 adds framework friction that has nothing to do with the science.</p> <p>AnyPINN is pure PyTorch from the ground up. Its core primitives (<code>Field</code>, <code>Parameter</code>, <code>Problem</code>, <code>Constraint</code>) are all <code>nn.Module</code> subclasses. This means the library participates naturally in the entire PyTorch ecosystem: mixed precision, <code>torch.compile</code>, distributed data parallel, gradient checkpointing \u2014 none of these require library-level support because the library never wraps the training loop.</p>"},{"location":"rationale/#12-the-training-engine-coupling-problem","title":"1.2 The Training Engine Coupling Problem","text":"<p>Every major competitor couples physics problem definition to its own training loop:</p> Library Coupling mechanism NeuroDiffEq <code>solve_ivp</code>-style API owns training DeepXDE <code>Model.train()</code> is not optional IDRLNet Computational graph node system owns training NVIDIA Modulus Full platform: owns Trainer, distributed, logging PINA Own <code>Trainer</code> wrapping Lightning <p>The consequence is that when the training ecosystem evolves \u2014 new optimizers, new distributed strategies, new precision formats \u2014 users must wait for the library to support them or write wrappers around the library's own abstractions.</p> <p>AnyPINN takes a different position: training is not the library's business. <code>Problem</code> is an <code>nn.Module</code>. <code>Problem.training_loss(batch, log)</code> takes a batch and returns a scalar tensor. The contract ends there. A minimal training loop looks exactly like any other PyTorch training loop:</p> <pre><code>optimizer = torch.optim.Adam(problem.parameters(), lr=1e-3)\nfor batch in dataloader:\n    optimizer.zero_grad()\n    loss = problem.training_loss(batch, log=my_log_fn)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>PyTorch Lightning is available as an optional layer (<code>anypinn.lightning</code>) for users who want batteries-included training. It is not required. This is the right default for a research library: researchers frequently need non-standard training procedures, and a library that owns the training loop forces workarounds.</p>"},{"location":"rationale/#13-inverse-problems-as-a-first-class-abstraction","title":"1.3 Inverse Problems as a First-Class Abstraction","text":"<p>Most PINN libraries were designed for forward problems: given a known PDE and boundary or initial conditions, find the solution field <code>u(x, t)</code>. Inverse problems \u2014 recover unknown parameters from partial observations \u2014 are typically supported as extensions or workarounds.</p> <p>AnyPINN treats the inverse problem as the primary use case. This is reflected throughout the type system:</p> <p><code>Parameter</code> is a top-level type, not a configuration flag. It can represent a learned scalar (e.g., a fixed transmission rate <code>\u03b2</code>) or a function-valued learnable parameter (e.g., a time-varying <code>\u03b2(t)</code> backed by a small MLP). Both expose the same <code>forward(x)</code> interface and integrate transparently into the <code>ArgsRegistry</code>, so the ODE function cannot distinguish between a fixed argument and a learnable one. Promoting a fixed parameter to a learnable one requires changing one config line, not restructuring the problem definition.</p> <p><code>ValidationRegistry</code> provides ground-truth tracking for recovered parameters. A CSV column can be bound to a parameter name at construction time. The library then logs the MSE between the recovered parameter and the known ground truth at every training step. This workflow \u2014 recovering parameters and continuously comparing them against a known reference \u2014 is central to inverse problems and is not present as a first-class feature in any of the listed alternatives.</p> <p><code>ArgsRegistry</code> unifies fixed and learnable arguments. The ODE callable receives an <code>ArgsRegistry</code> \u2014 a typed dict of <code>Argument</code> instances. <code>Parameter</code> inherits from <code>Argument</code>, so the same ODE function works whether the parameters are fixed or learned. This composability is absent from libraries that distinguish the two at the problem-definition level.</p>"},{"location":"rationale/#14-progressive-complexity-and-the-three-audience-model","title":"1.4 Progressive Complexity and the Three-Audience Model","text":"<p>Research libraries tend to optimize for one audience. Modulus targets HPC engineers running 3D CFD. NeuroDiffEq targets researchers who want a declarative ODE/PDE API. Neither serves the user who wants to start with a known problem and progressively take control of more layers.</p> <p>AnyPINN is explicitly designed for three audiences, and the architecture enforces the separation:</p> <pre><code>anypinn.catalog      \u2190 Experimenter: pick a known problem, change config, run\nanypinn.problems     \u2190 Researcher: define new physics, use provided constraints\nanypinn.core         \u2190 Framework builder: skip Lightning, own your training loop\nanypinn.lightning    \u2190 Optional at every level\n</code></pre> <p>The strict dependency direction (<code>catalog \u2192 problems \u2192 core</code>, <code>lightning \u2192 core</code>) ensures that each layer is usable without the ones above it. A user who only needs <code>anypinn.core</code> takes no dependency on Lightning, no dependency on problem-specific constraint implementations, and no dependency on the catalog. The core is 193 lines of <code>problem.py</code> and can be understood in a single reading.</p>"},{"location":"rationale/#15-developer-experience-as-a-first-class-concern","title":"1.5 Developer Experience as a First-Class Concern","text":"<p>Research libraries are routinely distributed in a state that would be unacceptable in production software: <code>assert</code> statements for validation, positional-argument constructors, no config validation, no type stubs. These are not cosmetic complaints \u2014 they produce silent failures, confusing errors, and codebases that are hard to extend.</p> <p>AnyPINN addresses this with:</p> <ul> <li>Typed frozen dataclasses for all configuration. Every hyperparameter is a <code>@dataclass(frozen=True, kw_only=True)</code>. Configs are immutable, keyword-only, and introspectable. This is in contrast to libraries like NeuroDiffEq that pass hyperparameters as positional arguments, or DeepXDE that uses a mix of constructor arguments and setter methods.</li> <li>Static type checking with <code>ty</code> in CI. Type errors are caught before runtime.</li> <li>Semantic versioning with automated releases. Conventional Commits trigger patch/minor/major releases automatically. Users can pin versions with confidence.</li> <li>Modern packaging. <code>uv</code> for dependency management, <code>hatchling</code> with dynamic versioning from VCS tags, Ruff for linting and formatting.</li> </ul>"},{"location":"rationale/#16-the-bootstrap-cli","title":"1.6 The Bootstrap CLI","text":"<p>No library in the comparison set ships a project scaffolding tool. Every user faces the same cold-start problem: before writing a single line of physics, they must learn the library API, set up a project structure, wire together data loading, training, and logging.</p> <p><code>anypinn create my-project</code> resolves this:</p> <pre><code>$ anypinn create my-project\n\n\u25c7  Choose a starting point\n\u2502  SIR Epidemic Model\n\n\u25c7  Select training data source\n\u2502  Generate synthetic data\n\n\u25c7  Include Lightning training wrapper?\n\u2502  Yes\n\n\u25c7  Creating my-project/...\n\u2502  pyproject.toml   \u2014 project dependencies\n\u2502  ode.py           \u2014 mathematical definition\n\u2502  config.py        \u2014 training configuration\n\u2502  train.py         \u2014 execution script\n\u2502  data/            \u2014 data directory\n\n\u25cf  Done! cd my-project &amp;&amp; uv sync &amp;&amp; uv run train.py\n</code></pre> <p>The generated project is fully runnable. The researcher's first meaningful action is reading and modifying <code>ode.py</code> \u2014 not reading library documentation to understand what arguments <code>PINNDataModule</code> needs. The <code>--lightning / --no-lightning</code> flag is particularly valuable: it generates a training script matched to the user's chosen layer, teaching the two-layer architecture by letting the user choose.</p>"},{"location":"rationale/#17-honest-scope-limitations","title":"1.7 Honest Scope Limitations","text":"<p>The argument above holds within a specific scope. AnyPINN is not the right choice for:</p> <ul> <li>Complex PDE problems (heat, wave, Navier-Stokes). The PDE foundation is in place:   multi-dimensional <code>Domain</code>, boundary condition constraints (<code>DirichletBCConstraint</code>,   <code>NeumannBCConstraint</code>), interior residual constraints with field-subset scoping   (<code>PDEResidualConstraint</code>), collocation samplers (uniform grid, random, Latin hypercube, adaptive   residual-based), spatial encodings (<code>FourierEncoding</code>, <code>RandomFourierFeatures</code>), and composable   differential operators (<code>grad</code>, <code>laplacian</code>, <code>hessian</code>, <code>divergence</code>). DeepXDE and PINA remain   more battle-tested for complex 3D PDE problems.</li> <li>Large-scale 3D simulations on GPU clusters. NVIDIA Modulus is purpose-built for this and   has no peer in that space.</li> <li>Users already productive in TensorFlow. Switching frameworks for a library is rarely   justified unless the problem demands it.</li> </ul> <p>The library's primary justification is the ODE inverse problem workflow: recovering parameters from partial observations of a dynamical system (epidemiological, mechanical, ecological). This is where the <code>Parameter</code> / <code>ValidationRegistry</code> / <code>ArgsRegistry</code> design is most directly valuable and most clearly unmatched by existing alternatives.</p>"},{"location":"rationale/#2-scope-and-capabilities","title":"2. Scope and Capabilities","text":"<p>This section describes what AnyPINN can do, grouped by layer.</p>"},{"location":"rationale/#ode-inverse-problems","title":"ODE inverse problems","text":"<p>The primary use case. <code>ODEInverseProblem</code> composes three constraint types \u2014 residual, initial condition, and data matching \u2014 over arbitrary ODE systems:</p> <ul> <li>Arbitrary-order residual constraints. <code>ODEProperties.order</code> specifies the ODE order (default   1). <code>ResidualsConstraint</code> chains autograd at each derivative level up to <code>order</code>, comparing the   highest derivative against the ODE callable's output. First-order callables work unchanged.</li> <li>Native higher-order IC enforcement. <code>ODEProperties.dy0</code> holds initial conditions for each   lower-order derivative. <code>ICConstraint</code> enforces all of them at <code>t0</code> via chained autograd.</li> <li>Configurable loss criterion. <code>PINNHyperparameters.criterion</code> selects MSE, Huber, or L1 loss;   <code>ODEInverseProblem</code> uses this instead of a hardcoded <code>nn.MSELoss</code>.</li> <li>Learnable scalar and function-valued parameters. <code>Parameter</code> exposes a <code>forward(x)</code> interface   regardless of whether it backs a scalar or an MLP. Fixed and learnable arguments are   indistinguishable from the ODE callable's perspective.</li> <li><code>ValidationRegistry</code> for ground-truth tracking. CSV columns can be bound to parameter names   at construction time; the library logs MSE against known ground truth every training step.</li> </ul>"},{"location":"rationale/#pde-problems","title":"PDE problems","text":"<ul> <li>Multi-dimensional <code>Domain</code>. <code>InferredContext</code> infers N-dimensional domain bounds and step   sizes from training data. Collocation samplers produce <code>(M, d)</code> tensors for any <code>d</code>.</li> <li>Boundary condition constraints. <code>DirichletBCConstraint</code> enforces <code>u = g</code> on the boundary;   <code>NeumannBCConstraint</code> enforces <code>du/dn = h</code> using the <code>grad</code> operator from <code>anypinn.lib.diff</code>.</li> <li><code>PDEResidualConstraint</code> with field-subset scoping. Each constraint receives only the fields   and parameters it needs; coupled systems (e.g. velocity + pressure) are expressed naturally by   passing different sub-registries to each constraint.</li> <li>Composable differential operators. <code>anypinn.lib.diff</code> provides <code>grad</code>, <code>partial</code>,   <code>mixed_partial</code>, <code>laplacian</code>, <code>divergence</code>, and <code>hessian</code> \u2014 all built on <code>torch.autograd.grad</code>   and usable directly inside any constraint or ODE callable.</li> </ul>"},{"location":"rationale/#collocation","title":"Collocation","text":"<p>Five built-in samplers, selected via the <code>collocation_sampler</code> string literal in <code>TrainingDataConfig</code>: <code>\"uniform\"</code>, <code>\"random\"</code>, <code>\"latin_hypercube\"</code>, <code>\"log_uniform\"</code> (preserves log-space density for epidemic models), and <code>\"adaptive\"</code> (residual-weighted sampling with configurable exploration ratio via <code>AdaptiveCollocationCallback</code>).</p>"},{"location":"rationale/#input-encodings","title":"Input encodings","text":"<p><code>FourierEncoding</code> and <code>RandomFourierFeatures</code> in <code>anypinn.lib.encodings</code> are <code>nn.Module</code> subclasses that participate in <code>.parameters()</code>, <code>.state_dict()</code>, and device transfers. They are passed as the <code>encode</code> argument to <code>MLPConfig</code> and lift low-frequency MLPs to high-frequency solutions without changing the training loop.</p>"},{"location":"rationale/#training","title":"Training","text":"<p>Adam and L-BFGS optimizers; <code>ReduceLROnPlateau</code> and <code>CosineAnnealing</code> schedulers; SMMA-based early stopping with configurable lookback window. All selected via typed frozen dataclasses in <code>PINNHyperparameters</code>. The optional Lightning wrapper (<code>anypinn.lightning</code>) adds a <code>PINNModule</code> for users who want Lightning's training infrastructure; the core library never requires it.</p>"},{"location":"rationale/#summary","title":"Summary","text":"<p>AnyPINN occupies a specific niche that is genuinely unoccupied by existing libraries: a PyTorch-native, training-engine-agnostic library where ODE inverse problems are expressed as composable <code>nn.Module</code> objects, with a bootstrapper that eliminates cold-start friction and a typed configuration system that makes misconfiguration a compile-time error rather than a runtime one.</p> <p>The library's justification is strongest for researchers working on parameter recovery in dynamical systems \u2014 epidemiological models, mechanical oscillators, predator-prey dynamics \u2014 who want to define physics in PyTorch terms and bring their own training infrastructure. The PDE foundation and ODE ergonomics (arbitrary-order constraints, native derivative IC enforcement, configurable criteria, spatial encodings, coupled-system scoping, and adaptive collocation) are all in place, providing a complete platform for the stated scope.</p>"},{"location":"reference/anypinn/","title":"anypinn","text":""},{"location":"reference/anypinn/#anypinn","title":"<code>anypinn</code>","text":"<p>AnyPINN: Physics-Informed Neural Networks library.</p>"},{"location":"reference/anypinn/#anypinn.__version__","title":"<code>__version__ = version('anypinn')</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/","title":"catalog","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog","title":"<code>anypinn.catalog</code>","text":"<p>Catalog of ready-made ODE/PDE problem building blocks.</p>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.AC_U_KEY","title":"<code>AC_U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.ALPHA_KEY","title":"<code>ALPHA_KEY = 'alpha'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.BETA_KEY","title":"<code>BETA_KEY = 'beta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.BURGERS_NU_KEY","title":"<code>BURGERS_NU_KEY = 'nu'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.BURGERS_U_KEY","title":"<code>BURGERS_U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.DELTA_KEY","title":"<code>DELTA_KEY = 'delta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.DIFFUSIVITY_D_KEY","title":"<code>DIFFUSIVITY_D_KEY = 'D'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.DIFFUSIVITY_U_KEY","title":"<code>DIFFUSIVITY_U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.E_KEY","title":"<code>E_KEY = 'E'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.FHN_A_KEY","title":"<code>FHN_A_KEY = 'a'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.FHN_EPSILON_KEY","title":"<code>FHN_EPSILON_KEY = 'epsilon'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.FHN_V_KEY","title":"<code>FHN_V_KEY = 'v'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.FHN_W_KEY","title":"<code>FHN_W_KEY = 'w'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.GAMMA_KEY","title":"<code>GAMMA_KEY = 'gamma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.GS_DU_KEY","title":"<code>GS_DU_KEY = 'D_u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.GS_DV_KEY","title":"<code>GS_DV_KEY = 'D_v'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.GS_F_KEY","title":"<code>GS_F_KEY = 'F'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.GS_K_KEY","title":"<code>GS_K_KEY = 'k'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.GS_U_KEY","title":"<code>GS_U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.GS_V_KEY","title":"<code>GS_V_KEY = 'v'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.HEAT_ALPHA_KEY","title":"<code>HEAT_ALPHA_KEY = 'alpha'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.HEAT_U_KEY","title":"<code>HEAT_U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.LORENZ_BETA_KEY","title":"<code>LORENZ_BETA_KEY = 'beta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.LORENZ_RHO_KEY","title":"<code>LORENZ_RHO_KEY = 'rho'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.LORENZ_SIGMA_KEY","title":"<code>LORENZ_SIGMA_KEY = 'sigma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.LORENZ_X_KEY","title":"<code>LORENZ_X_KEY = 'x'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.LORENZ_Y_KEY","title":"<code>LORENZ_Y_KEY = 'y'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.LV_X_KEY","title":"<code>LV_X_KEY = 'x'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.MU_KEY","title":"<code>MU_KEY = 'mu'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.N_KEY","title":"<code>N_KEY = 'N'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.OMEGA_KEY","title":"<code>OMEGA_KEY = 'omega0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.POISSON_U_KEY","title":"<code>POISSON_U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.Rt_KEY","title":"<code>Rt_KEY = 'Rt'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.SEIR_BETA_KEY","title":"<code>SEIR_BETA_KEY = 'beta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.SEIR_GAMMA_KEY","title":"<code>SEIR_GAMMA_KEY = 'gamma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.SEIR_I_KEY","title":"<code>SEIR_I_KEY = 'I'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.SEIR_S_KEY","title":"<code>SEIR_S_KEY = 'S'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.SIGMA_KEY","title":"<code>SIGMA_KEY = 'sigma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.SIR_BETA_KEY","title":"<code>SIR_BETA_KEY = 'beta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.SIR_DELTA_KEY","title":"<code>SIR_DELTA_KEY = 'delta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.SIR_I_KEY","title":"<code>SIR_I_KEY = 'I'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.SIR_S_KEY","title":"<code>SIR_S_KEY = 'S'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.U_KEY","title":"<code>U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.V_KEY","title":"<code>V_KEY = 'v'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.WAVE_C_KEY","title":"<code>WAVE_C_KEY = 'c'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.WAVE_U_KEY","title":"<code>WAVE_U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.X_KEY","title":"<code>X_KEY = 'x'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.Y_KEY","title":"<code>Y_KEY = 'y'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.ZETA_KEY","title":"<code>ZETA_KEY = 'zeta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.Z_KEY","title":"<code>Z_KEY = 'z'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.__all__","title":"<code>__all__ = ['AC_U_KEY', 'ALPHA_KEY', 'BETA_KEY', 'BURGERS_NU_KEY', 'BURGERS_U_KEY', 'DELTA_KEY', 'DIFFUSIVITY_D_KEY', 'DIFFUSIVITY_U_KEY', 'E_KEY', 'FHN_A_KEY', 'FHN_EPSILON_KEY', 'FHN_V_KEY', 'FHN_W_KEY', 'GAMMA_KEY', 'GS_DU_KEY', 'GS_DV_KEY', 'GS_F_KEY', 'GS_K_KEY', 'GS_U_KEY', 'GS_V_KEY', 'HEAT_ALPHA_KEY', 'HEAT_U_KEY', 'LORENZ_BETA_KEY', 'LORENZ_RHO_KEY', 'LORENZ_SIGMA_KEY', 'LORENZ_X_KEY', 'LORENZ_Y_KEY', 'LV_X_KEY', 'MU_KEY', 'N_KEY', 'OMEGA_KEY', 'POISSON_U_KEY', 'SEIR_BETA_KEY', 'SEIR_GAMMA_KEY', 'SEIR_I_KEY', 'SEIR_S_KEY', 'SIGMA_KEY', 'SIR', 'SIR_BETA_KEY', 'SIR_DELTA_KEY', 'SIR_I_KEY', 'SIR_S_KEY', 'U_KEY', 'V_KEY', 'WAVE_C_KEY', 'WAVE_U_KEY', 'X_KEY', 'Y_KEY', 'ZETA_KEY', 'Z_KEY', 'AllenCahnDataModule', 'Burgers1DDataModule', 'DampedOscillatorDataModule', 'FitzHughNagumoDataModule', 'GrayScott2DDataModule', 'Heat1DDataModule', 'InverseDiffusivityDataModule', 'LorenzDataModule', 'LotkaVolterraDataModule', 'Poisson2DDataModule', 'Rt_KEY', 'SEIRDataModule', 'SIRInvDataModule', 'VanDerPolDataModule', 'Wave1DDataModule', 'rSIR']</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.AllenCahnDataModule","title":"<code>AllenCahnDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for 1D Allen-Cahn equation.</p> <p>gen_data produces ground-truth u(x,t) via scipy method-of-lines (central differences for d2u/dx2 with periodic ghost cells + ODE integration). The data is used for prediction/validation only \u2014 training uses PDEResidualConstraint + PeriodicBCConstraint + IC (no DataConstraint).</p> Source code in <code>src/anypinn/catalog/allen_cahn.py</code> <pre><code>class AllenCahnDataModule(PINNDataModule):\n    \"\"\"DataModule for 1D Allen-Cahn equation.\n\n    gen_data produces ground-truth u(x,t) via scipy method-of-lines\n    (central differences for d2u/dx2 with periodic ghost cells + ODE integration).\n    The data is used for prediction/validation only \u2014 training uses\n    PDEResidualConstraint + PeriodicBCConstraint + IC (no DataConstraint).\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        true_epsilon: float = TRUE_EPSILON,\n        n_x: int = 256,\n        n_t: int = 200,\n        grid_size: int = 50,\n        residual_scorer: ResidualScorer | None = None,\n        validation: ValidationRegistry | None = None,\n        callbacks: list[DataCallback] | None = None,\n    ):\n        self.true_epsilon = true_epsilon\n        self.n_x = n_x\n        self.n_t = n_t\n        self.grid_size = grid_size\n        super().__init__(hp, validation, callbacks, residual_scorer=residual_scorer)\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate numerical solution on a 2D meshgrid via method of lines.\"\"\"\n        import numpy as np\n        from scipy.integrate import solve_ivp\n        from scipy.interpolate import RegularGridInterpolator\n\n        n_x = self.n_x\n        eps = self.true_epsilon\n\n        # Periodic domain [-1, 1): n_x interior points, dx = 2/n_x\n        x_fd = np.linspace(-1, 1, n_x, endpoint=False)\n        dx = x_fd[1] - x_fd[0]\n\n        # IC: u(x,0) = -tanh(x / (sqrt(2*eps)))\n        scale = math.sqrt(2 * eps)\n        u0 = -np.tanh(x_fd / scale)\n\n        t_span = (0.0, 1.0)\n        t_eval = np.linspace(0, 1, self.n_t)\n\n        def rhs(_t: float, u: np.ndarray) -&gt; np.ndarray:\n            \"\"\"RHS: du/dt = eps*d2u/dx2 + u - u^3 with periodic BCs.\"\"\"\n            # Periodic padding\n            u_pad = np.empty(n_x + 2)\n            u_pad[1:-1] = u\n            u_pad[0] = u[-1]  # left ghost = rightmost interior\n            u_pad[-1] = u[0]  # right ghost = leftmost interior\n\n            # Central differences for d2u/dx2\n            d2u = (u_pad[2:] - 2 * u_pad[1:-1] + u_pad[:-2]) / dx**2\n\n            return eps * d2u + u - u**3\n\n        sol = solve_ivp(rhs, t_span, u0, t_eval=t_eval, method=\"RK45\", max_step=0.001)\n\n        # sol.y has shape (n_x, n_t)\n        x_sol = x_fd\n        t_sol = sol.t\n\n        interp = RegularGridInterpolator(\n            (x_sol, t_sol), sol.y, method=\"linear\", bounds_error=False, fill_value=None\n        )\n\n        # Output meshgrid\n        xs = torch.linspace(-1, 1, self.grid_size)\n        ts = torch.linspace(0, 1, self.grid_size)\n        grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n\n        x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n        pts = np.stack([x_grid[:, 0].numpy(), x_grid[:, 1].numpy()], axis=1)\n        u_ref = torch.tensor(interp(pts), dtype=torch.float32)\n\n        # Shape: (N, 1, 1) to match codebase convention\n        y_data = u_ref.unsqueeze(-1).unsqueeze(1)\n\n        return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.AllenCahnDataModule.grid_size","title":"<code>grid_size = grid_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.AllenCahnDataModule.n_t","title":"<code>n_t = n_t</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.AllenCahnDataModule.n_x","title":"<code>n_x = n_x</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.AllenCahnDataModule.true_epsilon","title":"<code>true_epsilon = true_epsilon</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.AllenCahnDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, true_epsilon: float = TRUE_EPSILON, n_x: int = 256, n_t: int = 200, grid_size: int = 50, residual_scorer: ResidualScorer | None = None, validation: ValidationRegistry | None = None, callbacks: list[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/allen_cahn.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    true_epsilon: float = TRUE_EPSILON,\n    n_x: int = 256,\n    n_t: int = 200,\n    grid_size: int = 50,\n    residual_scorer: ResidualScorer | None = None,\n    validation: ValidationRegistry | None = None,\n    callbacks: list[DataCallback] | None = None,\n):\n    self.true_epsilon = true_epsilon\n    self.n_x = n_x\n    self.n_t = n_t\n    self.grid_size = grid_size\n    super().__init__(hp, validation, callbacks, residual_scorer=residual_scorer)\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.AllenCahnDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate numerical solution on a 2D meshgrid via method of lines.</p> Source code in <code>src/anypinn/catalog/allen_cahn.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate numerical solution on a 2D meshgrid via method of lines.\"\"\"\n    import numpy as np\n    from scipy.integrate import solve_ivp\n    from scipy.interpolate import RegularGridInterpolator\n\n    n_x = self.n_x\n    eps = self.true_epsilon\n\n    # Periodic domain [-1, 1): n_x interior points, dx = 2/n_x\n    x_fd = np.linspace(-1, 1, n_x, endpoint=False)\n    dx = x_fd[1] - x_fd[0]\n\n    # IC: u(x,0) = -tanh(x / (sqrt(2*eps)))\n    scale = math.sqrt(2 * eps)\n    u0 = -np.tanh(x_fd / scale)\n\n    t_span = (0.0, 1.0)\n    t_eval = np.linspace(0, 1, self.n_t)\n\n    def rhs(_t: float, u: np.ndarray) -&gt; np.ndarray:\n        \"\"\"RHS: du/dt = eps*d2u/dx2 + u - u^3 with periodic BCs.\"\"\"\n        # Periodic padding\n        u_pad = np.empty(n_x + 2)\n        u_pad[1:-1] = u\n        u_pad[0] = u[-1]  # left ghost = rightmost interior\n        u_pad[-1] = u[0]  # right ghost = leftmost interior\n\n        # Central differences for d2u/dx2\n        d2u = (u_pad[2:] - 2 * u_pad[1:-1] + u_pad[:-2]) / dx**2\n\n        return eps * d2u + u - u**3\n\n    sol = solve_ivp(rhs, t_span, u0, t_eval=t_eval, method=\"RK45\", max_step=0.001)\n\n    # sol.y has shape (n_x, n_t)\n    x_sol = x_fd\n    t_sol = sol.t\n\n    interp = RegularGridInterpolator(\n        (x_sol, t_sol), sol.y, method=\"linear\", bounds_error=False, fill_value=None\n    )\n\n    # Output meshgrid\n    xs = torch.linspace(-1, 1, self.grid_size)\n    ts = torch.linspace(0, 1, self.grid_size)\n    grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n\n    x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n    pts = np.stack([x_grid[:, 0].numpy(), x_grid[:, 1].numpy()], axis=1)\n    u_ref = torch.tensor(interp(pts), dtype=torch.float32)\n\n    # Shape: (N, 1, 1) to match codebase convention\n    y_data = u_ref.unsqueeze(-1).unsqueeze(1)\n\n    return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.Burgers1DDataModule","title":"<code>Burgers1DDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for 1D Burgers equation inverse problem.</p> <p>gen_data produces ground-truth u(x,t) via scipy method-of-lines (finite-difference spatial discretization + ODE integration), with optional measurement noise.</p> Source code in <code>src/anypinn/catalog/burgers_1d.py</code> <pre><code>class Burgers1DDataModule(PINNDataModule):\n    \"\"\"DataModule for 1D Burgers equation inverse problem.\n\n    gen_data produces ground-truth u(x,t) via scipy method-of-lines\n    (finite-difference spatial discretization + ODE integration),\n    with optional measurement noise.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        true_nu: float = TRUE_NU,\n        noise_std: float = 0.01,\n        grid_size: int = 50,\n        residual_scorer: ResidualScorer | None = None,\n        validation: ValidationRegistry | None = None,\n        callbacks: list[DataCallback] | None = None,\n    ):\n        self.true_nu = true_nu\n        self.noise_std = noise_std\n        self.grid_size = grid_size\n        super().__init__(hp, validation, callbacks, residual_scorer=residual_scorer)\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate numerical solution on a 2D meshgrid via method of lines.\"\"\"\n        import numpy as np\n        from scipy.integrate import solve_ivp\n        from scipy.interpolate import RegularGridInterpolator\n\n        n_x = 256  # interior spatial points for FD discretization\n        x_fd = torch.linspace(-1, 1, n_x + 2).numpy()  # includes boundaries\n        dx = x_fd[1] - x_fd[0]\n\n        # IC: u(x,0) = -sin(pi*x)\n        u0 = -torch.sin(math.pi * torch.tensor(x_fd[1:-1])).numpy()\n\n        t_span = (0.0, 1.0)\n        t_eval = torch.linspace(0, 1, self.grid_size).numpy()\n\n        nu = self.true_nu\n\n        def rhs(_t, u):\n            \"\"\"RHS: du/dt = nu*d2u/dx2 - u*du/dx with homogeneous Dirichlet BCs.\"\"\"\n            # Pad with boundary values (u=0 at x=-1 and x=1)\n            u_pad = torch.zeros(len(u) + 2).numpy()\n            u_pad[1:-1] = u\n\n            # Central differences for d2u/dx2\n            d2u = (u_pad[2:] - 2 * u_pad[1:-1] + u_pad[:-2]) / dx**2\n\n            # Central differences for du/dx\n            du = (u_pad[2:] - u_pad[:-2]) / (2 * dx)\n\n            return nu * d2u - u * du\n\n        sol = solve_ivp(rhs, t_span, u0, t_eval=t_eval, method=\"RK45\", max_step=0.001)\n\n        # sol.y has shape (n_x, len(t_eval))\n        # Interpolate onto output meshgrid\n        x_interior = x_fd[1:-1]\n        t_sol = sol.t\n\n        interp = RegularGridInterpolator(\n            (x_interior, t_sol), sol.y, method=\"linear\", bounds_error=False, fill_value=None\n        )\n\n        # Output meshgrid\n        xs = torch.linspace(-1, 1, self.grid_size)\n        ts = torch.linspace(0, 1, self.grid_size)\n        grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n\n        x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n        pts = np.stack([x_grid[:, 0].numpy(), x_grid[:, 1].numpy()], axis=1)\n        u_ref = torch.tensor(interp(pts), dtype=torch.float32)\n\n        # Add measurement noise\n        u_noisy = u_ref + self.noise_std * torch.randn_like(u_ref)\n\n        # Shape: (N, 1, 1) to match codebase convention\n        y_data = u_noisy.unsqueeze(-1).unsqueeze(1)\n\n        return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.Burgers1DDataModule.grid_size","title":"<code>grid_size = grid_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.Burgers1DDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.Burgers1DDataModule.true_nu","title":"<code>true_nu = true_nu</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.Burgers1DDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, true_nu: float = TRUE_NU, noise_std: float = 0.01, grid_size: int = 50, residual_scorer: ResidualScorer | None = None, validation: ValidationRegistry | None = None, callbacks: list[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/burgers_1d.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    true_nu: float = TRUE_NU,\n    noise_std: float = 0.01,\n    grid_size: int = 50,\n    residual_scorer: ResidualScorer | None = None,\n    validation: ValidationRegistry | None = None,\n    callbacks: list[DataCallback] | None = None,\n):\n    self.true_nu = true_nu\n    self.noise_std = noise_std\n    self.grid_size = grid_size\n    super().__init__(hp, validation, callbacks, residual_scorer=residual_scorer)\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.Burgers1DDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate numerical solution on a 2D meshgrid via method of lines.</p> Source code in <code>src/anypinn/catalog/burgers_1d.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate numerical solution on a 2D meshgrid via method of lines.\"\"\"\n    import numpy as np\n    from scipy.integrate import solve_ivp\n    from scipy.interpolate import RegularGridInterpolator\n\n    n_x = 256  # interior spatial points for FD discretization\n    x_fd = torch.linspace(-1, 1, n_x + 2).numpy()  # includes boundaries\n    dx = x_fd[1] - x_fd[0]\n\n    # IC: u(x,0) = -sin(pi*x)\n    u0 = -torch.sin(math.pi * torch.tensor(x_fd[1:-1])).numpy()\n\n    t_span = (0.0, 1.0)\n    t_eval = torch.linspace(0, 1, self.grid_size).numpy()\n\n    nu = self.true_nu\n\n    def rhs(_t, u):\n        \"\"\"RHS: du/dt = nu*d2u/dx2 - u*du/dx with homogeneous Dirichlet BCs.\"\"\"\n        # Pad with boundary values (u=0 at x=-1 and x=1)\n        u_pad = torch.zeros(len(u) + 2).numpy()\n        u_pad[1:-1] = u\n\n        # Central differences for d2u/dx2\n        d2u = (u_pad[2:] - 2 * u_pad[1:-1] + u_pad[:-2]) / dx**2\n\n        # Central differences for du/dx\n        du = (u_pad[2:] - u_pad[:-2]) / (2 * dx)\n\n        return nu * d2u - u * du\n\n    sol = solve_ivp(rhs, t_span, u0, t_eval=t_eval, method=\"RK45\", max_step=0.001)\n\n    # sol.y has shape (n_x, len(t_eval))\n    # Interpolate onto output meshgrid\n    x_interior = x_fd[1:-1]\n    t_sol = sol.t\n\n    interp = RegularGridInterpolator(\n        (x_interior, t_sol), sol.y, method=\"linear\", bounds_error=False, fill_value=None\n    )\n\n    # Output meshgrid\n    xs = torch.linspace(-1, 1, self.grid_size)\n    ts = torch.linspace(0, 1, self.grid_size)\n    grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n\n    x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n    pts = np.stack([x_grid[:, 0].numpy(), x_grid[:, 1].numpy()], axis=1)\n    u_ref = torch.tensor(interp(pts), dtype=torch.float32)\n\n    # Add measurement noise\n    u_noisy = u_ref + self.noise_std * torch.randn_like(u_ref)\n\n    # Shape: (N, 1, 1) to match codebase convention\n    y_data = u_noisy.unsqueeze(-1).unsqueeze(1)\n\n    return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.DampedOscillatorDataModule","title":"<code>DampedOscillatorDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for damped oscillator inverse problem. Generates synthetic data via odeint.</p> Source code in <code>src/anypinn/catalog/damped_oscillator.py</code> <pre><code>class DampedOscillatorDataModule(PINNDataModule):\n    \"\"\"DataModule for damped oscillator inverse problem. Generates synthetic data via odeint.\"\"\"\n\n    def __init__(\n        self,\n        hp: ODEHyperparameters,\n        gen_props: ODEProperties,\n        noise_std: float = 0.0,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ):\n        super().__init__(hp, validation, callbacks)\n        self.gen_props = gen_props\n        self.noise_std = noise_std\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate synthetic damped oscillator data using odeint + Gaussian noise.\"\"\"\n\n        def oscillator_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n            return self.gen_props.ode(t, y, self.gen_props.args)\n\n        t = config.x\n\n        sol = odeint(oscillator_ode, self.gen_props.y0, t)  # [T, 2]\n        x_true = sol[:, 0]\n\n        x_obs = x_true + self.noise_std * torch.randn_like(x_true)\n\n        return t.unsqueeze(-1), x_obs.unsqueeze(-1).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.DampedOscillatorDataModule.gen_props","title":"<code>gen_props = gen_props</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.DampedOscillatorDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.DampedOscillatorDataModule.__init__","title":"<code>__init__(hp: ODEHyperparameters, gen_props: ODEProperties, noise_std: float = 0.0, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/damped_oscillator.py</code> <pre><code>def __init__(\n    self,\n    hp: ODEHyperparameters,\n    gen_props: ODEProperties,\n    noise_std: float = 0.0,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n):\n    super().__init__(hp, validation, callbacks)\n    self.gen_props = gen_props\n    self.noise_std = noise_std\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.DampedOscillatorDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate synthetic damped oscillator data using odeint + Gaussian noise.</p> Source code in <code>src/anypinn/catalog/damped_oscillator.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate synthetic damped oscillator data using odeint + Gaussian noise.\"\"\"\n\n    def oscillator_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n        return self.gen_props.ode(t, y, self.gen_props.args)\n\n    t = config.x\n\n    sol = odeint(oscillator_ode, self.gen_props.y0, t)  # [T, 2]\n    x_true = sol[:, 0]\n\n    x_obs = x_true + self.noise_std * torch.randn_like(x_true)\n\n    return t.unsqueeze(-1), x_obs.unsqueeze(-1).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.FitzHughNagumoDataModule","title":"<code>FitzHughNagumoDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for FitzHugh-Nagumo inverse problem.</p> <p>Generates synthetic data via odeint. Only the voltage v is observed; the recovery variable w is inferred through ODE residuals alone.</p> Source code in <code>src/anypinn/catalog/fitzhugh_nagumo.py</code> <pre><code>class FitzHughNagumoDataModule(PINNDataModule):\n    \"\"\"DataModule for FitzHugh-Nagumo inverse problem.\n\n    Generates synthetic data via odeint. Only the voltage v is observed;\n    the recovery variable w is inferred through ODE residuals alone.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: ODEHyperparameters,\n        gen_props: ODEProperties,\n        noise_std: float = 0.0,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ):\n        super().__init__(hp, validation, callbacks)\n        self.gen_props = gen_props\n        self.noise_std = noise_std\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate synthetic FHN data. Returns only v (partially observed).\"\"\"\n\n        def fhn_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n            return self.gen_props.ode(t, y, self.gen_props.args)\n\n        t = config.x\n\n        sol = odeint(fhn_ode, self.gen_props.y0, t)  # [T, 2]\n        v_true = sol[:, 0]\n\n        v_obs = v_true + self.noise_std * torch.randn_like(v_true)\n\n        # (N, 1, 1) \u2014 single observed field\n        y_data = v_obs.unsqueeze(1).unsqueeze(-1)\n\n        return t.unsqueeze(-1), y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.FitzHughNagumoDataModule.gen_props","title":"<code>gen_props = gen_props</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.FitzHughNagumoDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.FitzHughNagumoDataModule.__init__","title":"<code>__init__(hp: ODEHyperparameters, gen_props: ODEProperties, noise_std: float = 0.0, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/fitzhugh_nagumo.py</code> <pre><code>def __init__(\n    self,\n    hp: ODEHyperparameters,\n    gen_props: ODEProperties,\n    noise_std: float = 0.0,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n):\n    super().__init__(hp, validation, callbacks)\n    self.gen_props = gen_props\n    self.noise_std = noise_std\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.FitzHughNagumoDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate synthetic FHN data. Returns only v (partially observed).</p> Source code in <code>src/anypinn/catalog/fitzhugh_nagumo.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate synthetic FHN data. Returns only v (partially observed).\"\"\"\n\n    def fhn_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n        return self.gen_props.ode(t, y, self.gen_props.args)\n\n    t = config.x\n\n    sol = odeint(fhn_ode, self.gen_props.y0, t)  # [T, 2]\n    v_true = sol[:, 0]\n\n    v_obs = v_true + self.noise_std * torch.randn_like(v_true)\n\n    # (N, 1, 1) \u2014 single observed field\n    y_data = v_obs.unsqueeze(1).unsqueeze(-1)\n\n    return t.unsqueeze(-1), y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.GrayScott2DDataModule","title":"<code>GrayScott2DDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for 2D Gray-Scott reaction-diffusion inverse problem.</p> <p>gen_data produces ground-truth u(x,y,t) and v(x,y,t) via scipy method-of-lines (finite-difference spatial discretization + ODE integration), with optional measurement noise.</p> Source code in <code>src/anypinn/catalog/gray_scott_2d.py</code> <pre><code>class GrayScott2DDataModule(PINNDataModule):\n    \"\"\"DataModule for 2D Gray-Scott reaction-diffusion inverse problem.\n\n    gen_data produces ground-truth u(x,y,t) and v(x,y,t) via scipy\n    method-of-lines (finite-difference spatial discretization + ODE\n    integration), with optional measurement noise.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        true_du: float = TRUE_DU,\n        true_dv: float = TRUE_DV,\n        true_f: float = TRUE_F,\n        true_k: float = TRUE_K,\n        noise_std: float = 0.01,\n        sim_size: int = 64,\n        residual_scorer: ResidualScorer | None = None,\n        validation: ValidationRegistry | None = None,\n        callbacks: list[DataCallback] | None = None,\n    ):\n        self.true_du = true_du\n        self.true_dv = true_dv\n        self.true_f = true_f\n        self.true_k = true_k\n        self.noise_std = noise_std\n        self.sim_size = sim_size\n        super().__init__(hp, validation, callbacks, residual_scorer=residual_scorer)\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate numerical solution on a 3D meshgrid via method of lines.\"\"\"\n        import numpy as np\n        from scipy.integrate import solve_ivp\n        from scipy.interpolate import RegularGridInterpolator\n\n        n = self.sim_size\n        dx = 1.0 / (n - 1)\n        x_fd = np.linspace(0, 1, n)\n\n        # ICs: u=1, v=0 everywhere; center square u=0.5, v=0.25\n        u0 = np.ones((n, n))\n        v0 = np.zeros((n, n))\n        lo = int(0.4 * n)\n        hi = int(0.6 * n)\n        u0[lo:hi, lo:hi] = 0.5\n        v0[lo:hi, lo:hi] = 0.25\n\n        y0 = np.concatenate([u0.ravel(), v0.ravel()])\n\n        du_val = self.true_du\n        dv_val = self.true_dv\n        f_val = self.true_f\n        k_val = self.true_k\n\n        def rhs(_t: float, y: np.ndarray) -&gt; np.ndarray:\n            u = y[: n * n].reshape(n, n)\n            v = y[n * n :].reshape(n, n)\n\n            # 5-point FD Laplacian with Neumann (zero-flux) BCs via padding\n            u_pad = np.pad(u, 1, mode=\"edge\")\n            v_pad = np.pad(v, 1, mode=\"edge\")\n\n            lap_u = (\n                u_pad[2:, 1:-1]\n                + u_pad[:-2, 1:-1]\n                + u_pad[1:-1, 2:]\n                + u_pad[1:-1, :-2]\n                - 4 * u_pad[1:-1, 1:-1]\n            ) / dx**2\n            lap_v = (\n                v_pad[2:, 1:-1]\n                + v_pad[:-2, 1:-1]\n                + v_pad[1:-1, 2:]\n                + v_pad[1:-1, :-2]\n                - 4 * v_pad[1:-1, 1:-1]\n            ) / dx**2\n\n            uv2 = u * v**2\n            du_dt = du_val * lap_u - uv2 + f_val * (1 - u)\n            dv_dt = dv_val * lap_v + uv2 - (f_val + k_val) * v\n\n            return np.concatenate([du_dt.ravel(), dv_dt.ravel()])\n\n        t_span = (0.0, T_TOTAL)\n        n_t_sim = 50\n        t_eval = np.linspace(0, T_TOTAL, n_t_sim)\n\n        sol = solve_ivp(rhs, t_span, y0, t_eval=t_eval, method=\"Radau\", max_step=5.0)\n\n        # sol.y shape: (2*n*n, n_t_sim)\n        u_sol = sol.y[: n * n, :].reshape(n, n, -1)  # (n, n, n_t)\n        v_sol = sol.y[n * n :, :].reshape(n, n, -1)\n\n        # Build interpolators for u and v\n        t_norm_sim = sol.t / T_TOTAL  # normalize to [0, 1]\n        interp_u = RegularGridInterpolator(\n            (x_fd, x_fd, t_norm_sim),\n            u_sol,\n            method=\"linear\",\n            bounds_error=False,\n            fill_value=None,\n        )\n        interp_v = RegularGridInterpolator(\n            (x_fd, x_fd, t_norm_sim),\n            v_sol,\n            method=\"linear\",\n            bounds_error=False,\n            fill_value=None,\n        )\n\n        # Output measurement grid in [0,1]^3\n        n_xy = 20\n        n_t = 10\n        xs = torch.linspace(0, 1, n_xy)\n        ys = torch.linspace(0, 1, n_xy)\n        ts = torch.linspace(0, 1, n_t)\n        gx, gy, gt = torch.meshgrid(xs, ys, ts, indexing=\"ij\")\n        x_grid = torch.stack([gx.reshape(-1), gy.reshape(-1), gt.reshape(-1)], dim=1)\n\n        pts = x_grid.numpy()\n        u_ref = torch.tensor(interp_u(pts), dtype=torch.float32)\n        v_ref = torch.tensor(interp_v(pts), dtype=torch.float32)\n\n        # Add measurement noise\n        u_noisy = u_ref + self.noise_std * torch.randn_like(u_ref)\n        v_noisy = v_ref + self.noise_std * torch.randn_like(v_ref)\n\n        # Shape: (N, 2, 1)\n        y_data = torch.stack([u_noisy, v_noisy], dim=1).unsqueeze(-1)\n\n        return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.GrayScott2DDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.GrayScott2DDataModule.sim_size","title":"<code>sim_size = sim_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.GrayScott2DDataModule.true_du","title":"<code>true_du = true_du</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.GrayScott2DDataModule.true_dv","title":"<code>true_dv = true_dv</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.GrayScott2DDataModule.true_f","title":"<code>true_f = true_f</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.GrayScott2DDataModule.true_k","title":"<code>true_k = true_k</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.GrayScott2DDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, true_du: float = TRUE_DU, true_dv: float = TRUE_DV, true_f: float = TRUE_F, true_k: float = TRUE_K, noise_std: float = 0.01, sim_size: int = 64, residual_scorer: ResidualScorer | None = None, validation: ValidationRegistry | None = None, callbacks: list[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/gray_scott_2d.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    true_du: float = TRUE_DU,\n    true_dv: float = TRUE_DV,\n    true_f: float = TRUE_F,\n    true_k: float = TRUE_K,\n    noise_std: float = 0.01,\n    sim_size: int = 64,\n    residual_scorer: ResidualScorer | None = None,\n    validation: ValidationRegistry | None = None,\n    callbacks: list[DataCallback] | None = None,\n):\n    self.true_du = true_du\n    self.true_dv = true_dv\n    self.true_f = true_f\n    self.true_k = true_k\n    self.noise_std = noise_std\n    self.sim_size = sim_size\n    super().__init__(hp, validation, callbacks, residual_scorer=residual_scorer)\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.GrayScott2DDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate numerical solution on a 3D meshgrid via method of lines.</p> Source code in <code>src/anypinn/catalog/gray_scott_2d.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate numerical solution on a 3D meshgrid via method of lines.\"\"\"\n    import numpy as np\n    from scipy.integrate import solve_ivp\n    from scipy.interpolate import RegularGridInterpolator\n\n    n = self.sim_size\n    dx = 1.0 / (n - 1)\n    x_fd = np.linspace(0, 1, n)\n\n    # ICs: u=1, v=0 everywhere; center square u=0.5, v=0.25\n    u0 = np.ones((n, n))\n    v0 = np.zeros((n, n))\n    lo = int(0.4 * n)\n    hi = int(0.6 * n)\n    u0[lo:hi, lo:hi] = 0.5\n    v0[lo:hi, lo:hi] = 0.25\n\n    y0 = np.concatenate([u0.ravel(), v0.ravel()])\n\n    du_val = self.true_du\n    dv_val = self.true_dv\n    f_val = self.true_f\n    k_val = self.true_k\n\n    def rhs(_t: float, y: np.ndarray) -&gt; np.ndarray:\n        u = y[: n * n].reshape(n, n)\n        v = y[n * n :].reshape(n, n)\n\n        # 5-point FD Laplacian with Neumann (zero-flux) BCs via padding\n        u_pad = np.pad(u, 1, mode=\"edge\")\n        v_pad = np.pad(v, 1, mode=\"edge\")\n\n        lap_u = (\n            u_pad[2:, 1:-1]\n            + u_pad[:-2, 1:-1]\n            + u_pad[1:-1, 2:]\n            + u_pad[1:-1, :-2]\n            - 4 * u_pad[1:-1, 1:-1]\n        ) / dx**2\n        lap_v = (\n            v_pad[2:, 1:-1]\n            + v_pad[:-2, 1:-1]\n            + v_pad[1:-1, 2:]\n            + v_pad[1:-1, :-2]\n            - 4 * v_pad[1:-1, 1:-1]\n        ) / dx**2\n\n        uv2 = u * v**2\n        du_dt = du_val * lap_u - uv2 + f_val * (1 - u)\n        dv_dt = dv_val * lap_v + uv2 - (f_val + k_val) * v\n\n        return np.concatenate([du_dt.ravel(), dv_dt.ravel()])\n\n    t_span = (0.0, T_TOTAL)\n    n_t_sim = 50\n    t_eval = np.linspace(0, T_TOTAL, n_t_sim)\n\n    sol = solve_ivp(rhs, t_span, y0, t_eval=t_eval, method=\"Radau\", max_step=5.0)\n\n    # sol.y shape: (2*n*n, n_t_sim)\n    u_sol = sol.y[: n * n, :].reshape(n, n, -1)  # (n, n, n_t)\n    v_sol = sol.y[n * n :, :].reshape(n, n, -1)\n\n    # Build interpolators for u and v\n    t_norm_sim = sol.t / T_TOTAL  # normalize to [0, 1]\n    interp_u = RegularGridInterpolator(\n        (x_fd, x_fd, t_norm_sim),\n        u_sol,\n        method=\"linear\",\n        bounds_error=False,\n        fill_value=None,\n    )\n    interp_v = RegularGridInterpolator(\n        (x_fd, x_fd, t_norm_sim),\n        v_sol,\n        method=\"linear\",\n        bounds_error=False,\n        fill_value=None,\n    )\n\n    # Output measurement grid in [0,1]^3\n    n_xy = 20\n    n_t = 10\n    xs = torch.linspace(0, 1, n_xy)\n    ys = torch.linspace(0, 1, n_xy)\n    ts = torch.linspace(0, 1, n_t)\n    gx, gy, gt = torch.meshgrid(xs, ys, ts, indexing=\"ij\")\n    x_grid = torch.stack([gx.reshape(-1), gy.reshape(-1), gt.reshape(-1)], dim=1)\n\n    pts = x_grid.numpy()\n    u_ref = torch.tensor(interp_u(pts), dtype=torch.float32)\n    v_ref = torch.tensor(interp_v(pts), dtype=torch.float32)\n\n    # Add measurement noise\n    u_noisy = u_ref + self.noise_std * torch.randn_like(u_ref)\n    v_noisy = v_ref + self.noise_std * torch.randn_like(v_ref)\n\n    # Shape: (N, 2, 1)\n    y_data = torch.stack([u_noisy, v_noisy], dim=1).unsqueeze(-1)\n\n    return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.Heat1DDataModule","title":"<code>Heat1DDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for 1D heat equation inverse problem.</p> <p>gen_data produces sparse interior measurements from the analytic solution u(x,t) = exp(-alpha pi^2 t) sin(pi x), with optional noise. These measurements are used by DataConstraint during training to recover alpha.</p> Source code in <code>src/anypinn/catalog/heat_1d.py</code> <pre><code>class Heat1DDataModule(PINNDataModule):\n    \"\"\"DataModule for 1D heat equation inverse problem.\n\n    gen_data produces sparse interior measurements from the analytic\n    solution u(x,t) = exp(-alpha pi^2 t) sin(pi x), with optional noise.\n    These measurements are used by DataConstraint during training\n    to recover alpha.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        true_alpha: float = TRUE_ALPHA,\n        n_measurements: int = 200,\n        noise_std: float = 0.01,\n        grid_size: int = 50,\n        validation: ValidationRegistry | None = None,\n        callbacks: list[DataCallback] | None = None,\n    ):\n        self.true_alpha = true_alpha\n        self.n_measurements = n_measurements\n        self.noise_std = noise_std\n        self.grid_size = grid_size\n        super().__init__(hp, validation, callbacks)\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate analytic solution on a 2D meshgrid for training + prediction.\"\"\"\n        xs = torch.linspace(0, 1, self.grid_size)\n        ts = torch.linspace(0, 1, self.grid_size)\n        grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n\n        x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n        u_analytic = torch.exp(-self.true_alpha * math.pi**2 * x_grid[:, 1]) * torch.sin(\n            math.pi * x_grid[:, 0]\n        )\n\n        # Add measurement noise\n        u_noisy = u_analytic + self.noise_std * torch.randn_like(u_analytic)\n\n        # Shape: (N, 1, 1) to match codebase convention\n        y_data = u_noisy.unsqueeze(-1).unsqueeze(1)\n\n        return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.Heat1DDataModule.grid_size","title":"<code>grid_size = grid_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.Heat1DDataModule.n_measurements","title":"<code>n_measurements = n_measurements</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.Heat1DDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.Heat1DDataModule.true_alpha","title":"<code>true_alpha = true_alpha</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.Heat1DDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, true_alpha: float = TRUE_ALPHA, n_measurements: int = 200, noise_std: float = 0.01, grid_size: int = 50, validation: ValidationRegistry | None = None, callbacks: list[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/heat_1d.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    true_alpha: float = TRUE_ALPHA,\n    n_measurements: int = 200,\n    noise_std: float = 0.01,\n    grid_size: int = 50,\n    validation: ValidationRegistry | None = None,\n    callbacks: list[DataCallback] | None = None,\n):\n    self.true_alpha = true_alpha\n    self.n_measurements = n_measurements\n    self.noise_std = noise_std\n    self.grid_size = grid_size\n    super().__init__(hp, validation, callbacks)\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.Heat1DDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate analytic solution on a 2D meshgrid for training + prediction.</p> Source code in <code>src/anypinn/catalog/heat_1d.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate analytic solution on a 2D meshgrid for training + prediction.\"\"\"\n    xs = torch.linspace(0, 1, self.grid_size)\n    ts = torch.linspace(0, 1, self.grid_size)\n    grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n\n    x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n    u_analytic = torch.exp(-self.true_alpha * math.pi**2 * x_grid[:, 1]) * torch.sin(\n        math.pi * x_grid[:, 0]\n    )\n\n    # Add measurement noise\n    u_noisy = u_analytic + self.noise_std * torch.randn_like(u_analytic)\n\n    # Shape: (N, 1, 1) to match codebase convention\n    y_data = u_noisy.unsqueeze(-1).unsqueeze(1)\n\n    return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.InverseDiffusivityDataModule","title":"<code>InverseDiffusivityDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for 1D inverse diffusivity problem.</p> <p>gen_data produces ground-truth u(x,t) via scipy method-of-lines (central differences with variable D(x), integrated with solve_ivp), with optional measurement noise.</p> Source code in <code>src/anypinn/catalog/inverse_diffusivity.py</code> <pre><code>class InverseDiffusivityDataModule(PINNDataModule):\n    \"\"\"DataModule for 1D inverse diffusivity problem.\n\n    gen_data produces ground-truth u(x,t) via scipy method-of-lines\n    (central differences with variable D(x), integrated with solve_ivp),\n    with optional measurement noise.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        n_x: int = 80,\n        n_t: int = 80,\n        noise_std: float = 0.01,\n        grid_size: int = 50,\n        validation: ValidationRegistry | None = None,\n        callbacks: list[DataCallback] | None = None,\n    ):\n        self.n_x = n_x\n        self.n_t = n_t\n        self.noise_std = noise_std\n        self.grid_size = grid_size\n        super().__init__(hp, validation, callbacks)\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate numerical solution on a 2D meshgrid via method of lines.\"\"\"\n        import numpy as np\n        from scipy.integrate import solve_ivp\n        from scipy.interpolate import RegularGridInterpolator\n\n        n_x = self.n_x\n        dx = 1.0 / (n_x - 1)\n        x_fd = np.linspace(0, 1, n_x)\n\n        # True D(x) at grid points\n        d_vals = 0.1 + 0.05 * np.sin(2 * np.pi * x_fd)\n\n        # IC: u(x,0) = sin(pi*x)\n        u0 = np.sin(np.pi * x_fd)\n        # Enforce Dirichlet BCs\n        u0[0] = 0.0\n        u0[-1] = 0.0\n\n        def rhs(_t: float, u: np.ndarray) -&gt; np.ndarray:\n            du_dt = np.zeros_like(u)\n            # Interior points: d/dx(D(x) du/dx) via central differences\n            for i in range(1, n_x - 1):\n                # D at half-points\n                d_right = 0.5 * (d_vals[i] + d_vals[i + 1])\n                d_left = 0.5 * (d_vals[i - 1] + d_vals[i])\n                du_dt[i] = (d_right * (u[i + 1] - u[i]) - d_left * (u[i] - u[i - 1])) / dx**2\n            # BCs: u(0,t) = u(1,t) = 0 =&gt; du_dt = 0 at boundaries\n            return du_dt\n\n        t_span = (0.0, 1.0)\n        t_eval = np.linspace(0, 1, self.n_t)\n\n        sol = solve_ivp(rhs, t_span, u0, t_eval=t_eval, method=\"Radau\", max_step=0.05)\n\n        # sol.y shape: (n_x, n_t_actual)\n        u_sol = sol.y  # (n_x, n_t)\n\n        # Build interpolator\n        interp_u = RegularGridInterpolator(\n            (x_fd, sol.t),\n            u_sol,\n            method=\"linear\",\n            bounds_error=False,\n            fill_value=None,\n        )\n\n        # Output measurement grid\n        xs = torch.linspace(0, 1, self.grid_size)\n        ts = torch.linspace(0, 1, self.grid_size)\n        grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n        x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n        pts = x_grid.numpy()\n        u_ref = torch.tensor(interp_u(pts), dtype=torch.float32)\n\n        # Add measurement noise\n        u_noisy = u_ref + self.noise_std * torch.randn_like(u_ref)\n\n        # Shape: (N, 1, 1) to match codebase convention\n        y_data = u_noisy.unsqueeze(-1).unsqueeze(1)\n\n        return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.InverseDiffusivityDataModule.grid_size","title":"<code>grid_size = grid_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.InverseDiffusivityDataModule.n_t","title":"<code>n_t = n_t</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.InverseDiffusivityDataModule.n_x","title":"<code>n_x = n_x</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.InverseDiffusivityDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.InverseDiffusivityDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, n_x: int = 80, n_t: int = 80, noise_std: float = 0.01, grid_size: int = 50, validation: ValidationRegistry | None = None, callbacks: list[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/inverse_diffusivity.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    n_x: int = 80,\n    n_t: int = 80,\n    noise_std: float = 0.01,\n    grid_size: int = 50,\n    validation: ValidationRegistry | None = None,\n    callbacks: list[DataCallback] | None = None,\n):\n    self.n_x = n_x\n    self.n_t = n_t\n    self.noise_std = noise_std\n    self.grid_size = grid_size\n    super().__init__(hp, validation, callbacks)\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.InverseDiffusivityDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate numerical solution on a 2D meshgrid via method of lines.</p> Source code in <code>src/anypinn/catalog/inverse_diffusivity.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate numerical solution on a 2D meshgrid via method of lines.\"\"\"\n    import numpy as np\n    from scipy.integrate import solve_ivp\n    from scipy.interpolate import RegularGridInterpolator\n\n    n_x = self.n_x\n    dx = 1.0 / (n_x - 1)\n    x_fd = np.linspace(0, 1, n_x)\n\n    # True D(x) at grid points\n    d_vals = 0.1 + 0.05 * np.sin(2 * np.pi * x_fd)\n\n    # IC: u(x,0) = sin(pi*x)\n    u0 = np.sin(np.pi * x_fd)\n    # Enforce Dirichlet BCs\n    u0[0] = 0.0\n    u0[-1] = 0.0\n\n    def rhs(_t: float, u: np.ndarray) -&gt; np.ndarray:\n        du_dt = np.zeros_like(u)\n        # Interior points: d/dx(D(x) du/dx) via central differences\n        for i in range(1, n_x - 1):\n            # D at half-points\n            d_right = 0.5 * (d_vals[i] + d_vals[i + 1])\n            d_left = 0.5 * (d_vals[i - 1] + d_vals[i])\n            du_dt[i] = (d_right * (u[i + 1] - u[i]) - d_left * (u[i] - u[i - 1])) / dx**2\n        # BCs: u(0,t) = u(1,t) = 0 =&gt; du_dt = 0 at boundaries\n        return du_dt\n\n    t_span = (0.0, 1.0)\n    t_eval = np.linspace(0, 1, self.n_t)\n\n    sol = solve_ivp(rhs, t_span, u0, t_eval=t_eval, method=\"Radau\", max_step=0.05)\n\n    # sol.y shape: (n_x, n_t_actual)\n    u_sol = sol.y  # (n_x, n_t)\n\n    # Build interpolator\n    interp_u = RegularGridInterpolator(\n        (x_fd, sol.t),\n        u_sol,\n        method=\"linear\",\n        bounds_error=False,\n        fill_value=None,\n    )\n\n    # Output measurement grid\n    xs = torch.linspace(0, 1, self.grid_size)\n    ts = torch.linspace(0, 1, self.grid_size)\n    grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n    x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n    pts = x_grid.numpy()\n    u_ref = torch.tensor(interp_u(pts), dtype=torch.float32)\n\n    # Add measurement noise\n    u_noisy = u_ref + self.noise_std * torch.randn_like(u_ref)\n\n    # Shape: (N, 1, 1) to match codebase convention\n    y_data = u_noisy.unsqueeze(-1).unsqueeze(1)\n\n    return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.LorenzDataModule","title":"<code>LorenzDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for Lorenz system inverse problem. Generates synthetic data via odeint.</p> Source code in <code>src/anypinn/catalog/lorenz.py</code> <pre><code>class LorenzDataModule(PINNDataModule):\n    \"\"\"DataModule for Lorenz system inverse problem. Generates synthetic data via odeint.\"\"\"\n\n    def __init__(\n        self,\n        hp: ODEHyperparameters,\n        gen_props: ODEProperties,\n        noise_std: float = 0.0,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ):\n        super().__init__(hp, validation, callbacks)\n        self.gen_props = gen_props\n        self.noise_std = noise_std\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate synthetic Lorenz data using odeint + additive Gaussian noise.\"\"\"\n\n        def lorenz_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n            return self.gen_props.ode(t, y, self.gen_props.args)\n\n        t = config.x\n\n        sol = odeint(lorenz_ode, self.gen_props.y0, t)  # [T, 3]\n        x_true = sol[:, 0]\n        y_true = sol[:, 1]\n        z_true = sol[:, 2]\n\n        x_obs = x_true + self.noise_std * torch.randn_like(x_true)\n        y_obs = y_true + self.noise_std * torch.randn_like(y_true)\n        z_obs = z_true + self.noise_std * torch.randn_like(z_true)\n\n        y_data = torch.stack([x_obs, y_obs, z_obs], dim=1).unsqueeze(-1)\n\n        return t.unsqueeze(-1), y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.LorenzDataModule.gen_props","title":"<code>gen_props = gen_props</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.LorenzDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.LorenzDataModule.__init__","title":"<code>__init__(hp: ODEHyperparameters, gen_props: ODEProperties, noise_std: float = 0.0, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/lorenz.py</code> <pre><code>def __init__(\n    self,\n    hp: ODEHyperparameters,\n    gen_props: ODEProperties,\n    noise_std: float = 0.0,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n):\n    super().__init__(hp, validation, callbacks)\n    self.gen_props = gen_props\n    self.noise_std = noise_std\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.LorenzDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate synthetic Lorenz data using odeint + additive Gaussian noise.</p> Source code in <code>src/anypinn/catalog/lorenz.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate synthetic Lorenz data using odeint + additive Gaussian noise.\"\"\"\n\n    def lorenz_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n        return self.gen_props.ode(t, y, self.gen_props.args)\n\n    t = config.x\n\n    sol = odeint(lorenz_ode, self.gen_props.y0, t)  # [T, 3]\n    x_true = sol[:, 0]\n    y_true = sol[:, 1]\n    z_true = sol[:, 2]\n\n    x_obs = x_true + self.noise_std * torch.randn_like(x_true)\n    y_obs = y_true + self.noise_std * torch.randn_like(y_true)\n    z_obs = z_true + self.noise_std * torch.randn_like(z_true)\n\n    y_data = torch.stack([x_obs, y_obs, z_obs], dim=1).unsqueeze(-1)\n\n    return t.unsqueeze(-1), y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.LotkaVolterraDataModule","title":"<code>LotkaVolterraDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for Lotka-Volterra inverse problem. Generates synthetic data via odeint.</p> Source code in <code>src/anypinn/catalog/lotka_volterra.py</code> <pre><code>class LotkaVolterraDataModule(PINNDataModule):\n    \"\"\"DataModule for Lotka-Volterra inverse problem. Generates synthetic data via odeint.\"\"\"\n\n    def __init__(\n        self,\n        hp: ODEHyperparameters,\n        gen_props: ODEProperties,\n        noise_frac: float = 0.0,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ):\n        super().__init__(hp, validation, callbacks)\n        self.gen_props = gen_props\n        self.noise_frac = noise_frac\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate synthetic Lotka-Volterra data using odeint + Gaussian noise.\"\"\"\n\n        def lv_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n            return self.gen_props.ode(t, y, self.gen_props.args)\n\n        t = config.x\n\n        sol = odeint(lv_ode, self.gen_props.y0, t)  # [T, 2]\n        x_true = sol[:, 0].clamp_min(0.0)\n        y_true = sol[:, 1].clamp_min(0.0)\n\n        x_obs = x_true + self.noise_frac * x_true.abs().mean() * torch.randn_like(x_true)\n        y_obs = y_true + self.noise_frac * y_true.abs().mean() * torch.randn_like(y_true)\n        x_obs = x_obs.clamp_min(0.0)\n        y_obs = y_obs.clamp_min(0.0)\n\n        y_data = torch.stack([x_obs, y_obs], dim=1).unsqueeze(-1)\n\n        return t.unsqueeze(-1), y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.LotkaVolterraDataModule.gen_props","title":"<code>gen_props = gen_props</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.LotkaVolterraDataModule.noise_frac","title":"<code>noise_frac = noise_frac</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.LotkaVolterraDataModule.__init__","title":"<code>__init__(hp: ODEHyperparameters, gen_props: ODEProperties, noise_frac: float = 0.0, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/lotka_volterra.py</code> <pre><code>def __init__(\n    self,\n    hp: ODEHyperparameters,\n    gen_props: ODEProperties,\n    noise_frac: float = 0.0,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n):\n    super().__init__(hp, validation, callbacks)\n    self.gen_props = gen_props\n    self.noise_frac = noise_frac\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.LotkaVolterraDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate synthetic Lotka-Volterra data using odeint + Gaussian noise.</p> Source code in <code>src/anypinn/catalog/lotka_volterra.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate synthetic Lotka-Volterra data using odeint + Gaussian noise.\"\"\"\n\n    def lv_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n        return self.gen_props.ode(t, y, self.gen_props.args)\n\n    t = config.x\n\n    sol = odeint(lv_ode, self.gen_props.y0, t)  # [T, 2]\n    x_true = sol[:, 0].clamp_min(0.0)\n    y_true = sol[:, 1].clamp_min(0.0)\n\n    x_obs = x_true + self.noise_frac * x_true.abs().mean() * torch.randn_like(x_true)\n    y_obs = y_true + self.noise_frac * y_true.abs().mean() * torch.randn_like(y_true)\n    x_obs = x_obs.clamp_min(0.0)\n    y_obs = y_obs.clamp_min(0.0)\n\n    y_data = torch.stack([x_obs, y_obs], dim=1).unsqueeze(-1)\n\n    return t.unsqueeze(-1), y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.Poisson2DDataModule","title":"<code>Poisson2DDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for 2D Poisson equation on [0,1]^2.</p> <p>Generates a meshgrid with the analytic solution u(x,y) = sin(pix)sin(pi*y). The data is used for prediction/validation only -- training uses PDEResidualConstraint + DirichletBCConstraints (no DataConstraint).</p> Source code in <code>src/anypinn/catalog/poisson_2d.py</code> <pre><code>class Poisson2DDataModule(PINNDataModule):\n    \"\"\"DataModule for 2D Poisson equation on [0,1]^2.\n\n    Generates a meshgrid with the analytic solution u(x,y) = sin(pi*x)*sin(pi*y).\n    The data is used for prediction/validation only -- training uses\n    PDEResidualConstraint + DirichletBCConstraints (no DataConstraint).\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        grid_size: int = 30,\n        validation: ValidationRegistry | None = None,\n        callbacks: list[DataCallback] | None = None,\n    ):\n        self.grid_size = grid_size\n        super().__init__(hp, validation, callbacks)\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate analytic solution on a 2D meshgrid for prediction.\"\"\"\n        xs = torch.linspace(0, 1, self.grid_size)\n        ys = torch.linspace(0, 1, self.grid_size)\n        grid_x, grid_y = torch.meshgrid(xs, ys, indexing=\"ij\")\n\n        x_grid = torch.stack([grid_x.reshape(-1), grid_y.reshape(-1)], dim=1)  # (N, 2)\n\n        u_analytic = torch.sin(math.pi * x_grid[:, 0]) * torch.sin(math.pi * x_grid[:, 1])\n\n        # Shape: (N, 1, 1) to match codebase convention\n        y_data = u_analytic.unsqueeze(-1).unsqueeze(1)\n\n        return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.Poisson2DDataModule.grid_size","title":"<code>grid_size = grid_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.Poisson2DDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, grid_size: int = 30, validation: ValidationRegistry | None = None, callbacks: list[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/poisson_2d.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    grid_size: int = 30,\n    validation: ValidationRegistry | None = None,\n    callbacks: list[DataCallback] | None = None,\n):\n    self.grid_size = grid_size\n    super().__init__(hp, validation, callbacks)\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.Poisson2DDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate analytic solution on a 2D meshgrid for prediction.</p> Source code in <code>src/anypinn/catalog/poisson_2d.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate analytic solution on a 2D meshgrid for prediction.\"\"\"\n    xs = torch.linspace(0, 1, self.grid_size)\n    ys = torch.linspace(0, 1, self.grid_size)\n    grid_x, grid_y = torch.meshgrid(xs, ys, indexing=\"ij\")\n\n    x_grid = torch.stack([grid_x.reshape(-1), grid_y.reshape(-1)], dim=1)  # (N, 2)\n\n    u_analytic = torch.sin(math.pi * x_grid[:, 0]) * torch.sin(math.pi * x_grid[:, 1])\n\n    # Shape: (N, 1, 1) to match codebase convention\n    y_data = u_analytic.unsqueeze(-1).unsqueeze(1)\n\n    return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.SEIRDataModule","title":"<code>SEIRDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for SEIR inverse problem. Generates synthetic data via odeint.</p> Source code in <code>src/anypinn/catalog/seir.py</code> <pre><code>class SEIRDataModule(PINNDataModule):\n    \"\"\"DataModule for SEIR inverse problem. Generates synthetic data via odeint.\"\"\"\n\n    def __init__(\n        self,\n        hp: ODEHyperparameters,\n        gen_props: ODEProperties,\n        noise_std: float = 0.0,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ):\n        super().__init__(hp, validation, callbacks)\n        self.gen_props = gen_props\n        self.noise_std = noise_std\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate synthetic SEIR data using odeint + Gaussian noise.\"\"\"\n\n        def seir_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n            return self.gen_props.ode(t, y, self.gen_props.args)\n\n        t = config.x\n\n        sol = odeint(seir_ode, self.gen_props.y0, t)  # [T, 3]\n        I_true = sol[:, 2].clamp_min(0.0)\n\n        I_obs = I_true + self.noise_std * torch.randn_like(I_true)\n        I_obs = I_obs.clamp_min(0.0)\n\n        return t.unsqueeze(-1), I_obs.unsqueeze(-1).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.SEIRDataModule.gen_props","title":"<code>gen_props = gen_props</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.SEIRDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.SEIRDataModule.__init__","title":"<code>__init__(hp: ODEHyperparameters, gen_props: ODEProperties, noise_std: float = 0.0, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/seir.py</code> <pre><code>def __init__(\n    self,\n    hp: ODEHyperparameters,\n    gen_props: ODEProperties,\n    noise_std: float = 0.0,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n):\n    super().__init__(hp, validation, callbacks)\n    self.gen_props = gen_props\n    self.noise_std = noise_std\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.SEIRDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate synthetic SEIR data using odeint + Gaussian noise.</p> Source code in <code>src/anypinn/catalog/seir.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate synthetic SEIR data using odeint + Gaussian noise.\"\"\"\n\n    def seir_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n        return self.gen_props.ode(t, y, self.gen_props.args)\n\n    t = config.x\n\n    sol = odeint(seir_ode, self.gen_props.y0, t)  # [T, 3]\n    I_true = sol[:, 2].clamp_min(0.0)\n\n    I_obs = I_true + self.noise_std * torch.randn_like(I_true)\n    I_obs = I_obs.clamp_min(0.0)\n\n    return t.unsqueeze(-1), I_obs.unsqueeze(-1).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.SIRInvDataModule","title":"<code>SIRInvDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for SIR Inverse problem.</p> Source code in <code>src/anypinn/catalog/sir.py</code> <pre><code>class SIRInvDataModule(PINNDataModule):\n    \"\"\"\n    DataModule for SIR Inverse problem.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: ODEHyperparameters,\n        gen_props: ODEProperties | None = None,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ):\n        super().__init__(hp, validation, callbacks)\n        self.gen_props = gen_props\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate synthetic data.\"\"\"\n        assert self.gen_props is not None, \"SIR properties are required to generate data\"\n        gen_props = self.gen_props\n\n        args = gen_props.args.copy()\n        args.update(config.args_to_train)\n\n        data = odeint(\n            lambda x, y: gen_props.ode(x, y, args),\n            gen_props.y0,\n            config.x,\n        )\n\n        I_true = data[:, 1].clamp_min(0.0)\n\n        I_obs = self._noise(I_true, config.noise_level)\n\n        return config.x.unsqueeze(-1), I_obs.unsqueeze(-1).unsqueeze(1)\n\n    def _noise(self, I_true: Tensor, noise_level: float) -&gt; Tensor:\n        if noise_level &lt; 1.0:\n            return I_true\n        else:\n            return torch.poisson(I_true / noise_level) * noise_level\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.SIRInvDataModule.gen_props","title":"<code>gen_props = gen_props</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.SIRInvDataModule.__init__","title":"<code>__init__(hp: ODEHyperparameters, gen_props: ODEProperties | None = None, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/sir.py</code> <pre><code>def __init__(\n    self,\n    hp: ODEHyperparameters,\n    gen_props: ODEProperties | None = None,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n):\n    super().__init__(hp, validation, callbacks)\n    self.gen_props = gen_props\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.SIRInvDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate synthetic data.</p> Source code in <code>src/anypinn/catalog/sir.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate synthetic data.\"\"\"\n    assert self.gen_props is not None, \"SIR properties are required to generate data\"\n    gen_props = self.gen_props\n\n    args = gen_props.args.copy()\n    args.update(config.args_to_train)\n\n    data = odeint(\n        lambda x, y: gen_props.ode(x, y, args),\n        gen_props.y0,\n        config.x,\n    )\n\n    I_true = data[:, 1].clamp_min(0.0)\n\n    I_obs = self._noise(I_true, config.noise_level)\n\n    return config.x.unsqueeze(-1), I_obs.unsqueeze(-1).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.VanDerPolDataModule","title":"<code>VanDerPolDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for Van der Pol oscillator inverse problem.</p> <p>Generates synthetic data via odeint.</p> Source code in <code>src/anypinn/catalog/van_der_pol.py</code> <pre><code>class VanDerPolDataModule(PINNDataModule):\n    \"\"\"DataModule for Van der Pol oscillator inverse problem.\n\n    Generates synthetic data via odeint.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: ODEHyperparameters,\n        gen_props: ODEProperties,\n        noise_std: float = 0.0,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ):\n        super().__init__(hp, validation, callbacks)\n        self.gen_props = gen_props\n        self.noise_std = noise_std\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate synthetic Van der Pol data using odeint + Gaussian noise.\"\"\"\n\n        def vdp_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n            return self.gen_props.ode(t, y, self.gen_props.args)\n\n        t = config.x\n\n        sol = odeint(vdp_ode, self.gen_props.y0, t)  # [T, 2]\n        u_true = sol[:, 0]\n\n        u_obs = u_true + self.noise_std * torch.randn_like(u_true)\n\n        return t.unsqueeze(-1), u_obs.unsqueeze(-1).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.VanDerPolDataModule.gen_props","title":"<code>gen_props = gen_props</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.VanDerPolDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.VanDerPolDataModule.__init__","title":"<code>__init__(hp: ODEHyperparameters, gen_props: ODEProperties, noise_std: float = 0.0, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/van_der_pol.py</code> <pre><code>def __init__(\n    self,\n    hp: ODEHyperparameters,\n    gen_props: ODEProperties,\n    noise_std: float = 0.0,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n):\n    super().__init__(hp, validation, callbacks)\n    self.gen_props = gen_props\n    self.noise_std = noise_std\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.VanDerPolDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate synthetic Van der Pol data using odeint + Gaussian noise.</p> Source code in <code>src/anypinn/catalog/van_der_pol.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate synthetic Van der Pol data using odeint + Gaussian noise.\"\"\"\n\n    def vdp_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n        return self.gen_props.ode(t, y, self.gen_props.args)\n\n    t = config.x\n\n    sol = odeint(vdp_ode, self.gen_props.y0, t)  # [T, 2]\n    u_true = sol[:, 0]\n\n    u_obs = u_true + self.noise_std * torch.randn_like(u_true)\n\n    return t.unsqueeze(-1), u_obs.unsqueeze(-1).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.Wave1DDataModule","title":"<code>Wave1DDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for 1D wave equation inverse problem.</p> <p>gen_data produces sparse interior measurements from the analytic solution u(x,t) = sin(pi x) cos(c pi t), with optional noise. These measurements are used by DataConstraint during training to recover c.</p> Source code in <code>src/anypinn/catalog/wave_1d.py</code> <pre><code>class Wave1DDataModule(PINNDataModule):\n    \"\"\"DataModule for 1D wave equation inverse problem.\n\n    gen_data produces sparse interior measurements from the analytic\n    solution u(x,t) = sin(pi x) cos(c pi t), with optional noise.\n    These measurements are used by DataConstraint during training\n    to recover c.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        true_c: float = TRUE_C,\n        n_measurements: int = 200,\n        noise_std: float = 0.01,\n        grid_size: int = 50,\n        validation: ValidationRegistry | None = None,\n        callbacks: list[DataCallback] | None = None,\n    ):\n        self.true_c = true_c\n        self.n_measurements = n_measurements\n        self.noise_std = noise_std\n        self.grid_size = grid_size\n        super().__init__(hp, validation, callbacks)\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate analytic solution on a 2D meshgrid for training + prediction.\"\"\"\n        xs = torch.linspace(0, 1, self.grid_size)\n        ts = torch.linspace(0, 1, self.grid_size)\n        grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n\n        x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n        u_analytic = torch.sin(math.pi * x_grid[:, 0]) * torch.cos(\n            self.true_c * math.pi * x_grid[:, 1]\n        )\n\n        # Add measurement noise\n        u_noisy = u_analytic + self.noise_std * torch.randn_like(u_analytic)\n\n        # Shape: (N, 1, 1) to match codebase convention\n        y_data = u_noisy.unsqueeze(-1).unsqueeze(1)\n\n        return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.Wave1DDataModule.grid_size","title":"<code>grid_size = grid_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.Wave1DDataModule.n_measurements","title":"<code>n_measurements = n_measurements</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.Wave1DDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.Wave1DDataModule.true_c","title":"<code>true_c = true_c</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/#anypinn.catalog.Wave1DDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, true_c: float = TRUE_C, n_measurements: int = 200, noise_std: float = 0.01, grid_size: int = 50, validation: ValidationRegistry | None = None, callbacks: list[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/wave_1d.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    true_c: float = TRUE_C,\n    n_measurements: int = 200,\n    noise_std: float = 0.01,\n    grid_size: int = 50,\n    validation: ValidationRegistry | None = None,\n    callbacks: list[DataCallback] | None = None,\n):\n    self.true_c = true_c\n    self.n_measurements = n_measurements\n    self.noise_std = noise_std\n    self.grid_size = grid_size\n    super().__init__(hp, validation, callbacks)\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.Wave1DDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate analytic solution on a 2D meshgrid for training + prediction.</p> Source code in <code>src/anypinn/catalog/wave_1d.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate analytic solution on a 2D meshgrid for training + prediction.\"\"\"\n    xs = torch.linspace(0, 1, self.grid_size)\n    ts = torch.linspace(0, 1, self.grid_size)\n    grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n\n    x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n    u_analytic = torch.sin(math.pi * x_grid[:, 0]) * torch.cos(\n        self.true_c * math.pi * x_grid[:, 1]\n    )\n\n    # Add measurement noise\n    u_noisy = u_analytic + self.noise_std * torch.randn_like(u_analytic)\n\n    # Shape: (N, 1, 1) to match codebase convention\n    y_data = u_noisy.unsqueeze(-1).unsqueeze(1)\n\n    return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.SIR","title":"<code>SIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>The SIR ODE system. $$ \\begin{align} \\frac{dS}{dt} &amp;= -\\beta \\frac{SI}{N} \\ \\frac{dI}{dt} &amp;= \\beta \\frac{SI}{N} - \\delta I \\ \\frac{dR}{dt} &amp;= \\delta I \\ \\end{align} $$</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Time variable.</p> required <code>y</code> <code>Tensor</code> <p>State variables [S, I].</p> required <code>args</code> <code>ArgsRegistry</code> <p>Arguments dictionary (beta, delta, N).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Derivatives [dS/dt, dI/dt].</p> Source code in <code>src/anypinn/catalog/sir.py</code> <pre><code>def SIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"\n    The SIR ODE system.\n    $$\n    \\\\begin{align}\n    \\\\frac{dS}{dt} &amp;= -\\\\beta \\\\frac{SI}{N} \\\\\\\\\n    \\\\frac{dI}{dt} &amp;= \\\\beta \\\\frac{SI}{N} - \\\\delta I \\\\\\\\\n    \\\\frac{dR}{dt} &amp;= \\\\delta I \\\\\\\\\n    \\\\end{align}\n    $$\n\n    Args:\n        x: Time variable.\n        y: State variables [S, I].\n        args: Arguments dictionary (beta, delta, N).\n\n    Returns:\n        Derivatives [dS/dt, dI/dt].\n    \"\"\"\n    S, I = y\n    b, d, N = args[BETA_KEY], args[DELTA_KEY], args[N_KEY]\n\n    dS = -b(x) * S * I / N(x)\n    dI = b(x) * S * I / N(x) - d(x) * I\n    return torch.stack([dS, dI])\n</code></pre>"},{"location":"reference/anypinn/catalog/#anypinn.catalog.rSIR","title":"<code>rSIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>The reduced SIR ODE system. $$ \\begin{align} \\frac{dI}{dt} &amp;= \\delta (R_t - 1) I \\end{align} $$</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Time variable.</p> required <code>y</code> <code>Tensor</code> <p>State variables [I].</p> required <code>args</code> <code>ArgsRegistry</code> <p>Arguments dictionary (delta, Rt).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Derivatives [dI/dt].</p> Source code in <code>src/anypinn/catalog/sir.py</code> <pre><code>def rSIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"\n    The reduced SIR ODE system.\n    $$\n    \\\\begin{align}\n    \\\\frac{dI}{dt} &amp;= \\\\delta (R_t - 1) I\n    \\\\end{align}\n    $$\n\n    Args:\n        x: Time variable.\n        y: State variables [I].\n        args: Arguments dictionary (delta, Rt).\n\n    Returns:\n        Derivatives [dI/dt].\n    \"\"\"\n    I = y\n    d, Rt = args[DELTA_KEY], args[Rt_KEY]\n\n    dI = d(x) * (Rt(x) - 1) * I\n    return dI\n</code></pre>"},{"location":"reference/anypinn/catalog/allen_cahn/","title":"allen_cahn","text":""},{"location":"reference/anypinn/catalog/allen_cahn/#anypinn.catalog.allen_cahn","title":"<code>anypinn.catalog.allen_cahn</code>","text":""},{"location":"reference/anypinn/catalog/allen_cahn/#anypinn.catalog.allen_cahn.TRUE_EPSILON","title":"<code>TRUE_EPSILON = 0.01</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/allen_cahn/#anypinn.catalog.allen_cahn.U_KEY","title":"<code>U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/allen_cahn/#anypinn.catalog.allen_cahn.AllenCahnDataModule","title":"<code>AllenCahnDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for 1D Allen-Cahn equation.</p> <p>gen_data produces ground-truth u(x,t) via scipy method-of-lines (central differences for d2u/dx2 with periodic ghost cells + ODE integration). The data is used for prediction/validation only \u2014 training uses PDEResidualConstraint + PeriodicBCConstraint + IC (no DataConstraint).</p> Source code in <code>src/anypinn/catalog/allen_cahn.py</code> <pre><code>class AllenCahnDataModule(PINNDataModule):\n    \"\"\"DataModule for 1D Allen-Cahn equation.\n\n    gen_data produces ground-truth u(x,t) via scipy method-of-lines\n    (central differences for d2u/dx2 with periodic ghost cells + ODE integration).\n    The data is used for prediction/validation only \u2014 training uses\n    PDEResidualConstraint + PeriodicBCConstraint + IC (no DataConstraint).\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        true_epsilon: float = TRUE_EPSILON,\n        n_x: int = 256,\n        n_t: int = 200,\n        grid_size: int = 50,\n        residual_scorer: ResidualScorer | None = None,\n        validation: ValidationRegistry | None = None,\n        callbacks: list[DataCallback] | None = None,\n    ):\n        self.true_epsilon = true_epsilon\n        self.n_x = n_x\n        self.n_t = n_t\n        self.grid_size = grid_size\n        super().__init__(hp, validation, callbacks, residual_scorer=residual_scorer)\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate numerical solution on a 2D meshgrid via method of lines.\"\"\"\n        import numpy as np\n        from scipy.integrate import solve_ivp\n        from scipy.interpolate import RegularGridInterpolator\n\n        n_x = self.n_x\n        eps = self.true_epsilon\n\n        # Periodic domain [-1, 1): n_x interior points, dx = 2/n_x\n        x_fd = np.linspace(-1, 1, n_x, endpoint=False)\n        dx = x_fd[1] - x_fd[0]\n\n        # IC: u(x,0) = -tanh(x / (sqrt(2*eps)))\n        scale = math.sqrt(2 * eps)\n        u0 = -np.tanh(x_fd / scale)\n\n        t_span = (0.0, 1.0)\n        t_eval = np.linspace(0, 1, self.n_t)\n\n        def rhs(_t: float, u: np.ndarray) -&gt; np.ndarray:\n            \"\"\"RHS: du/dt = eps*d2u/dx2 + u - u^3 with periodic BCs.\"\"\"\n            # Periodic padding\n            u_pad = np.empty(n_x + 2)\n            u_pad[1:-1] = u\n            u_pad[0] = u[-1]  # left ghost = rightmost interior\n            u_pad[-1] = u[0]  # right ghost = leftmost interior\n\n            # Central differences for d2u/dx2\n            d2u = (u_pad[2:] - 2 * u_pad[1:-1] + u_pad[:-2]) / dx**2\n\n            return eps * d2u + u - u**3\n\n        sol = solve_ivp(rhs, t_span, u0, t_eval=t_eval, method=\"RK45\", max_step=0.001)\n\n        # sol.y has shape (n_x, n_t)\n        x_sol = x_fd\n        t_sol = sol.t\n\n        interp = RegularGridInterpolator(\n            (x_sol, t_sol), sol.y, method=\"linear\", bounds_error=False, fill_value=None\n        )\n\n        # Output meshgrid\n        xs = torch.linspace(-1, 1, self.grid_size)\n        ts = torch.linspace(0, 1, self.grid_size)\n        grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n\n        x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n        pts = np.stack([x_grid[:, 0].numpy(), x_grid[:, 1].numpy()], axis=1)\n        u_ref = torch.tensor(interp(pts), dtype=torch.float32)\n\n        # Shape: (N, 1, 1) to match codebase convention\n        y_data = u_ref.unsqueeze(-1).unsqueeze(1)\n\n        return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/allen_cahn/#anypinn.catalog.allen_cahn.AllenCahnDataModule.grid_size","title":"<code>grid_size = grid_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/allen_cahn/#anypinn.catalog.allen_cahn.AllenCahnDataModule.n_t","title":"<code>n_t = n_t</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/allen_cahn/#anypinn.catalog.allen_cahn.AllenCahnDataModule.n_x","title":"<code>n_x = n_x</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/allen_cahn/#anypinn.catalog.allen_cahn.AllenCahnDataModule.true_epsilon","title":"<code>true_epsilon = true_epsilon</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/allen_cahn/#anypinn.catalog.allen_cahn.AllenCahnDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, true_epsilon: float = TRUE_EPSILON, n_x: int = 256, n_t: int = 200, grid_size: int = 50, residual_scorer: ResidualScorer | None = None, validation: ValidationRegistry | None = None, callbacks: list[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/allen_cahn.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    true_epsilon: float = TRUE_EPSILON,\n    n_x: int = 256,\n    n_t: int = 200,\n    grid_size: int = 50,\n    residual_scorer: ResidualScorer | None = None,\n    validation: ValidationRegistry | None = None,\n    callbacks: list[DataCallback] | None = None,\n):\n    self.true_epsilon = true_epsilon\n    self.n_x = n_x\n    self.n_t = n_t\n    self.grid_size = grid_size\n    super().__init__(hp, validation, callbacks, residual_scorer=residual_scorer)\n</code></pre>"},{"location":"reference/anypinn/catalog/allen_cahn/#anypinn.catalog.allen_cahn.AllenCahnDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate numerical solution on a 2D meshgrid via method of lines.</p> Source code in <code>src/anypinn/catalog/allen_cahn.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate numerical solution on a 2D meshgrid via method of lines.\"\"\"\n    import numpy as np\n    from scipy.integrate import solve_ivp\n    from scipy.interpolate import RegularGridInterpolator\n\n    n_x = self.n_x\n    eps = self.true_epsilon\n\n    # Periodic domain [-1, 1): n_x interior points, dx = 2/n_x\n    x_fd = np.linspace(-1, 1, n_x, endpoint=False)\n    dx = x_fd[1] - x_fd[0]\n\n    # IC: u(x,0) = -tanh(x / (sqrt(2*eps)))\n    scale = math.sqrt(2 * eps)\n    u0 = -np.tanh(x_fd / scale)\n\n    t_span = (0.0, 1.0)\n    t_eval = np.linspace(0, 1, self.n_t)\n\n    def rhs(_t: float, u: np.ndarray) -&gt; np.ndarray:\n        \"\"\"RHS: du/dt = eps*d2u/dx2 + u - u^3 with periodic BCs.\"\"\"\n        # Periodic padding\n        u_pad = np.empty(n_x + 2)\n        u_pad[1:-1] = u\n        u_pad[0] = u[-1]  # left ghost = rightmost interior\n        u_pad[-1] = u[0]  # right ghost = leftmost interior\n\n        # Central differences for d2u/dx2\n        d2u = (u_pad[2:] - 2 * u_pad[1:-1] + u_pad[:-2]) / dx**2\n\n        return eps * d2u + u - u**3\n\n    sol = solve_ivp(rhs, t_span, u0, t_eval=t_eval, method=\"RK45\", max_step=0.001)\n\n    # sol.y has shape (n_x, n_t)\n    x_sol = x_fd\n    t_sol = sol.t\n\n    interp = RegularGridInterpolator(\n        (x_sol, t_sol), sol.y, method=\"linear\", bounds_error=False, fill_value=None\n    )\n\n    # Output meshgrid\n    xs = torch.linspace(-1, 1, self.grid_size)\n    ts = torch.linspace(0, 1, self.grid_size)\n    grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n\n    x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n    pts = np.stack([x_grid[:, 0].numpy(), x_grid[:, 1].numpy()], axis=1)\n    u_ref = torch.tensor(interp(pts), dtype=torch.float32)\n\n    # Shape: (N, 1, 1) to match codebase convention\n    y_data = u_ref.unsqueeze(-1).unsqueeze(1)\n\n    return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/burgers_1d/","title":"burgers_1d","text":""},{"location":"reference/anypinn/catalog/burgers_1d/#anypinn.catalog.burgers_1d","title":"<code>anypinn.catalog.burgers_1d</code>","text":""},{"location":"reference/anypinn/catalog/burgers_1d/#anypinn.catalog.burgers_1d.NU_KEY","title":"<code>NU_KEY = 'nu'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/burgers_1d/#anypinn.catalog.burgers_1d.TRUE_NU","title":"<code>TRUE_NU = 0.01 / math.pi</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/burgers_1d/#anypinn.catalog.burgers_1d.U_KEY","title":"<code>U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/burgers_1d/#anypinn.catalog.burgers_1d.Burgers1DDataModule","title":"<code>Burgers1DDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for 1D Burgers equation inverse problem.</p> <p>gen_data produces ground-truth u(x,t) via scipy method-of-lines (finite-difference spatial discretization + ODE integration), with optional measurement noise.</p> Source code in <code>src/anypinn/catalog/burgers_1d.py</code> <pre><code>class Burgers1DDataModule(PINNDataModule):\n    \"\"\"DataModule for 1D Burgers equation inverse problem.\n\n    gen_data produces ground-truth u(x,t) via scipy method-of-lines\n    (finite-difference spatial discretization + ODE integration),\n    with optional measurement noise.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        true_nu: float = TRUE_NU,\n        noise_std: float = 0.01,\n        grid_size: int = 50,\n        residual_scorer: ResidualScorer | None = None,\n        validation: ValidationRegistry | None = None,\n        callbacks: list[DataCallback] | None = None,\n    ):\n        self.true_nu = true_nu\n        self.noise_std = noise_std\n        self.grid_size = grid_size\n        super().__init__(hp, validation, callbacks, residual_scorer=residual_scorer)\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate numerical solution on a 2D meshgrid via method of lines.\"\"\"\n        import numpy as np\n        from scipy.integrate import solve_ivp\n        from scipy.interpolate import RegularGridInterpolator\n\n        n_x = 256  # interior spatial points for FD discretization\n        x_fd = torch.linspace(-1, 1, n_x + 2).numpy()  # includes boundaries\n        dx = x_fd[1] - x_fd[0]\n\n        # IC: u(x,0) = -sin(pi*x)\n        u0 = -torch.sin(math.pi * torch.tensor(x_fd[1:-1])).numpy()\n\n        t_span = (0.0, 1.0)\n        t_eval = torch.linspace(0, 1, self.grid_size).numpy()\n\n        nu = self.true_nu\n\n        def rhs(_t, u):\n            \"\"\"RHS: du/dt = nu*d2u/dx2 - u*du/dx with homogeneous Dirichlet BCs.\"\"\"\n            # Pad with boundary values (u=0 at x=-1 and x=1)\n            u_pad = torch.zeros(len(u) + 2).numpy()\n            u_pad[1:-1] = u\n\n            # Central differences for d2u/dx2\n            d2u = (u_pad[2:] - 2 * u_pad[1:-1] + u_pad[:-2]) / dx**2\n\n            # Central differences for du/dx\n            du = (u_pad[2:] - u_pad[:-2]) / (2 * dx)\n\n            return nu * d2u - u * du\n\n        sol = solve_ivp(rhs, t_span, u0, t_eval=t_eval, method=\"RK45\", max_step=0.001)\n\n        # sol.y has shape (n_x, len(t_eval))\n        # Interpolate onto output meshgrid\n        x_interior = x_fd[1:-1]\n        t_sol = sol.t\n\n        interp = RegularGridInterpolator(\n            (x_interior, t_sol), sol.y, method=\"linear\", bounds_error=False, fill_value=None\n        )\n\n        # Output meshgrid\n        xs = torch.linspace(-1, 1, self.grid_size)\n        ts = torch.linspace(0, 1, self.grid_size)\n        grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n\n        x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n        pts = np.stack([x_grid[:, 0].numpy(), x_grid[:, 1].numpy()], axis=1)\n        u_ref = torch.tensor(interp(pts), dtype=torch.float32)\n\n        # Add measurement noise\n        u_noisy = u_ref + self.noise_std * torch.randn_like(u_ref)\n\n        # Shape: (N, 1, 1) to match codebase convention\n        y_data = u_noisy.unsqueeze(-1).unsqueeze(1)\n\n        return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/burgers_1d/#anypinn.catalog.burgers_1d.Burgers1DDataModule.grid_size","title":"<code>grid_size = grid_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/burgers_1d/#anypinn.catalog.burgers_1d.Burgers1DDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/burgers_1d/#anypinn.catalog.burgers_1d.Burgers1DDataModule.true_nu","title":"<code>true_nu = true_nu</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/burgers_1d/#anypinn.catalog.burgers_1d.Burgers1DDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, true_nu: float = TRUE_NU, noise_std: float = 0.01, grid_size: int = 50, residual_scorer: ResidualScorer | None = None, validation: ValidationRegistry | None = None, callbacks: list[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/burgers_1d.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    true_nu: float = TRUE_NU,\n    noise_std: float = 0.01,\n    grid_size: int = 50,\n    residual_scorer: ResidualScorer | None = None,\n    validation: ValidationRegistry | None = None,\n    callbacks: list[DataCallback] | None = None,\n):\n    self.true_nu = true_nu\n    self.noise_std = noise_std\n    self.grid_size = grid_size\n    super().__init__(hp, validation, callbacks, residual_scorer=residual_scorer)\n</code></pre>"},{"location":"reference/anypinn/catalog/burgers_1d/#anypinn.catalog.burgers_1d.Burgers1DDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate numerical solution on a 2D meshgrid via method of lines.</p> Source code in <code>src/anypinn/catalog/burgers_1d.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate numerical solution on a 2D meshgrid via method of lines.\"\"\"\n    import numpy as np\n    from scipy.integrate import solve_ivp\n    from scipy.interpolate import RegularGridInterpolator\n\n    n_x = 256  # interior spatial points for FD discretization\n    x_fd = torch.linspace(-1, 1, n_x + 2).numpy()  # includes boundaries\n    dx = x_fd[1] - x_fd[0]\n\n    # IC: u(x,0) = -sin(pi*x)\n    u0 = -torch.sin(math.pi * torch.tensor(x_fd[1:-1])).numpy()\n\n    t_span = (0.0, 1.0)\n    t_eval = torch.linspace(0, 1, self.grid_size).numpy()\n\n    nu = self.true_nu\n\n    def rhs(_t, u):\n        \"\"\"RHS: du/dt = nu*d2u/dx2 - u*du/dx with homogeneous Dirichlet BCs.\"\"\"\n        # Pad with boundary values (u=0 at x=-1 and x=1)\n        u_pad = torch.zeros(len(u) + 2).numpy()\n        u_pad[1:-1] = u\n\n        # Central differences for d2u/dx2\n        d2u = (u_pad[2:] - 2 * u_pad[1:-1] + u_pad[:-2]) / dx**2\n\n        # Central differences for du/dx\n        du = (u_pad[2:] - u_pad[:-2]) / (2 * dx)\n\n        return nu * d2u - u * du\n\n    sol = solve_ivp(rhs, t_span, u0, t_eval=t_eval, method=\"RK45\", max_step=0.001)\n\n    # sol.y has shape (n_x, len(t_eval))\n    # Interpolate onto output meshgrid\n    x_interior = x_fd[1:-1]\n    t_sol = sol.t\n\n    interp = RegularGridInterpolator(\n        (x_interior, t_sol), sol.y, method=\"linear\", bounds_error=False, fill_value=None\n    )\n\n    # Output meshgrid\n    xs = torch.linspace(-1, 1, self.grid_size)\n    ts = torch.linspace(0, 1, self.grid_size)\n    grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n\n    x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n    pts = np.stack([x_grid[:, 0].numpy(), x_grid[:, 1].numpy()], axis=1)\n    u_ref = torch.tensor(interp(pts), dtype=torch.float32)\n\n    # Add measurement noise\n    u_noisy = u_ref + self.noise_std * torch.randn_like(u_ref)\n\n    # Shape: (N, 1, 1) to match codebase convention\n    y_data = u_noisy.unsqueeze(-1).unsqueeze(1)\n\n    return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/damped_oscillator/","title":"damped_oscillator","text":""},{"location":"reference/anypinn/catalog/damped_oscillator/#anypinn.catalog.damped_oscillator","title":"<code>anypinn.catalog.damped_oscillator</code>","text":""},{"location":"reference/anypinn/catalog/damped_oscillator/#anypinn.catalog.damped_oscillator.OMEGA_KEY","title":"<code>OMEGA_KEY = 'omega0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/damped_oscillator/#anypinn.catalog.damped_oscillator.V_KEY","title":"<code>V_KEY = 'v'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/damped_oscillator/#anypinn.catalog.damped_oscillator.X_KEY","title":"<code>X_KEY = 'x'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/damped_oscillator/#anypinn.catalog.damped_oscillator.ZETA_KEY","title":"<code>ZETA_KEY = 'zeta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/damped_oscillator/#anypinn.catalog.damped_oscillator.DampedOscillatorDataModule","title":"<code>DampedOscillatorDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for damped oscillator inverse problem. Generates synthetic data via odeint.</p> Source code in <code>src/anypinn/catalog/damped_oscillator.py</code> <pre><code>class DampedOscillatorDataModule(PINNDataModule):\n    \"\"\"DataModule for damped oscillator inverse problem. Generates synthetic data via odeint.\"\"\"\n\n    def __init__(\n        self,\n        hp: ODEHyperparameters,\n        gen_props: ODEProperties,\n        noise_std: float = 0.0,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ):\n        super().__init__(hp, validation, callbacks)\n        self.gen_props = gen_props\n        self.noise_std = noise_std\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate synthetic damped oscillator data using odeint + Gaussian noise.\"\"\"\n\n        def oscillator_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n            return self.gen_props.ode(t, y, self.gen_props.args)\n\n        t = config.x\n\n        sol = odeint(oscillator_ode, self.gen_props.y0, t)  # [T, 2]\n        x_true = sol[:, 0]\n\n        x_obs = x_true + self.noise_std * torch.randn_like(x_true)\n\n        return t.unsqueeze(-1), x_obs.unsqueeze(-1).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/catalog/damped_oscillator/#anypinn.catalog.damped_oscillator.DampedOscillatorDataModule.gen_props","title":"<code>gen_props = gen_props</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/damped_oscillator/#anypinn.catalog.damped_oscillator.DampedOscillatorDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/damped_oscillator/#anypinn.catalog.damped_oscillator.DampedOscillatorDataModule.__init__","title":"<code>__init__(hp: ODEHyperparameters, gen_props: ODEProperties, noise_std: float = 0.0, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/damped_oscillator.py</code> <pre><code>def __init__(\n    self,\n    hp: ODEHyperparameters,\n    gen_props: ODEProperties,\n    noise_std: float = 0.0,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n):\n    super().__init__(hp, validation, callbacks)\n    self.gen_props = gen_props\n    self.noise_std = noise_std\n</code></pre>"},{"location":"reference/anypinn/catalog/damped_oscillator/#anypinn.catalog.damped_oscillator.DampedOscillatorDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate synthetic damped oscillator data using odeint + Gaussian noise.</p> Source code in <code>src/anypinn/catalog/damped_oscillator.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate synthetic damped oscillator data using odeint + Gaussian noise.\"\"\"\n\n    def oscillator_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n        return self.gen_props.ode(t, y, self.gen_props.args)\n\n    t = config.x\n\n    sol = odeint(oscillator_ode, self.gen_props.y0, t)  # [T, 2]\n    x_true = sol[:, 0]\n\n    x_obs = x_true + self.noise_std * torch.randn_like(x_true)\n\n    return t.unsqueeze(-1), x_obs.unsqueeze(-1).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/catalog/fitzhugh_nagumo/","title":"fitzhugh_nagumo","text":""},{"location":"reference/anypinn/catalog/fitzhugh_nagumo/#anypinn.catalog.fitzhugh_nagumo","title":"<code>anypinn.catalog.fitzhugh_nagumo</code>","text":""},{"location":"reference/anypinn/catalog/fitzhugh_nagumo/#anypinn.catalog.fitzhugh_nagumo.A_KEY","title":"<code>A_KEY = 'a'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/fitzhugh_nagumo/#anypinn.catalog.fitzhugh_nagumo.EPSILON_KEY","title":"<code>EPSILON_KEY = 'epsilon'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/fitzhugh_nagumo/#anypinn.catalog.fitzhugh_nagumo.V_KEY","title":"<code>V_KEY = 'v'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/fitzhugh_nagumo/#anypinn.catalog.fitzhugh_nagumo.W_KEY","title":"<code>W_KEY = 'w'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/fitzhugh_nagumo/#anypinn.catalog.fitzhugh_nagumo.FitzHughNagumoDataModule","title":"<code>FitzHughNagumoDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for FitzHugh-Nagumo inverse problem.</p> <p>Generates synthetic data via odeint. Only the voltage v is observed; the recovery variable w is inferred through ODE residuals alone.</p> Source code in <code>src/anypinn/catalog/fitzhugh_nagumo.py</code> <pre><code>class FitzHughNagumoDataModule(PINNDataModule):\n    \"\"\"DataModule for FitzHugh-Nagumo inverse problem.\n\n    Generates synthetic data via odeint. Only the voltage v is observed;\n    the recovery variable w is inferred through ODE residuals alone.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: ODEHyperparameters,\n        gen_props: ODEProperties,\n        noise_std: float = 0.0,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ):\n        super().__init__(hp, validation, callbacks)\n        self.gen_props = gen_props\n        self.noise_std = noise_std\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate synthetic FHN data. Returns only v (partially observed).\"\"\"\n\n        def fhn_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n            return self.gen_props.ode(t, y, self.gen_props.args)\n\n        t = config.x\n\n        sol = odeint(fhn_ode, self.gen_props.y0, t)  # [T, 2]\n        v_true = sol[:, 0]\n\n        v_obs = v_true + self.noise_std * torch.randn_like(v_true)\n\n        # (N, 1, 1) \u2014 single observed field\n        y_data = v_obs.unsqueeze(1).unsqueeze(-1)\n\n        return t.unsqueeze(-1), y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/fitzhugh_nagumo/#anypinn.catalog.fitzhugh_nagumo.FitzHughNagumoDataModule.gen_props","title":"<code>gen_props = gen_props</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/fitzhugh_nagumo/#anypinn.catalog.fitzhugh_nagumo.FitzHughNagumoDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/fitzhugh_nagumo/#anypinn.catalog.fitzhugh_nagumo.FitzHughNagumoDataModule.__init__","title":"<code>__init__(hp: ODEHyperparameters, gen_props: ODEProperties, noise_std: float = 0.0, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/fitzhugh_nagumo.py</code> <pre><code>def __init__(\n    self,\n    hp: ODEHyperparameters,\n    gen_props: ODEProperties,\n    noise_std: float = 0.0,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n):\n    super().__init__(hp, validation, callbacks)\n    self.gen_props = gen_props\n    self.noise_std = noise_std\n</code></pre>"},{"location":"reference/anypinn/catalog/fitzhugh_nagumo/#anypinn.catalog.fitzhugh_nagumo.FitzHughNagumoDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate synthetic FHN data. Returns only v (partially observed).</p> Source code in <code>src/anypinn/catalog/fitzhugh_nagumo.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate synthetic FHN data. Returns only v (partially observed).\"\"\"\n\n    def fhn_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n        return self.gen_props.ode(t, y, self.gen_props.args)\n\n    t = config.x\n\n    sol = odeint(fhn_ode, self.gen_props.y0, t)  # [T, 2]\n    v_true = sol[:, 0]\n\n    v_obs = v_true + self.noise_std * torch.randn_like(v_true)\n\n    # (N, 1, 1) \u2014 single observed field\n    y_data = v_obs.unsqueeze(1).unsqueeze(-1)\n\n    return t.unsqueeze(-1), y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/gray_scott_2d/","title":"gray_scott_2d","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d","title":"<code>anypinn.catalog.gray_scott_2d</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.DU_KEY","title":"<code>DU_KEY = 'D_u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.DV_KEY","title":"<code>DV_KEY = 'D_v'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.F_KEY","title":"<code>F_KEY = 'F'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.K_KEY","title":"<code>K_KEY = 'k'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.TRUE_DU","title":"<code>TRUE_DU = 0.005</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.TRUE_DV","title":"<code>TRUE_DV = 0.0025</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.TRUE_F","title":"<code>TRUE_F = 0.04</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.TRUE_K","title":"<code>TRUE_K = 0.06</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.T_TOTAL","title":"<code>T_TOTAL = 200</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.U_KEY","title":"<code>U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.V_KEY","title":"<code>V_KEY = 'v'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.GrayScott2DDataModule","title":"<code>GrayScott2DDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for 2D Gray-Scott reaction-diffusion inverse problem.</p> <p>gen_data produces ground-truth u(x,y,t) and v(x,y,t) via scipy method-of-lines (finite-difference spatial discretization + ODE integration), with optional measurement noise.</p> Source code in <code>src/anypinn/catalog/gray_scott_2d.py</code> <pre><code>class GrayScott2DDataModule(PINNDataModule):\n    \"\"\"DataModule for 2D Gray-Scott reaction-diffusion inverse problem.\n\n    gen_data produces ground-truth u(x,y,t) and v(x,y,t) via scipy\n    method-of-lines (finite-difference spatial discretization + ODE\n    integration), with optional measurement noise.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        true_du: float = TRUE_DU,\n        true_dv: float = TRUE_DV,\n        true_f: float = TRUE_F,\n        true_k: float = TRUE_K,\n        noise_std: float = 0.01,\n        sim_size: int = 64,\n        residual_scorer: ResidualScorer | None = None,\n        validation: ValidationRegistry | None = None,\n        callbacks: list[DataCallback] | None = None,\n    ):\n        self.true_du = true_du\n        self.true_dv = true_dv\n        self.true_f = true_f\n        self.true_k = true_k\n        self.noise_std = noise_std\n        self.sim_size = sim_size\n        super().__init__(hp, validation, callbacks, residual_scorer=residual_scorer)\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate numerical solution on a 3D meshgrid via method of lines.\"\"\"\n        import numpy as np\n        from scipy.integrate import solve_ivp\n        from scipy.interpolate import RegularGridInterpolator\n\n        n = self.sim_size\n        dx = 1.0 / (n - 1)\n        x_fd = np.linspace(0, 1, n)\n\n        # ICs: u=1, v=0 everywhere; center square u=0.5, v=0.25\n        u0 = np.ones((n, n))\n        v0 = np.zeros((n, n))\n        lo = int(0.4 * n)\n        hi = int(0.6 * n)\n        u0[lo:hi, lo:hi] = 0.5\n        v0[lo:hi, lo:hi] = 0.25\n\n        y0 = np.concatenate([u0.ravel(), v0.ravel()])\n\n        du_val = self.true_du\n        dv_val = self.true_dv\n        f_val = self.true_f\n        k_val = self.true_k\n\n        def rhs(_t: float, y: np.ndarray) -&gt; np.ndarray:\n            u = y[: n * n].reshape(n, n)\n            v = y[n * n :].reshape(n, n)\n\n            # 5-point FD Laplacian with Neumann (zero-flux) BCs via padding\n            u_pad = np.pad(u, 1, mode=\"edge\")\n            v_pad = np.pad(v, 1, mode=\"edge\")\n\n            lap_u = (\n                u_pad[2:, 1:-1]\n                + u_pad[:-2, 1:-1]\n                + u_pad[1:-1, 2:]\n                + u_pad[1:-1, :-2]\n                - 4 * u_pad[1:-1, 1:-1]\n            ) / dx**2\n            lap_v = (\n                v_pad[2:, 1:-1]\n                + v_pad[:-2, 1:-1]\n                + v_pad[1:-1, 2:]\n                + v_pad[1:-1, :-2]\n                - 4 * v_pad[1:-1, 1:-1]\n            ) / dx**2\n\n            uv2 = u * v**2\n            du_dt = du_val * lap_u - uv2 + f_val * (1 - u)\n            dv_dt = dv_val * lap_v + uv2 - (f_val + k_val) * v\n\n            return np.concatenate([du_dt.ravel(), dv_dt.ravel()])\n\n        t_span = (0.0, T_TOTAL)\n        n_t_sim = 50\n        t_eval = np.linspace(0, T_TOTAL, n_t_sim)\n\n        sol = solve_ivp(rhs, t_span, y0, t_eval=t_eval, method=\"Radau\", max_step=5.0)\n\n        # sol.y shape: (2*n*n, n_t_sim)\n        u_sol = sol.y[: n * n, :].reshape(n, n, -1)  # (n, n, n_t)\n        v_sol = sol.y[n * n :, :].reshape(n, n, -1)\n\n        # Build interpolators for u and v\n        t_norm_sim = sol.t / T_TOTAL  # normalize to [0, 1]\n        interp_u = RegularGridInterpolator(\n            (x_fd, x_fd, t_norm_sim),\n            u_sol,\n            method=\"linear\",\n            bounds_error=False,\n            fill_value=None,\n        )\n        interp_v = RegularGridInterpolator(\n            (x_fd, x_fd, t_norm_sim),\n            v_sol,\n            method=\"linear\",\n            bounds_error=False,\n            fill_value=None,\n        )\n\n        # Output measurement grid in [0,1]^3\n        n_xy = 20\n        n_t = 10\n        xs = torch.linspace(0, 1, n_xy)\n        ys = torch.linspace(0, 1, n_xy)\n        ts = torch.linspace(0, 1, n_t)\n        gx, gy, gt = torch.meshgrid(xs, ys, ts, indexing=\"ij\")\n        x_grid = torch.stack([gx.reshape(-1), gy.reshape(-1), gt.reshape(-1)], dim=1)\n\n        pts = x_grid.numpy()\n        u_ref = torch.tensor(interp_u(pts), dtype=torch.float32)\n        v_ref = torch.tensor(interp_v(pts), dtype=torch.float32)\n\n        # Add measurement noise\n        u_noisy = u_ref + self.noise_std * torch.randn_like(u_ref)\n        v_noisy = v_ref + self.noise_std * torch.randn_like(v_ref)\n\n        # Shape: (N, 2, 1)\n        y_data = torch.stack([u_noisy, v_noisy], dim=1).unsqueeze(-1)\n\n        return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.GrayScott2DDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.GrayScott2DDataModule.sim_size","title":"<code>sim_size = sim_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.GrayScott2DDataModule.true_du","title":"<code>true_du = true_du</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.GrayScott2DDataModule.true_dv","title":"<code>true_dv = true_dv</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.GrayScott2DDataModule.true_f","title":"<code>true_f = true_f</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.GrayScott2DDataModule.true_k","title":"<code>true_k = true_k</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.GrayScott2DDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, true_du: float = TRUE_DU, true_dv: float = TRUE_DV, true_f: float = TRUE_F, true_k: float = TRUE_K, noise_std: float = 0.01, sim_size: int = 64, residual_scorer: ResidualScorer | None = None, validation: ValidationRegistry | None = None, callbacks: list[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/gray_scott_2d.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    true_du: float = TRUE_DU,\n    true_dv: float = TRUE_DV,\n    true_f: float = TRUE_F,\n    true_k: float = TRUE_K,\n    noise_std: float = 0.01,\n    sim_size: int = 64,\n    residual_scorer: ResidualScorer | None = None,\n    validation: ValidationRegistry | None = None,\n    callbacks: list[DataCallback] | None = None,\n):\n    self.true_du = true_du\n    self.true_dv = true_dv\n    self.true_f = true_f\n    self.true_k = true_k\n    self.noise_std = noise_std\n    self.sim_size = sim_size\n    super().__init__(hp, validation, callbacks, residual_scorer=residual_scorer)\n</code></pre>"},{"location":"reference/anypinn/catalog/gray_scott_2d/#anypinn.catalog.gray_scott_2d.GrayScott2DDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate numerical solution on a 3D meshgrid via method of lines.</p> Source code in <code>src/anypinn/catalog/gray_scott_2d.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate numerical solution on a 3D meshgrid via method of lines.\"\"\"\n    import numpy as np\n    from scipy.integrate import solve_ivp\n    from scipy.interpolate import RegularGridInterpolator\n\n    n = self.sim_size\n    dx = 1.0 / (n - 1)\n    x_fd = np.linspace(0, 1, n)\n\n    # ICs: u=1, v=0 everywhere; center square u=0.5, v=0.25\n    u0 = np.ones((n, n))\n    v0 = np.zeros((n, n))\n    lo = int(0.4 * n)\n    hi = int(0.6 * n)\n    u0[lo:hi, lo:hi] = 0.5\n    v0[lo:hi, lo:hi] = 0.25\n\n    y0 = np.concatenate([u0.ravel(), v0.ravel()])\n\n    du_val = self.true_du\n    dv_val = self.true_dv\n    f_val = self.true_f\n    k_val = self.true_k\n\n    def rhs(_t: float, y: np.ndarray) -&gt; np.ndarray:\n        u = y[: n * n].reshape(n, n)\n        v = y[n * n :].reshape(n, n)\n\n        # 5-point FD Laplacian with Neumann (zero-flux) BCs via padding\n        u_pad = np.pad(u, 1, mode=\"edge\")\n        v_pad = np.pad(v, 1, mode=\"edge\")\n\n        lap_u = (\n            u_pad[2:, 1:-1]\n            + u_pad[:-2, 1:-1]\n            + u_pad[1:-1, 2:]\n            + u_pad[1:-1, :-2]\n            - 4 * u_pad[1:-1, 1:-1]\n        ) / dx**2\n        lap_v = (\n            v_pad[2:, 1:-1]\n            + v_pad[:-2, 1:-1]\n            + v_pad[1:-1, 2:]\n            + v_pad[1:-1, :-2]\n            - 4 * v_pad[1:-1, 1:-1]\n        ) / dx**2\n\n        uv2 = u * v**2\n        du_dt = du_val * lap_u - uv2 + f_val * (1 - u)\n        dv_dt = dv_val * lap_v + uv2 - (f_val + k_val) * v\n\n        return np.concatenate([du_dt.ravel(), dv_dt.ravel()])\n\n    t_span = (0.0, T_TOTAL)\n    n_t_sim = 50\n    t_eval = np.linspace(0, T_TOTAL, n_t_sim)\n\n    sol = solve_ivp(rhs, t_span, y0, t_eval=t_eval, method=\"Radau\", max_step=5.0)\n\n    # sol.y shape: (2*n*n, n_t_sim)\n    u_sol = sol.y[: n * n, :].reshape(n, n, -1)  # (n, n, n_t)\n    v_sol = sol.y[n * n :, :].reshape(n, n, -1)\n\n    # Build interpolators for u and v\n    t_norm_sim = sol.t / T_TOTAL  # normalize to [0, 1]\n    interp_u = RegularGridInterpolator(\n        (x_fd, x_fd, t_norm_sim),\n        u_sol,\n        method=\"linear\",\n        bounds_error=False,\n        fill_value=None,\n    )\n    interp_v = RegularGridInterpolator(\n        (x_fd, x_fd, t_norm_sim),\n        v_sol,\n        method=\"linear\",\n        bounds_error=False,\n        fill_value=None,\n    )\n\n    # Output measurement grid in [0,1]^3\n    n_xy = 20\n    n_t = 10\n    xs = torch.linspace(0, 1, n_xy)\n    ys = torch.linspace(0, 1, n_xy)\n    ts = torch.linspace(0, 1, n_t)\n    gx, gy, gt = torch.meshgrid(xs, ys, ts, indexing=\"ij\")\n    x_grid = torch.stack([gx.reshape(-1), gy.reshape(-1), gt.reshape(-1)], dim=1)\n\n    pts = x_grid.numpy()\n    u_ref = torch.tensor(interp_u(pts), dtype=torch.float32)\n    v_ref = torch.tensor(interp_v(pts), dtype=torch.float32)\n\n    # Add measurement noise\n    u_noisy = u_ref + self.noise_std * torch.randn_like(u_ref)\n    v_noisy = v_ref + self.noise_std * torch.randn_like(v_ref)\n\n    # Shape: (N, 2, 1)\n    y_data = torch.stack([u_noisy, v_noisy], dim=1).unsqueeze(-1)\n\n    return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/heat_1d/","title":"heat_1d","text":""},{"location":"reference/anypinn/catalog/heat_1d/#anypinn.catalog.heat_1d","title":"<code>anypinn.catalog.heat_1d</code>","text":""},{"location":"reference/anypinn/catalog/heat_1d/#anypinn.catalog.heat_1d.ALPHA_KEY","title":"<code>ALPHA_KEY = 'alpha'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/heat_1d/#anypinn.catalog.heat_1d.TRUE_ALPHA","title":"<code>TRUE_ALPHA = 0.1</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/heat_1d/#anypinn.catalog.heat_1d.U_KEY","title":"<code>U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/heat_1d/#anypinn.catalog.heat_1d.Heat1DDataModule","title":"<code>Heat1DDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for 1D heat equation inverse problem.</p> <p>gen_data produces sparse interior measurements from the analytic solution u(x,t) = exp(-alpha pi^2 t) sin(pi x), with optional noise. These measurements are used by DataConstraint during training to recover alpha.</p> Source code in <code>src/anypinn/catalog/heat_1d.py</code> <pre><code>class Heat1DDataModule(PINNDataModule):\n    \"\"\"DataModule for 1D heat equation inverse problem.\n\n    gen_data produces sparse interior measurements from the analytic\n    solution u(x,t) = exp(-alpha pi^2 t) sin(pi x), with optional noise.\n    These measurements are used by DataConstraint during training\n    to recover alpha.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        true_alpha: float = TRUE_ALPHA,\n        n_measurements: int = 200,\n        noise_std: float = 0.01,\n        grid_size: int = 50,\n        validation: ValidationRegistry | None = None,\n        callbacks: list[DataCallback] | None = None,\n    ):\n        self.true_alpha = true_alpha\n        self.n_measurements = n_measurements\n        self.noise_std = noise_std\n        self.grid_size = grid_size\n        super().__init__(hp, validation, callbacks)\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate analytic solution on a 2D meshgrid for training + prediction.\"\"\"\n        xs = torch.linspace(0, 1, self.grid_size)\n        ts = torch.linspace(0, 1, self.grid_size)\n        grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n\n        x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n        u_analytic = torch.exp(-self.true_alpha * math.pi**2 * x_grid[:, 1]) * torch.sin(\n            math.pi * x_grid[:, 0]\n        )\n\n        # Add measurement noise\n        u_noisy = u_analytic + self.noise_std * torch.randn_like(u_analytic)\n\n        # Shape: (N, 1, 1) to match codebase convention\n        y_data = u_noisy.unsqueeze(-1).unsqueeze(1)\n\n        return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/heat_1d/#anypinn.catalog.heat_1d.Heat1DDataModule.grid_size","title":"<code>grid_size = grid_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/heat_1d/#anypinn.catalog.heat_1d.Heat1DDataModule.n_measurements","title":"<code>n_measurements = n_measurements</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/heat_1d/#anypinn.catalog.heat_1d.Heat1DDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/heat_1d/#anypinn.catalog.heat_1d.Heat1DDataModule.true_alpha","title":"<code>true_alpha = true_alpha</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/heat_1d/#anypinn.catalog.heat_1d.Heat1DDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, true_alpha: float = TRUE_ALPHA, n_measurements: int = 200, noise_std: float = 0.01, grid_size: int = 50, validation: ValidationRegistry | None = None, callbacks: list[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/heat_1d.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    true_alpha: float = TRUE_ALPHA,\n    n_measurements: int = 200,\n    noise_std: float = 0.01,\n    grid_size: int = 50,\n    validation: ValidationRegistry | None = None,\n    callbacks: list[DataCallback] | None = None,\n):\n    self.true_alpha = true_alpha\n    self.n_measurements = n_measurements\n    self.noise_std = noise_std\n    self.grid_size = grid_size\n    super().__init__(hp, validation, callbacks)\n</code></pre>"},{"location":"reference/anypinn/catalog/heat_1d/#anypinn.catalog.heat_1d.Heat1DDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate analytic solution on a 2D meshgrid for training + prediction.</p> Source code in <code>src/anypinn/catalog/heat_1d.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate analytic solution on a 2D meshgrid for training + prediction.\"\"\"\n    xs = torch.linspace(0, 1, self.grid_size)\n    ts = torch.linspace(0, 1, self.grid_size)\n    grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n\n    x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n    u_analytic = torch.exp(-self.true_alpha * math.pi**2 * x_grid[:, 1]) * torch.sin(\n        math.pi * x_grid[:, 0]\n    )\n\n    # Add measurement noise\n    u_noisy = u_analytic + self.noise_std * torch.randn_like(u_analytic)\n\n    # Shape: (N, 1, 1) to match codebase convention\n    y_data = u_noisy.unsqueeze(-1).unsqueeze(1)\n\n    return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/inverse_diffusivity/","title":"inverse_diffusivity","text":""},{"location":"reference/anypinn/catalog/inverse_diffusivity/#anypinn.catalog.inverse_diffusivity","title":"<code>anypinn.catalog.inverse_diffusivity</code>","text":""},{"location":"reference/anypinn/catalog/inverse_diffusivity/#anypinn.catalog.inverse_diffusivity.D_KEY","title":"<code>D_KEY = 'D'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/inverse_diffusivity/#anypinn.catalog.inverse_diffusivity.TRUE_D_FN","title":"<code>TRUE_D_FN: Callable[[Tensor], Tensor] = true_d_fn</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/inverse_diffusivity/#anypinn.catalog.inverse_diffusivity.U_KEY","title":"<code>U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/inverse_diffusivity/#anypinn.catalog.inverse_diffusivity.InverseDiffusivityDataModule","title":"<code>InverseDiffusivityDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for 1D inverse diffusivity problem.</p> <p>gen_data produces ground-truth u(x,t) via scipy method-of-lines (central differences with variable D(x), integrated with solve_ivp), with optional measurement noise.</p> Source code in <code>src/anypinn/catalog/inverse_diffusivity.py</code> <pre><code>class InverseDiffusivityDataModule(PINNDataModule):\n    \"\"\"DataModule for 1D inverse diffusivity problem.\n\n    gen_data produces ground-truth u(x,t) via scipy method-of-lines\n    (central differences with variable D(x), integrated with solve_ivp),\n    with optional measurement noise.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        n_x: int = 80,\n        n_t: int = 80,\n        noise_std: float = 0.01,\n        grid_size: int = 50,\n        validation: ValidationRegistry | None = None,\n        callbacks: list[DataCallback] | None = None,\n    ):\n        self.n_x = n_x\n        self.n_t = n_t\n        self.noise_std = noise_std\n        self.grid_size = grid_size\n        super().__init__(hp, validation, callbacks)\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate numerical solution on a 2D meshgrid via method of lines.\"\"\"\n        import numpy as np\n        from scipy.integrate import solve_ivp\n        from scipy.interpolate import RegularGridInterpolator\n\n        n_x = self.n_x\n        dx = 1.0 / (n_x - 1)\n        x_fd = np.linspace(0, 1, n_x)\n\n        # True D(x) at grid points\n        d_vals = 0.1 + 0.05 * np.sin(2 * np.pi * x_fd)\n\n        # IC: u(x,0) = sin(pi*x)\n        u0 = np.sin(np.pi * x_fd)\n        # Enforce Dirichlet BCs\n        u0[0] = 0.0\n        u0[-1] = 0.0\n\n        def rhs(_t: float, u: np.ndarray) -&gt; np.ndarray:\n            du_dt = np.zeros_like(u)\n            # Interior points: d/dx(D(x) du/dx) via central differences\n            for i in range(1, n_x - 1):\n                # D at half-points\n                d_right = 0.5 * (d_vals[i] + d_vals[i + 1])\n                d_left = 0.5 * (d_vals[i - 1] + d_vals[i])\n                du_dt[i] = (d_right * (u[i + 1] - u[i]) - d_left * (u[i] - u[i - 1])) / dx**2\n            # BCs: u(0,t) = u(1,t) = 0 =&gt; du_dt = 0 at boundaries\n            return du_dt\n\n        t_span = (0.0, 1.0)\n        t_eval = np.linspace(0, 1, self.n_t)\n\n        sol = solve_ivp(rhs, t_span, u0, t_eval=t_eval, method=\"Radau\", max_step=0.05)\n\n        # sol.y shape: (n_x, n_t_actual)\n        u_sol = sol.y  # (n_x, n_t)\n\n        # Build interpolator\n        interp_u = RegularGridInterpolator(\n            (x_fd, sol.t),\n            u_sol,\n            method=\"linear\",\n            bounds_error=False,\n            fill_value=None,\n        )\n\n        # Output measurement grid\n        xs = torch.linspace(0, 1, self.grid_size)\n        ts = torch.linspace(0, 1, self.grid_size)\n        grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n        x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n        pts = x_grid.numpy()\n        u_ref = torch.tensor(interp_u(pts), dtype=torch.float32)\n\n        # Add measurement noise\n        u_noisy = u_ref + self.noise_std * torch.randn_like(u_ref)\n\n        # Shape: (N, 1, 1) to match codebase convention\n        y_data = u_noisy.unsqueeze(-1).unsqueeze(1)\n\n        return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/inverse_diffusivity/#anypinn.catalog.inverse_diffusivity.InverseDiffusivityDataModule.grid_size","title":"<code>grid_size = grid_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/inverse_diffusivity/#anypinn.catalog.inverse_diffusivity.InverseDiffusivityDataModule.n_t","title":"<code>n_t = n_t</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/inverse_diffusivity/#anypinn.catalog.inverse_diffusivity.InverseDiffusivityDataModule.n_x","title":"<code>n_x = n_x</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/inverse_diffusivity/#anypinn.catalog.inverse_diffusivity.InverseDiffusivityDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/inverse_diffusivity/#anypinn.catalog.inverse_diffusivity.InverseDiffusivityDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, n_x: int = 80, n_t: int = 80, noise_std: float = 0.01, grid_size: int = 50, validation: ValidationRegistry | None = None, callbacks: list[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/inverse_diffusivity.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    n_x: int = 80,\n    n_t: int = 80,\n    noise_std: float = 0.01,\n    grid_size: int = 50,\n    validation: ValidationRegistry | None = None,\n    callbacks: list[DataCallback] | None = None,\n):\n    self.n_x = n_x\n    self.n_t = n_t\n    self.noise_std = noise_std\n    self.grid_size = grid_size\n    super().__init__(hp, validation, callbacks)\n</code></pre>"},{"location":"reference/anypinn/catalog/inverse_diffusivity/#anypinn.catalog.inverse_diffusivity.InverseDiffusivityDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate numerical solution on a 2D meshgrid via method of lines.</p> Source code in <code>src/anypinn/catalog/inverse_diffusivity.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate numerical solution on a 2D meshgrid via method of lines.\"\"\"\n    import numpy as np\n    from scipy.integrate import solve_ivp\n    from scipy.interpolate import RegularGridInterpolator\n\n    n_x = self.n_x\n    dx = 1.0 / (n_x - 1)\n    x_fd = np.linspace(0, 1, n_x)\n\n    # True D(x) at grid points\n    d_vals = 0.1 + 0.05 * np.sin(2 * np.pi * x_fd)\n\n    # IC: u(x,0) = sin(pi*x)\n    u0 = np.sin(np.pi * x_fd)\n    # Enforce Dirichlet BCs\n    u0[0] = 0.0\n    u0[-1] = 0.0\n\n    def rhs(_t: float, u: np.ndarray) -&gt; np.ndarray:\n        du_dt = np.zeros_like(u)\n        # Interior points: d/dx(D(x) du/dx) via central differences\n        for i in range(1, n_x - 1):\n            # D at half-points\n            d_right = 0.5 * (d_vals[i] + d_vals[i + 1])\n            d_left = 0.5 * (d_vals[i - 1] + d_vals[i])\n            du_dt[i] = (d_right * (u[i + 1] - u[i]) - d_left * (u[i] - u[i - 1])) / dx**2\n        # BCs: u(0,t) = u(1,t) = 0 =&gt; du_dt = 0 at boundaries\n        return du_dt\n\n    t_span = (0.0, 1.0)\n    t_eval = np.linspace(0, 1, self.n_t)\n\n    sol = solve_ivp(rhs, t_span, u0, t_eval=t_eval, method=\"Radau\", max_step=0.05)\n\n    # sol.y shape: (n_x, n_t_actual)\n    u_sol = sol.y  # (n_x, n_t)\n\n    # Build interpolator\n    interp_u = RegularGridInterpolator(\n        (x_fd, sol.t),\n        u_sol,\n        method=\"linear\",\n        bounds_error=False,\n        fill_value=None,\n    )\n\n    # Output measurement grid\n    xs = torch.linspace(0, 1, self.grid_size)\n    ts = torch.linspace(0, 1, self.grid_size)\n    grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n    x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n    pts = x_grid.numpy()\n    u_ref = torch.tensor(interp_u(pts), dtype=torch.float32)\n\n    # Add measurement noise\n    u_noisy = u_ref + self.noise_std * torch.randn_like(u_ref)\n\n    # Shape: (N, 1, 1) to match codebase convention\n    y_data = u_noisy.unsqueeze(-1).unsqueeze(1)\n\n    return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/inverse_diffusivity/#anypinn.catalog.inverse_diffusivity.true_d_fn","title":"<code>true_d_fn(x: Tensor) -&gt; Tensor</code>","text":"<p>True diffusivity profile: D(x) = 0.1 + 0.05 sin(2 pi x).</p> Source code in <code>src/anypinn/catalog/inverse_diffusivity.py</code> <pre><code>def true_d_fn(x: Tensor) -&gt; Tensor:\n    \"\"\"True diffusivity profile: D(x) = 0.1 + 0.05 sin(2 pi x).\"\"\"\n    return 0.1 + 0.05 * torch.sin(2 * math.pi * x)\n</code></pre>"},{"location":"reference/anypinn/catalog/lorenz/","title":"lorenz","text":""},{"location":"reference/anypinn/catalog/lorenz/#anypinn.catalog.lorenz","title":"<code>anypinn.catalog.lorenz</code>","text":""},{"location":"reference/anypinn/catalog/lorenz/#anypinn.catalog.lorenz.BETA_KEY","title":"<code>BETA_KEY = 'beta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/lorenz/#anypinn.catalog.lorenz.RHO_KEY","title":"<code>RHO_KEY = 'rho'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/lorenz/#anypinn.catalog.lorenz.SIGMA_KEY","title":"<code>SIGMA_KEY = 'sigma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/lorenz/#anypinn.catalog.lorenz.X_KEY","title":"<code>X_KEY = 'x'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/lorenz/#anypinn.catalog.lorenz.Y_KEY","title":"<code>Y_KEY = 'y'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/lorenz/#anypinn.catalog.lorenz.Z_KEY","title":"<code>Z_KEY = 'z'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/lorenz/#anypinn.catalog.lorenz.LorenzDataModule","title":"<code>LorenzDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for Lorenz system inverse problem. Generates synthetic data via odeint.</p> Source code in <code>src/anypinn/catalog/lorenz.py</code> <pre><code>class LorenzDataModule(PINNDataModule):\n    \"\"\"DataModule for Lorenz system inverse problem. Generates synthetic data via odeint.\"\"\"\n\n    def __init__(\n        self,\n        hp: ODEHyperparameters,\n        gen_props: ODEProperties,\n        noise_std: float = 0.0,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ):\n        super().__init__(hp, validation, callbacks)\n        self.gen_props = gen_props\n        self.noise_std = noise_std\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate synthetic Lorenz data using odeint + additive Gaussian noise.\"\"\"\n\n        def lorenz_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n            return self.gen_props.ode(t, y, self.gen_props.args)\n\n        t = config.x\n\n        sol = odeint(lorenz_ode, self.gen_props.y0, t)  # [T, 3]\n        x_true = sol[:, 0]\n        y_true = sol[:, 1]\n        z_true = sol[:, 2]\n\n        x_obs = x_true + self.noise_std * torch.randn_like(x_true)\n        y_obs = y_true + self.noise_std * torch.randn_like(y_true)\n        z_obs = z_true + self.noise_std * torch.randn_like(z_true)\n\n        y_data = torch.stack([x_obs, y_obs, z_obs], dim=1).unsqueeze(-1)\n\n        return t.unsqueeze(-1), y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/lorenz/#anypinn.catalog.lorenz.LorenzDataModule.gen_props","title":"<code>gen_props = gen_props</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/lorenz/#anypinn.catalog.lorenz.LorenzDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/lorenz/#anypinn.catalog.lorenz.LorenzDataModule.__init__","title":"<code>__init__(hp: ODEHyperparameters, gen_props: ODEProperties, noise_std: float = 0.0, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/lorenz.py</code> <pre><code>def __init__(\n    self,\n    hp: ODEHyperparameters,\n    gen_props: ODEProperties,\n    noise_std: float = 0.0,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n):\n    super().__init__(hp, validation, callbacks)\n    self.gen_props = gen_props\n    self.noise_std = noise_std\n</code></pre>"},{"location":"reference/anypinn/catalog/lorenz/#anypinn.catalog.lorenz.LorenzDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate synthetic Lorenz data using odeint + additive Gaussian noise.</p> Source code in <code>src/anypinn/catalog/lorenz.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate synthetic Lorenz data using odeint + additive Gaussian noise.\"\"\"\n\n    def lorenz_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n        return self.gen_props.ode(t, y, self.gen_props.args)\n\n    t = config.x\n\n    sol = odeint(lorenz_ode, self.gen_props.y0, t)  # [T, 3]\n    x_true = sol[:, 0]\n    y_true = sol[:, 1]\n    z_true = sol[:, 2]\n\n    x_obs = x_true + self.noise_std * torch.randn_like(x_true)\n    y_obs = y_true + self.noise_std * torch.randn_like(y_true)\n    z_obs = z_true + self.noise_std * torch.randn_like(z_true)\n\n    y_data = torch.stack([x_obs, y_obs, z_obs], dim=1).unsqueeze(-1)\n\n    return t.unsqueeze(-1), y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/lotka_volterra/","title":"lotka_volterra","text":""},{"location":"reference/anypinn/catalog/lotka_volterra/#anypinn.catalog.lotka_volterra","title":"<code>anypinn.catalog.lotka_volterra</code>","text":""},{"location":"reference/anypinn/catalog/lotka_volterra/#anypinn.catalog.lotka_volterra.ALPHA_KEY","title":"<code>ALPHA_KEY = 'alpha'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/lotka_volterra/#anypinn.catalog.lotka_volterra.BETA_KEY","title":"<code>BETA_KEY = 'beta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/lotka_volterra/#anypinn.catalog.lotka_volterra.DELTA_KEY","title":"<code>DELTA_KEY = 'delta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/lotka_volterra/#anypinn.catalog.lotka_volterra.GAMMA_KEY","title":"<code>GAMMA_KEY = 'gamma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/lotka_volterra/#anypinn.catalog.lotka_volterra.X_KEY","title":"<code>X_KEY = 'x'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/lotka_volterra/#anypinn.catalog.lotka_volterra.Y_KEY","title":"<code>Y_KEY = 'y'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/lotka_volterra/#anypinn.catalog.lotka_volterra.LotkaVolterraDataModule","title":"<code>LotkaVolterraDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for Lotka-Volterra inverse problem. Generates synthetic data via odeint.</p> Source code in <code>src/anypinn/catalog/lotka_volterra.py</code> <pre><code>class LotkaVolterraDataModule(PINNDataModule):\n    \"\"\"DataModule for Lotka-Volterra inverse problem. Generates synthetic data via odeint.\"\"\"\n\n    def __init__(\n        self,\n        hp: ODEHyperparameters,\n        gen_props: ODEProperties,\n        noise_frac: float = 0.0,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ):\n        super().__init__(hp, validation, callbacks)\n        self.gen_props = gen_props\n        self.noise_frac = noise_frac\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate synthetic Lotka-Volterra data using odeint + Gaussian noise.\"\"\"\n\n        def lv_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n            return self.gen_props.ode(t, y, self.gen_props.args)\n\n        t = config.x\n\n        sol = odeint(lv_ode, self.gen_props.y0, t)  # [T, 2]\n        x_true = sol[:, 0].clamp_min(0.0)\n        y_true = sol[:, 1].clamp_min(0.0)\n\n        x_obs = x_true + self.noise_frac * x_true.abs().mean() * torch.randn_like(x_true)\n        y_obs = y_true + self.noise_frac * y_true.abs().mean() * torch.randn_like(y_true)\n        x_obs = x_obs.clamp_min(0.0)\n        y_obs = y_obs.clamp_min(0.0)\n\n        y_data = torch.stack([x_obs, y_obs], dim=1).unsqueeze(-1)\n\n        return t.unsqueeze(-1), y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/lotka_volterra/#anypinn.catalog.lotka_volterra.LotkaVolterraDataModule.gen_props","title":"<code>gen_props = gen_props</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/lotka_volterra/#anypinn.catalog.lotka_volterra.LotkaVolterraDataModule.noise_frac","title":"<code>noise_frac = noise_frac</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/lotka_volterra/#anypinn.catalog.lotka_volterra.LotkaVolterraDataModule.__init__","title":"<code>__init__(hp: ODEHyperparameters, gen_props: ODEProperties, noise_frac: float = 0.0, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/lotka_volterra.py</code> <pre><code>def __init__(\n    self,\n    hp: ODEHyperparameters,\n    gen_props: ODEProperties,\n    noise_frac: float = 0.0,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n):\n    super().__init__(hp, validation, callbacks)\n    self.gen_props = gen_props\n    self.noise_frac = noise_frac\n</code></pre>"},{"location":"reference/anypinn/catalog/lotka_volterra/#anypinn.catalog.lotka_volterra.LotkaVolterraDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate synthetic Lotka-Volterra data using odeint + Gaussian noise.</p> Source code in <code>src/anypinn/catalog/lotka_volterra.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate synthetic Lotka-Volterra data using odeint + Gaussian noise.\"\"\"\n\n    def lv_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n        return self.gen_props.ode(t, y, self.gen_props.args)\n\n    t = config.x\n\n    sol = odeint(lv_ode, self.gen_props.y0, t)  # [T, 2]\n    x_true = sol[:, 0].clamp_min(0.0)\n    y_true = sol[:, 1].clamp_min(0.0)\n\n    x_obs = x_true + self.noise_frac * x_true.abs().mean() * torch.randn_like(x_true)\n    y_obs = y_true + self.noise_frac * y_true.abs().mean() * torch.randn_like(y_true)\n    x_obs = x_obs.clamp_min(0.0)\n    y_obs = y_obs.clamp_min(0.0)\n\n    y_data = torch.stack([x_obs, y_obs], dim=1).unsqueeze(-1)\n\n    return t.unsqueeze(-1), y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/poisson_2d/","title":"poisson_2d","text":""},{"location":"reference/anypinn/catalog/poisson_2d/#anypinn.catalog.poisson_2d","title":"<code>anypinn.catalog.poisson_2d</code>","text":""},{"location":"reference/anypinn/catalog/poisson_2d/#anypinn.catalog.poisson_2d.U_KEY","title":"<code>U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/poisson_2d/#anypinn.catalog.poisson_2d.Poisson2DDataModule","title":"<code>Poisson2DDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for 2D Poisson equation on [0,1]^2.</p> <p>Generates a meshgrid with the analytic solution u(x,y) = sin(pix)sin(pi*y). The data is used for prediction/validation only -- training uses PDEResidualConstraint + DirichletBCConstraints (no DataConstraint).</p> Source code in <code>src/anypinn/catalog/poisson_2d.py</code> <pre><code>class Poisson2DDataModule(PINNDataModule):\n    \"\"\"DataModule for 2D Poisson equation on [0,1]^2.\n\n    Generates a meshgrid with the analytic solution u(x,y) = sin(pi*x)*sin(pi*y).\n    The data is used for prediction/validation only -- training uses\n    PDEResidualConstraint + DirichletBCConstraints (no DataConstraint).\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        grid_size: int = 30,\n        validation: ValidationRegistry | None = None,\n        callbacks: list[DataCallback] | None = None,\n    ):\n        self.grid_size = grid_size\n        super().__init__(hp, validation, callbacks)\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate analytic solution on a 2D meshgrid for prediction.\"\"\"\n        xs = torch.linspace(0, 1, self.grid_size)\n        ys = torch.linspace(0, 1, self.grid_size)\n        grid_x, grid_y = torch.meshgrid(xs, ys, indexing=\"ij\")\n\n        x_grid = torch.stack([grid_x.reshape(-1), grid_y.reshape(-1)], dim=1)  # (N, 2)\n\n        u_analytic = torch.sin(math.pi * x_grid[:, 0]) * torch.sin(math.pi * x_grid[:, 1])\n\n        # Shape: (N, 1, 1) to match codebase convention\n        y_data = u_analytic.unsqueeze(-1).unsqueeze(1)\n\n        return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/poisson_2d/#anypinn.catalog.poisson_2d.Poisson2DDataModule.grid_size","title":"<code>grid_size = grid_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/poisson_2d/#anypinn.catalog.poisson_2d.Poisson2DDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, grid_size: int = 30, validation: ValidationRegistry | None = None, callbacks: list[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/poisson_2d.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    grid_size: int = 30,\n    validation: ValidationRegistry | None = None,\n    callbacks: list[DataCallback] | None = None,\n):\n    self.grid_size = grid_size\n    super().__init__(hp, validation, callbacks)\n</code></pre>"},{"location":"reference/anypinn/catalog/poisson_2d/#anypinn.catalog.poisson_2d.Poisson2DDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate analytic solution on a 2D meshgrid for prediction.</p> Source code in <code>src/anypinn/catalog/poisson_2d.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate analytic solution on a 2D meshgrid for prediction.\"\"\"\n    xs = torch.linspace(0, 1, self.grid_size)\n    ys = torch.linspace(0, 1, self.grid_size)\n    grid_x, grid_y = torch.meshgrid(xs, ys, indexing=\"ij\")\n\n    x_grid = torch.stack([grid_x.reshape(-1), grid_y.reshape(-1)], dim=1)  # (N, 2)\n\n    u_analytic = torch.sin(math.pi * x_grid[:, 0]) * torch.sin(math.pi * x_grid[:, 1])\n\n    # Shape: (N, 1, 1) to match codebase convention\n    y_data = u_analytic.unsqueeze(-1).unsqueeze(1)\n\n    return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/seir/","title":"seir","text":""},{"location":"reference/anypinn/catalog/seir/#anypinn.catalog.seir","title":"<code>anypinn.catalog.seir</code>","text":""},{"location":"reference/anypinn/catalog/seir/#anypinn.catalog.seir.BETA_KEY","title":"<code>BETA_KEY = 'beta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/seir/#anypinn.catalog.seir.E_KEY","title":"<code>E_KEY = 'E'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/seir/#anypinn.catalog.seir.GAMMA_KEY","title":"<code>GAMMA_KEY = 'gamma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/seir/#anypinn.catalog.seir.I_KEY","title":"<code>I_KEY = 'I'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/seir/#anypinn.catalog.seir.SIGMA_KEY","title":"<code>SIGMA_KEY = 'sigma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/seir/#anypinn.catalog.seir.S_KEY","title":"<code>S_KEY = 'S'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/seir/#anypinn.catalog.seir.SEIRDataModule","title":"<code>SEIRDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for SEIR inverse problem. Generates synthetic data via odeint.</p> Source code in <code>src/anypinn/catalog/seir.py</code> <pre><code>class SEIRDataModule(PINNDataModule):\n    \"\"\"DataModule for SEIR inverse problem. Generates synthetic data via odeint.\"\"\"\n\n    def __init__(\n        self,\n        hp: ODEHyperparameters,\n        gen_props: ODEProperties,\n        noise_std: float = 0.0,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ):\n        super().__init__(hp, validation, callbacks)\n        self.gen_props = gen_props\n        self.noise_std = noise_std\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate synthetic SEIR data using odeint + Gaussian noise.\"\"\"\n\n        def seir_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n            return self.gen_props.ode(t, y, self.gen_props.args)\n\n        t = config.x\n\n        sol = odeint(seir_ode, self.gen_props.y0, t)  # [T, 3]\n        I_true = sol[:, 2].clamp_min(0.0)\n\n        I_obs = I_true + self.noise_std * torch.randn_like(I_true)\n        I_obs = I_obs.clamp_min(0.0)\n\n        return t.unsqueeze(-1), I_obs.unsqueeze(-1).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/catalog/seir/#anypinn.catalog.seir.SEIRDataModule.gen_props","title":"<code>gen_props = gen_props</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/seir/#anypinn.catalog.seir.SEIRDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/seir/#anypinn.catalog.seir.SEIRDataModule.__init__","title":"<code>__init__(hp: ODEHyperparameters, gen_props: ODEProperties, noise_std: float = 0.0, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/seir.py</code> <pre><code>def __init__(\n    self,\n    hp: ODEHyperparameters,\n    gen_props: ODEProperties,\n    noise_std: float = 0.0,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n):\n    super().__init__(hp, validation, callbacks)\n    self.gen_props = gen_props\n    self.noise_std = noise_std\n</code></pre>"},{"location":"reference/anypinn/catalog/seir/#anypinn.catalog.seir.SEIRDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate synthetic SEIR data using odeint + Gaussian noise.</p> Source code in <code>src/anypinn/catalog/seir.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate synthetic SEIR data using odeint + Gaussian noise.\"\"\"\n\n    def seir_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n        return self.gen_props.ode(t, y, self.gen_props.args)\n\n    t = config.x\n\n    sol = odeint(seir_ode, self.gen_props.y0, t)  # [T, 3]\n    I_true = sol[:, 2].clamp_min(0.0)\n\n    I_obs = I_true + self.noise_std * torch.randn_like(I_true)\n    I_obs = I_obs.clamp_min(0.0)\n\n    return t.unsqueeze(-1), I_obs.unsqueeze(-1).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/catalog/sir/","title":"sir","text":""},{"location":"reference/anypinn/catalog/sir/#anypinn.catalog.sir","title":"<code>anypinn.catalog.sir</code>","text":""},{"location":"reference/anypinn/catalog/sir/#anypinn.catalog.sir.BETA_KEY","title":"<code>BETA_KEY = 'beta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/sir/#anypinn.catalog.sir.DELTA_KEY","title":"<code>DELTA_KEY = 'delta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/sir/#anypinn.catalog.sir.I_KEY","title":"<code>I_KEY = 'I'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/sir/#anypinn.catalog.sir.N_KEY","title":"<code>N_KEY = 'N'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/sir/#anypinn.catalog.sir.Rt_KEY","title":"<code>Rt_KEY = 'Rt'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/sir/#anypinn.catalog.sir.S_KEY","title":"<code>S_KEY = 'S'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/sir/#anypinn.catalog.sir.SIRInvDataModule","title":"<code>SIRInvDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for SIR Inverse problem.</p> Source code in <code>src/anypinn/catalog/sir.py</code> <pre><code>class SIRInvDataModule(PINNDataModule):\n    \"\"\"\n    DataModule for SIR Inverse problem.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: ODEHyperparameters,\n        gen_props: ODEProperties | None = None,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ):\n        super().__init__(hp, validation, callbacks)\n        self.gen_props = gen_props\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate synthetic data.\"\"\"\n        assert self.gen_props is not None, \"SIR properties are required to generate data\"\n        gen_props = self.gen_props\n\n        args = gen_props.args.copy()\n        args.update(config.args_to_train)\n\n        data = odeint(\n            lambda x, y: gen_props.ode(x, y, args),\n            gen_props.y0,\n            config.x,\n        )\n\n        I_true = data[:, 1].clamp_min(0.0)\n\n        I_obs = self._noise(I_true, config.noise_level)\n\n        return config.x.unsqueeze(-1), I_obs.unsqueeze(-1).unsqueeze(1)\n\n    def _noise(self, I_true: Tensor, noise_level: float) -&gt; Tensor:\n        if noise_level &lt; 1.0:\n            return I_true\n        else:\n            return torch.poisson(I_true / noise_level) * noise_level\n</code></pre>"},{"location":"reference/anypinn/catalog/sir/#anypinn.catalog.sir.SIRInvDataModule.gen_props","title":"<code>gen_props = gen_props</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/sir/#anypinn.catalog.sir.SIRInvDataModule.__init__","title":"<code>__init__(hp: ODEHyperparameters, gen_props: ODEProperties | None = None, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/sir.py</code> <pre><code>def __init__(\n    self,\n    hp: ODEHyperparameters,\n    gen_props: ODEProperties | None = None,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n):\n    super().__init__(hp, validation, callbacks)\n    self.gen_props = gen_props\n</code></pre>"},{"location":"reference/anypinn/catalog/sir/#anypinn.catalog.sir.SIRInvDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate synthetic data.</p> Source code in <code>src/anypinn/catalog/sir.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate synthetic data.\"\"\"\n    assert self.gen_props is not None, \"SIR properties are required to generate data\"\n    gen_props = self.gen_props\n\n    args = gen_props.args.copy()\n    args.update(config.args_to_train)\n\n    data = odeint(\n        lambda x, y: gen_props.ode(x, y, args),\n        gen_props.y0,\n        config.x,\n    )\n\n    I_true = data[:, 1].clamp_min(0.0)\n\n    I_obs = self._noise(I_true, config.noise_level)\n\n    return config.x.unsqueeze(-1), I_obs.unsqueeze(-1).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/catalog/sir/#anypinn.catalog.sir.SIR","title":"<code>SIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>The SIR ODE system. $$ \\begin{align} \\frac{dS}{dt} &amp;= -\\beta \\frac{SI}{N} \\ \\frac{dI}{dt} &amp;= \\beta \\frac{SI}{N} - \\delta I \\ \\frac{dR}{dt} &amp;= \\delta I \\ \\end{align} $$</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Time variable.</p> required <code>y</code> <code>Tensor</code> <p>State variables [S, I].</p> required <code>args</code> <code>ArgsRegistry</code> <p>Arguments dictionary (beta, delta, N).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Derivatives [dS/dt, dI/dt].</p> Source code in <code>src/anypinn/catalog/sir.py</code> <pre><code>def SIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"\n    The SIR ODE system.\n    $$\n    \\\\begin{align}\n    \\\\frac{dS}{dt} &amp;= -\\\\beta \\\\frac{SI}{N} \\\\\\\\\n    \\\\frac{dI}{dt} &amp;= \\\\beta \\\\frac{SI}{N} - \\\\delta I \\\\\\\\\n    \\\\frac{dR}{dt} &amp;= \\\\delta I \\\\\\\\\n    \\\\end{align}\n    $$\n\n    Args:\n        x: Time variable.\n        y: State variables [S, I].\n        args: Arguments dictionary (beta, delta, N).\n\n    Returns:\n        Derivatives [dS/dt, dI/dt].\n    \"\"\"\n    S, I = y\n    b, d, N = args[BETA_KEY], args[DELTA_KEY], args[N_KEY]\n\n    dS = -b(x) * S * I / N(x)\n    dI = b(x) * S * I / N(x) - d(x) * I\n    return torch.stack([dS, dI])\n</code></pre>"},{"location":"reference/anypinn/catalog/sir/#anypinn.catalog.sir.rSIR","title":"<code>rSIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>The reduced SIR ODE system. $$ \\begin{align} \\frac{dI}{dt} &amp;= \\delta (R_t - 1) I \\end{align} $$</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Time variable.</p> required <code>y</code> <code>Tensor</code> <p>State variables [I].</p> required <code>args</code> <code>ArgsRegistry</code> <p>Arguments dictionary (delta, Rt).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Derivatives [dI/dt].</p> Source code in <code>src/anypinn/catalog/sir.py</code> <pre><code>def rSIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"\n    The reduced SIR ODE system.\n    $$\n    \\\\begin{align}\n    \\\\frac{dI}{dt} &amp;= \\\\delta (R_t - 1) I\n    \\\\end{align}\n    $$\n\n    Args:\n        x: Time variable.\n        y: State variables [I].\n        args: Arguments dictionary (delta, Rt).\n\n    Returns:\n        Derivatives [dI/dt].\n    \"\"\"\n    I = y\n    d, Rt = args[DELTA_KEY], args[Rt_KEY]\n\n    dI = d(x) * (Rt(x) - 1) * I\n    return dI\n</code></pre>"},{"location":"reference/anypinn/catalog/van_der_pol/","title":"van_der_pol","text":""},{"location":"reference/anypinn/catalog/van_der_pol/#anypinn.catalog.van_der_pol","title":"<code>anypinn.catalog.van_der_pol</code>","text":""},{"location":"reference/anypinn/catalog/van_der_pol/#anypinn.catalog.van_der_pol.MU_KEY","title":"<code>MU_KEY = 'mu'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/van_der_pol/#anypinn.catalog.van_der_pol.U_KEY","title":"<code>U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/van_der_pol/#anypinn.catalog.van_der_pol.VanDerPolDataModule","title":"<code>VanDerPolDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for Van der Pol oscillator inverse problem.</p> <p>Generates synthetic data via odeint.</p> Source code in <code>src/anypinn/catalog/van_der_pol.py</code> <pre><code>class VanDerPolDataModule(PINNDataModule):\n    \"\"\"DataModule for Van der Pol oscillator inverse problem.\n\n    Generates synthetic data via odeint.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: ODEHyperparameters,\n        gen_props: ODEProperties,\n        noise_std: float = 0.0,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n    ):\n        super().__init__(hp, validation, callbacks)\n        self.gen_props = gen_props\n        self.noise_std = noise_std\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate synthetic Van der Pol data using odeint + Gaussian noise.\"\"\"\n\n        def vdp_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n            return self.gen_props.ode(t, y, self.gen_props.args)\n\n        t = config.x\n\n        sol = odeint(vdp_ode, self.gen_props.y0, t)  # [T, 2]\n        u_true = sol[:, 0]\n\n        u_obs = u_true + self.noise_std * torch.randn_like(u_true)\n\n        return t.unsqueeze(-1), u_obs.unsqueeze(-1).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/catalog/van_der_pol/#anypinn.catalog.van_der_pol.VanDerPolDataModule.gen_props","title":"<code>gen_props = gen_props</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/van_der_pol/#anypinn.catalog.van_der_pol.VanDerPolDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/van_der_pol/#anypinn.catalog.van_der_pol.VanDerPolDataModule.__init__","title":"<code>__init__(hp: ODEHyperparameters, gen_props: ODEProperties, noise_std: float = 0.0, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/van_der_pol.py</code> <pre><code>def __init__(\n    self,\n    hp: ODEHyperparameters,\n    gen_props: ODEProperties,\n    noise_std: float = 0.0,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n):\n    super().__init__(hp, validation, callbacks)\n    self.gen_props = gen_props\n    self.noise_std = noise_std\n</code></pre>"},{"location":"reference/anypinn/catalog/van_der_pol/#anypinn.catalog.van_der_pol.VanDerPolDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate synthetic Van der Pol data using odeint + Gaussian noise.</p> Source code in <code>src/anypinn/catalog/van_der_pol.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate synthetic Van der Pol data using odeint + Gaussian noise.\"\"\"\n\n    def vdp_ode(t: Tensor, y: Tensor) -&gt; Tensor:\n        return self.gen_props.ode(t, y, self.gen_props.args)\n\n    t = config.x\n\n    sol = odeint(vdp_ode, self.gen_props.y0, t)  # [T, 2]\n    u_true = sol[:, 0]\n\n    u_obs = u_true + self.noise_std * torch.randn_like(u_true)\n\n    return t.unsqueeze(-1), u_obs.unsqueeze(-1).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/catalog/wave_1d/","title":"wave_1d","text":""},{"location":"reference/anypinn/catalog/wave_1d/#anypinn.catalog.wave_1d","title":"<code>anypinn.catalog.wave_1d</code>","text":""},{"location":"reference/anypinn/catalog/wave_1d/#anypinn.catalog.wave_1d.C_KEY","title":"<code>C_KEY = 'c'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/wave_1d/#anypinn.catalog.wave_1d.TRUE_C","title":"<code>TRUE_C = 1.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/wave_1d/#anypinn.catalog.wave_1d.U_KEY","title":"<code>U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/catalog/wave_1d/#anypinn.catalog.wave_1d.Wave1DDataModule","title":"<code>Wave1DDataModule</code>","text":"<p>               Bases: <code>PINNDataModule</code></p> <p>DataModule for 1D wave equation inverse problem.</p> <p>gen_data produces sparse interior measurements from the analytic solution u(x,t) = sin(pi x) cos(c pi t), with optional noise. These measurements are used by DataConstraint during training to recover c.</p> Source code in <code>src/anypinn/catalog/wave_1d.py</code> <pre><code>class Wave1DDataModule(PINNDataModule):\n    \"\"\"DataModule for 1D wave equation inverse problem.\n\n    gen_data produces sparse interior measurements from the analytic\n    solution u(x,t) = sin(pi x) cos(c pi t), with optional noise.\n    These measurements are used by DataConstraint during training\n    to recover c.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        true_c: float = TRUE_C,\n        n_measurements: int = 200,\n        noise_std: float = 0.01,\n        grid_size: int = 50,\n        validation: ValidationRegistry | None = None,\n        callbacks: list[DataCallback] | None = None,\n    ):\n        self.true_c = true_c\n        self.n_measurements = n_measurements\n        self.noise_std = noise_std\n        self.grid_size = grid_size\n        super().__init__(hp, validation, callbacks)\n\n    @override\n    def gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Generate analytic solution on a 2D meshgrid for training + prediction.\"\"\"\n        xs = torch.linspace(0, 1, self.grid_size)\n        ts = torch.linspace(0, 1, self.grid_size)\n        grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n\n        x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n        u_analytic = torch.sin(math.pi * x_grid[:, 0]) * torch.cos(\n            self.true_c * math.pi * x_grid[:, 1]\n        )\n\n        # Add measurement noise\n        u_noisy = u_analytic + self.noise_std * torch.randn_like(u_analytic)\n\n        # Shape: (N, 1, 1) to match codebase convention\n        y_data = u_noisy.unsqueeze(-1).unsqueeze(1)\n\n        return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/catalog/wave_1d/#anypinn.catalog.wave_1d.Wave1DDataModule.grid_size","title":"<code>grid_size = grid_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/wave_1d/#anypinn.catalog.wave_1d.Wave1DDataModule.n_measurements","title":"<code>n_measurements = n_measurements</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/wave_1d/#anypinn.catalog.wave_1d.Wave1DDataModule.noise_std","title":"<code>noise_std = noise_std</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/wave_1d/#anypinn.catalog.wave_1d.Wave1DDataModule.true_c","title":"<code>true_c = true_c</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/catalog/wave_1d/#anypinn.catalog.wave_1d.Wave1DDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, true_c: float = TRUE_C, n_measurements: int = 200, noise_std: float = 0.01, grid_size: int = 50, validation: ValidationRegistry | None = None, callbacks: list[DataCallback] | None = None)</code>","text":"Source code in <code>src/anypinn/catalog/wave_1d.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    true_c: float = TRUE_C,\n    n_measurements: int = 200,\n    noise_std: float = 0.01,\n    grid_size: int = 50,\n    validation: ValidationRegistry | None = None,\n    callbacks: list[DataCallback] | None = None,\n):\n    self.true_c = true_c\n    self.n_measurements = n_measurements\n    self.noise_std = noise_std\n    self.grid_size = grid_size\n    super().__init__(hp, validation, callbacks)\n</code></pre>"},{"location":"reference/anypinn/catalog/wave_1d/#anypinn.catalog.wave_1d.Wave1DDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; tuple[Tensor, Tensor]</code>","text":"<p>Generate analytic solution on a 2D meshgrid for training + prediction.</p> Source code in <code>src/anypinn/catalog/wave_1d.py</code> <pre><code>@override\ndef gen_data(self, config: GenerationConfig) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"Generate analytic solution on a 2D meshgrid for training + prediction.\"\"\"\n    xs = torch.linspace(0, 1, self.grid_size)\n    ts = torch.linspace(0, 1, self.grid_size)\n    grid_x, grid_t = torch.meshgrid(xs, ts, indexing=\"ij\")\n\n    x_grid = torch.stack([grid_x.reshape(-1), grid_t.reshape(-1)], dim=1)  # (N, 2)\n\n    u_analytic = torch.sin(math.pi * x_grid[:, 0]) * torch.cos(\n        self.true_c * math.pi * x_grid[:, 1]\n    )\n\n    # Add measurement noise\n    u_noisy = u_analytic + self.noise_std * torch.randn_like(u_analytic)\n\n    # Shape: (N, 1, 1) to match codebase convention\n    y_data = u_noisy.unsqueeze(-1).unsqueeze(1)\n\n    return x_grid, y_data\n</code></pre>"},{"location":"reference/anypinn/cli/","title":"cli","text":""},{"location":"reference/anypinn/cli/#anypinn.cli","title":"<code>anypinn.cli</code>","text":"<p>CLI for anypinn.</p>"},{"location":"reference/anypinn/cli/#anypinn.cli.__all__","title":"<code>__all__ = ['app']</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/app/","title":"app","text":""},{"location":"reference/anypinn/cli/app/#anypinn.cli.app","title":"<code>anypinn.cli.app</code>","text":"<p>Typer CLI application for anypinn.</p>"},{"location":"reference/anypinn/cli/app/#anypinn.cli.app.app","title":"<code>app = Typer(add_completion=False, context_settings={'help_option_names': ['-h', '--help']})</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/app/#anypinn.cli.app.create","title":"<code>create(project_name: Annotated[str, Argument(help='Name for the new project directory')], template_str: Annotated[str | None, Option('--template', '-t', help='Project template. Run with --list-templates / -l to see all options.', show_default=False)] = None, data_source: Annotated[DataSource | None, Option('--data', '-d', help='Training data source')] = None, lightning: Annotated[bool | None, Option('--lightning/--no-lightning', '-L/-NL', help='Include Lightning wrapper')] = None, list_templates: Annotated[bool, Option('--list-templates', '-l', help='List all available templates and exit.', callback=_list_templates_callback, is_eager=True, expose_value=False)] = False) -&gt; None</code>","text":"<p>Create a new PINN project.</p> Source code in <code>src/anypinn/cli/app.py</code> <pre><code>@app.command()\ndef create(\n    project_name: Annotated[str, Argument(help=\"Name for the new project directory\")],\n    template_str: Annotated[\n        str | None,\n        Option(\n            \"--template\",\n            \"-t\",\n            help=\"Project template. Run with --list-templates / -l to see all options.\",\n            show_default=False,\n        ),\n    ] = None,\n    data_source: Annotated[\n        DataSource | None, Option(\"--data\", \"-d\", help=\"Training data source\")\n    ] = None,\n    lightning: Annotated[\n        bool | None,\n        Option(\"--lightning/--no-lightning\", \"-L/-NL\", help=\"Include Lightning wrapper\"),\n    ] = None,\n    list_templates: Annotated[\n        bool,\n        Option(\n            \"--list-templates\",\n            \"-l\",\n            help=\"List all available templates and exit.\",\n            callback=_list_templates_callback,\n            is_eager=True,\n            expose_value=False,\n        ),\n    ] = False,\n) -&gt; None:\n    \"\"\"Create a new PINN project.\"\"\"\n    project_dir = Path(project_name)\n\n    if project_dir.exists():\n        _console.print(f\"[bold red]Error:[/] Directory '{project_name}' already exists.\")\n        raise Exit(code=1)\n\n    template: Template | None = None\n    if template_str is not None:\n        try:\n            template = Template(template_str)\n        except ValueError:\n            valid = \", \".join(f\"'{t.value}'\" for t in Template)\n            _console.print()\n            _console.print(\n                f\"[bold red]Error:[/] [bold]{template_str!r}[/] is not a valid template.\"\n            )\n            _console.print(f\"[dim]Valid values:[/] {valid}\")\n            _print_templates()\n            raise Exit(code=2) from None\n\n    # Header\n    _console.print()\n    _console.print(f\"[bold cyan]\u25cf[/]  anypinn v{anypinn.__version__}\")\n    _console.print(\"[dim]\u2502[/]\")\n\n    # Interactive prompts for missing options\n    if template is None:\n        template = prompt_template()\n    else:\n        _console.print(\"[bold green]\u25c7[/]  Choose a starting point\")\n        _console.print(f\"[dim]\u2502[/]  {template.label}\")\n        _console.print(\"[dim]\u2502[/]\")\n\n    if data_source is None:\n        data_source = prompt_data_source()\n    else:\n        _console.print(\"[bold green]\u25c7[/]  Select training data source\")\n        _console.print(f\"[dim]\u2502[/]  {data_source.label}\")\n        _console.print(\"[dim]\u2502[/]\")\n\n    if lightning is None:\n        lightning = prompt_lightning()\n    else:\n        display = \"Yes\" if lightning else \"No\"\n        _console.print(\"[bold green]\u25c7[/]  Include Lightning training wrapper?\")\n        _console.print(f\"[dim]\u2502[/]  {display}\")\n        _console.print(\"[dim]\u2502[/]\")\n\n    # Render\n    _console.print(f\"[bold green]\u25c7[/]  Creating {project_name}/...\")\n\n    created = render_project(project_dir, template, data_source, lightning)\n\n    for name in created:\n        desc = _FILE_DESCRIPTIONS.get(name, \"\")\n        desc_str = f\" [dim]\u2014 {desc}[/]\" if desc else \"\"\n        _console.print(f\"[dim]\u2502[/]  {name}{desc_str}\")\n\n    _console.print(\"[dim]\u2502[/]\")\n    _console.print(f\"[bold cyan]\u25cf[/]  Done! cd {project_name} &amp;&amp; uv sync &amp;&amp; uv run train.py\")\n    _console.print()\n</code></pre>"},{"location":"reference/anypinn/cli/app/#anypinn.cli.app.main","title":"<code>main() -&gt; None</code>","text":"<p>anypinn \u2014 scaffolding tool for Physics-Informed Neural Network projects.</p> Source code in <code>src/anypinn/cli/app.py</code> <pre><code>@app.callback()\ndef main() -&gt; None:\n    \"\"\"anypinn \u2014 scaffolding tool for Physics-Informed Neural Network projects.\"\"\"\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/","title":"scaffold","text":""},{"location":"reference/anypinn/cli/scaffold/#anypinn.cli.scaffold","title":"<code>anypinn.cli.scaffold</code>","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/","title":"allen_cahn","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/#anypinn.cli.scaffold.allen_cahn","title":"<code>anypinn.cli.scaffold.allen_cahn</code>","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/config_csv/","title":"config_csv","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/config_csv/#anypinn.cli.scaffold.allen_cahn.config_csv","title":"<code>anypinn.cli.scaffold.allen_cahn.config_csv</code>","text":"<p>Allen-Cahn Equation \u2014 training configuration (CSV).</p>"},{"location":"reference/anypinn/cli/scaffold/allen_cahn/config_csv/#anypinn.cli.scaffold.allen_cahn.config_csv.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/config_csv/#anypinn.cli.scaffold.allen_cahn.config_csv.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/config_csv/#anypinn.cli.scaffold.allen_cahn.config_csv.hp","title":"<code>hp = PINNHyperparameters(lr=0.001, max_epochs=3000, gradient_clip_val=0.5, criterion='huber', training_data=(IngestionConfig(batch_size=128, data_ratio=2, collocations=10000, collocation_sampler='adaptive', df_path=(Path('./data/data.csv')), y_columns=['u'])), fields_config=(MLPConfig(in_dim=256, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.0)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=200, threshold=0.0001, min_lr=1e-06)))</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/config_synthetic/","title":"config_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/config_synthetic/#anypinn.cli.scaffold.allen_cahn.config_synthetic","title":"<code>anypinn.cli.scaffold.allen_cahn.config_synthetic</code>","text":"<p>Allen-Cahn Equation \u2014 training configuration (synthetic).</p>"},{"location":"reference/anypinn/cli/scaffold/allen_cahn/config_synthetic/#anypinn.cli.scaffold.allen_cahn.config_synthetic.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/config_synthetic/#anypinn.cli.scaffold.allen_cahn.config_synthetic.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/config_synthetic/#anypinn.cli.scaffold.allen_cahn.config_synthetic.hp","title":"<code>hp = PINNHyperparameters(lr=0.001, max_epochs=3000, gradient_clip_val=0.5, criterion='huber', training_data=(GenerationConfig(batch_size=128, data_ratio=2, collocations=10000, collocation_sampler='adaptive', x=(torch.linspace(-1, 1, steps=50)), args_to_train={}, noise_level=0)), fields_config=(MLPConfig(in_dim=256, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.0)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=200, threshold=0.0001, min_lr=1e-06)))</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_csv/","title":"ode_csv","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_csv/#anypinn.cli.scaffold.allen_cahn.ode_csv","title":"<code>anypinn.cli.scaffold.allen_cahn.ode_csv</code>","text":"<p>Allen-Cahn Equation \u2014 stiff reaction-diffusion forward problem definition (CSV data).</p>"},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_csv/#anypinn.cli.scaffold.allen_cahn.ode_csv.EPSILON","title":"<code>EPSILON = TRUE_EPSILON</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_csv/#anypinn.cli.scaffold.allen_cahn.ode_csv.GRID_SIZE","title":"<code>GRID_SIZE = 50</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_csv/#anypinn.cli.scaffold.allen_cahn.ode_csv.AllenCahnResidualScorer","title":"<code>AllenCahnResidualScorer</code>","text":"<p>ResidualScorer protocol implementation for adaptive collocation.</p> Source code in <code>src/anypinn/cli/scaffold/allen_cahn/ode_csv.py</code> <pre><code>class AllenCahnResidualScorer:\n    \"\"\"ResidualScorer protocol implementation for adaptive collocation.\"\"\"\n\n    def __init__(self, fields: FieldsRegistry, params: ParamsRegistry) -&gt; None:\n        self.fields = fields\n        self.params = params\n\n    def residual_score(self, x: Tensor) -&gt; Tensor:\n        device = next(iter(self.fields.values())).parameters().__next__().device\n        x = x.detach().to(device).requires_grad_(True)\n        with torch.enable_grad():\n            res = allen_cahn_residual(x, self.fields, self.params)\n        return res.detach().cpu().abs().mean(dim=-1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_csv/#anypinn.cli.scaffold.allen_cahn.ode_csv.AllenCahnResidualScorer.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_csv/#anypinn.cli.scaffold.allen_cahn.ode_csv.AllenCahnResidualScorer.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_csv/#anypinn.cli.scaffold.allen_cahn.ode_csv.AllenCahnResidualScorer.__init__","title":"<code>__init__(fields: FieldsRegistry, params: ParamsRegistry) -&gt; None</code>","text":"Source code in <code>src/anypinn/cli/scaffold/allen_cahn/ode_csv.py</code> <pre><code>def __init__(self, fields: FieldsRegistry, params: ParamsRegistry) -&gt; None:\n    self.fields = fields\n    self.params = params\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_csv/#anypinn.cli.scaffold.allen_cahn.ode_csv.AllenCahnResidualScorer.residual_score","title":"<code>residual_score(x: Tensor) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/cli/scaffold/allen_cahn/ode_csv.py</code> <pre><code>def residual_score(self, x: Tensor) -&gt; Tensor:\n    device = next(iter(self.fields.values())).parameters().__next__().device\n    x = x.detach().to(device).requires_grad_(True)\n    with torch.enable_grad():\n        res = allen_cahn_residual(x, self.fields, self.params)\n    return res.detach().cpu().abs().mean(dim=-1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_csv/#anypinn.cli.scaffold.allen_cahn.ode_csv.allen_cahn_residual","title":"<code>allen_cahn_residual(x: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor</code>","text":"<p>PDE residual: du/dt - eps*d2u/dx2 - u + u^3 = 0.</p> Source code in <code>src/anypinn/cli/scaffold/allen_cahn/ode_csv.py</code> <pre><code>def allen_cahn_residual(x: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n    \"\"\"PDE residual: du/dt - eps*d2u/dx2 - u + u^3 = 0.\"\"\"\n    u = fields[U_KEY](x)\n    du_dt = partial(u, x, dim=1, order=1)\n    d2u_dx2 = partial(u, x, dim=0, order=2)\n    return du_dt - EPSILON * d2u_dx2 - u + u**3\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_csv/#anypinn.cli.scaffold.allen_cahn.ode_csv.create_data_module","title":"<code>create_data_module(hp: PINNHyperparameters, fields: FieldsRegistry, params: ParamsRegistry) -&gt; AllenCahnDataModule</code>","text":"Source code in <code>src/anypinn/cli/scaffold/allen_cahn/ode_csv.py</code> <pre><code>def create_data_module(\n    hp: PINNHyperparameters,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n) -&gt; AllenCahnDataModule:\n    scorer = AllenCahnResidualScorer(fields, params)\n    return AllenCahnDataModule(\n        hp=hp,\n        true_epsilon=TRUE_EPSILON,\n        grid_size=GRID_SIZE,\n        residual_scorer=scorer,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_csv/#anypinn.cli.scaffold.allen_cahn.ode_csv.create_problem","title":"<code>create_problem(hp: PINNHyperparameters) -&gt; Problem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/allen_cahn/ode_csv.py</code> <pre><code>def create_problem(hp: PINNHyperparameters) -&gt; Problem:\n    rff = RandomFourierFeatures(in_dim=2, num_features=128, scale=5.0, seed=42)\n    field_u = Field(\n        config=MLPConfig(\n            in_dim=rff.out_dim,\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=hp.fields_config.output_activation,\n            encode=rff,\n        )\n    )\n\n    fields = FieldsRegistry({U_KEY: field_u})\n    params = ParamsRegistry({})\n\n    bcs = [\n        PeriodicBCConstraint(\n            bc_left=BoundaryCondition(sampler=_periodic_left, value=_dummy, n_pts=100),\n            bc_right=BoundaryCondition(sampler=_periodic_right, value=_dummy, n_pts=100),\n            field=field_u,\n            match_dim=0,\n            log_key=\"loss/bc_periodic\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_initial_condition, value=_ic_value, n_pts=200),\n            field_u,\n            log_key=\"loss/ic\",\n            weight=10.0,\n        ),\n    ]\n\n    pde = PDEResidualConstraint(\n        fields=fields,\n        params=params,\n        residual_fn=allen_cahn_residual,\n        log_key=\"loss/pde_residual\",\n        weight=1.0,\n    )\n\n    return Problem(\n        constraints=[pde, *bcs],\n        criterion=build_criterion(hp.criterion),\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_synthetic/","title":"ode_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_synthetic/#anypinn.cli.scaffold.allen_cahn.ode_synthetic","title":"<code>anypinn.cli.scaffold.allen_cahn.ode_synthetic</code>","text":"<p>Allen-Cahn Equation \u2014 stiff reaction-diffusion forward problem definition (synthetic).</p>"},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_synthetic/#anypinn.cli.scaffold.allen_cahn.ode_synthetic.EPSILON","title":"<code>EPSILON = TRUE_EPSILON</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_synthetic/#anypinn.cli.scaffold.allen_cahn.ode_synthetic.GRID_SIZE","title":"<code>GRID_SIZE = 50</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_synthetic/#anypinn.cli.scaffold.allen_cahn.ode_synthetic.AllenCahnResidualScorer","title":"<code>AllenCahnResidualScorer</code>","text":"<p>ResidualScorer protocol implementation for adaptive collocation.</p> Source code in <code>src/anypinn/cli/scaffold/allen_cahn/ode_synthetic.py</code> <pre><code>class AllenCahnResidualScorer:\n    \"\"\"ResidualScorer protocol implementation for adaptive collocation.\"\"\"\n\n    def __init__(self, fields: FieldsRegistry, params: ParamsRegistry) -&gt; None:\n        self.fields = fields\n        self.params = params\n\n    def residual_score(self, x: Tensor) -&gt; Tensor:\n        device = next(iter(self.fields.values())).parameters().__next__().device\n        x = x.detach().to(device).requires_grad_(True)\n        with torch.enable_grad():\n            res = allen_cahn_residual(x, self.fields, self.params)\n        return res.detach().cpu().abs().mean(dim=-1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_synthetic/#anypinn.cli.scaffold.allen_cahn.ode_synthetic.AllenCahnResidualScorer.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_synthetic/#anypinn.cli.scaffold.allen_cahn.ode_synthetic.AllenCahnResidualScorer.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_synthetic/#anypinn.cli.scaffold.allen_cahn.ode_synthetic.AllenCahnResidualScorer.__init__","title":"<code>__init__(fields: FieldsRegistry, params: ParamsRegistry) -&gt; None</code>","text":"Source code in <code>src/anypinn/cli/scaffold/allen_cahn/ode_synthetic.py</code> <pre><code>def __init__(self, fields: FieldsRegistry, params: ParamsRegistry) -&gt; None:\n    self.fields = fields\n    self.params = params\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_synthetic/#anypinn.cli.scaffold.allen_cahn.ode_synthetic.AllenCahnResidualScorer.residual_score","title":"<code>residual_score(x: Tensor) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/cli/scaffold/allen_cahn/ode_synthetic.py</code> <pre><code>def residual_score(self, x: Tensor) -&gt; Tensor:\n    device = next(iter(self.fields.values())).parameters().__next__().device\n    x = x.detach().to(device).requires_grad_(True)\n    with torch.enable_grad():\n        res = allen_cahn_residual(x, self.fields, self.params)\n    return res.detach().cpu().abs().mean(dim=-1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_synthetic/#anypinn.cli.scaffold.allen_cahn.ode_synthetic.allen_cahn_residual","title":"<code>allen_cahn_residual(x: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor</code>","text":"<p>PDE residual: du/dt - eps*d2u/dx2 - u + u^3 = 0.</p> Source code in <code>src/anypinn/cli/scaffold/allen_cahn/ode_synthetic.py</code> <pre><code>def allen_cahn_residual(x: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n    \"\"\"PDE residual: du/dt - eps*d2u/dx2 - u + u^3 = 0.\"\"\"\n    u = fields[U_KEY](x)\n    du_dt = partial(u, x, dim=1, order=1)\n    d2u_dx2 = partial(u, x, dim=0, order=2)\n    return du_dt - EPSILON * d2u_dx2 - u + u**3\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_synthetic/#anypinn.cli.scaffold.allen_cahn.ode_synthetic.create_data_module","title":"<code>create_data_module(hp: PINNHyperparameters, fields: FieldsRegistry, params: ParamsRegistry) -&gt; AllenCahnDataModule</code>","text":"Source code in <code>src/anypinn/cli/scaffold/allen_cahn/ode_synthetic.py</code> <pre><code>def create_data_module(\n    hp: PINNHyperparameters,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n) -&gt; AllenCahnDataModule:\n    scorer = AllenCahnResidualScorer(fields, params)\n    return AllenCahnDataModule(\n        hp=hp,\n        true_epsilon=TRUE_EPSILON,\n        grid_size=GRID_SIZE,\n        residual_scorer=scorer,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/allen_cahn/ode_synthetic/#anypinn.cli.scaffold.allen_cahn.ode_synthetic.create_problem","title":"<code>create_problem(hp: PINNHyperparameters) -&gt; Problem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/allen_cahn/ode_synthetic.py</code> <pre><code>def create_problem(hp: PINNHyperparameters) -&gt; Problem:\n    rff = RandomFourierFeatures(in_dim=2, num_features=128, scale=5.0, seed=42)\n    field_u = Field(\n        config=MLPConfig(\n            in_dim=rff.out_dim,\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=hp.fields_config.output_activation,\n            encode=rff,\n        )\n    )\n\n    fields = FieldsRegistry({U_KEY: field_u})\n    params = ParamsRegistry({})\n\n    bcs = [\n        PeriodicBCConstraint(\n            bc_left=BoundaryCondition(sampler=_periodic_left, value=_dummy, n_pts=100),\n            bc_right=BoundaryCondition(sampler=_periodic_right, value=_dummy, n_pts=100),\n            field=field_u,\n            match_dim=0,\n            log_key=\"loss/bc_periodic\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_initial_condition, value=_ic_value, n_pts=200),\n            field_u,\n            log_key=\"loss/ic\",\n            weight=10.0,\n        ),\n    ]\n\n    pde = PDEResidualConstraint(\n        fields=fields,\n        params=params,\n        residual_fn=allen_cahn_residual,\n        log_key=\"loss/pde_residual\",\n        weight=1.0,\n    )\n\n    return Problem(\n        constraints=[pde, *bcs],\n        criterion=build_criterion(hp.criterion),\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/blank/","title":"blank","text":""},{"location":"reference/anypinn/cli/scaffold/blank/#anypinn.cli.scaffold.blank","title":"<code>anypinn.cli.scaffold.blank</code>","text":""},{"location":"reference/anypinn/cli/scaffold/blank/config_csv/","title":"config_csv","text":""},{"location":"reference/anypinn/cli/scaffold/blank/config_csv/#anypinn.cli.scaffold.blank.config_csv","title":"<code>anypinn.cli.scaffold.blank.config_csv</code>","text":"<p>Training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/blank/config_csv/#anypinn.cli.scaffold.blank.config_csv.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/blank/config_csv/#anypinn.cli.scaffold.blank.config_csv.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/blank/config_csv/#anypinn.cli.scaffold.blank.config_csv.hp","title":"<code>hp = ODEHyperparameters(lr=0.001, max_epochs=2000, gradient_clip_val=0.1, training_data=(IngestionConfig(batch_size=100, data_ratio=2, collocations=6000, df_path=(Path('./data/data.csv')), y_columns=['y_obs'])), fields_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.5)), pde_weight=1, ic_weight=1, data_weight=1)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/blank/config_synthetic/","title":"config_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/blank/config_synthetic/#anypinn.cli.scaffold.blank.config_synthetic","title":"<code>anypinn.cli.scaffold.blank.config_synthetic</code>","text":"<p>Training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/blank/config_synthetic/#anypinn.cli.scaffold.blank.config_synthetic.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/blank/config_synthetic/#anypinn.cli.scaffold.blank.config_synthetic.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/blank/config_synthetic/#anypinn.cli.scaffold.blank.config_synthetic.hp","title":"<code>hp = ODEHyperparameters(lr=0.001, max_epochs=2000, gradient_clip_val=0.1, training_data=(GenerationConfig(batch_size=100, data_ratio=2, collocations=6000, x=(torch.linspace(start=0, end=10, steps=100)), args_to_train={}, noise_level=0)), fields_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.5)), pde_weight=1, ic_weight=1, data_weight=1)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/blank/ode_csv/","title":"ode_csv","text":""},{"location":"reference/anypinn/cli/scaffold/blank/ode_csv/#anypinn.cli.scaffold.blank.ode_csv","title":"<code>anypinn.cli.scaffold.blank.ode_csv</code>","text":"<p>ODE mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/blank/ode_csv/#anypinn.cli.scaffold.blank.ode_csv.validation","title":"<code>validation: ValidationRegistry = {}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/blank/ode_csv/#anypinn.cli.scaffold.blank.ode_csv.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/blank/ode_csv.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    raise NotImplementedError(\"TODO: implement create_data_module\")\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/blank/ode_csv/#anypinn.cli.scaffold.blank.ode_csv.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/blank/ode_csv.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    raise NotImplementedError(\"TODO: implement create_problem\")\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/blank/ode_synthetic/","title":"ode_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/blank/ode_synthetic/#anypinn.cli.scaffold.blank.ode_synthetic","title":"<code>anypinn.cli.scaffold.blank.ode_synthetic</code>","text":"<p>ODE mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/blank/ode_synthetic/#anypinn.cli.scaffold.blank.ode_synthetic.validation","title":"<code>validation: ValidationRegistry = {}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/blank/ode_synthetic/#anypinn.cli.scaffold.blank.ode_synthetic.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/blank/ode_synthetic.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    raise NotImplementedError(\"TODO: implement create_data_module\")\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/blank/ode_synthetic/#anypinn.cli.scaffold.blank.ode_synthetic.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/blank/ode_synthetic.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    raise NotImplementedError(\"TODO: implement create_problem\")\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/","title":"burgers_1d","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/#anypinn.cli.scaffold.burgers_1d","title":"<code>anypinn.cli.scaffold.burgers_1d</code>","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/config_csv/","title":"config_csv","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/config_csv/#anypinn.cli.scaffold.burgers_1d.config_csv","title":"<code>anypinn.cli.scaffold.burgers_1d.config_csv</code>","text":"<p>Burgers Equation 1D \u2014 training configuration (CSV).</p>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/config_csv/#anypinn.cli.scaffold.burgers_1d.config_csv.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/config_csv/#anypinn.cli.scaffold.burgers_1d.config_csv.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/config_csv/#anypinn.cli.scaffold.burgers_1d.config_csv.hp","title":"<code>hp = PINNHyperparameters(lr=0.001, max_epochs=5000, gradient_clip_val=0.5, training_data=(IngestionConfig(batch_size=128, data_ratio=2, collocations=10000, collocation_sampler='adaptive', df_path=(Path('./data/data.csv')), y_columns=['u'])), fields_config=(MLPConfig(in_dim=256, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.1)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=200, threshold=0.0001, min_lr=1e-06)))</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/config_synthetic/","title":"config_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/config_synthetic/#anypinn.cli.scaffold.burgers_1d.config_synthetic","title":"<code>anypinn.cli.scaffold.burgers_1d.config_synthetic</code>","text":"<p>Burgers Equation 1D \u2014 training configuration (synthetic).</p>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/config_synthetic/#anypinn.cli.scaffold.burgers_1d.config_synthetic.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/config_synthetic/#anypinn.cli.scaffold.burgers_1d.config_synthetic.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/config_synthetic/#anypinn.cli.scaffold.burgers_1d.config_synthetic.hp","title":"<code>hp = PINNHyperparameters(lr=0.001, max_epochs=5000, gradient_clip_val=0.5, training_data=(GenerationConfig(batch_size=128, data_ratio=2, collocations=10000, collocation_sampler='adaptive', x=(torch.linspace(-1, 1, steps=50)), args_to_train={}, noise_level=0)), fields_config=(MLPConfig(in_dim=256, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.1)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=200, threshold=0.0001, min_lr=1e-06)))</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_csv/","title":"ode_csv","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_csv/#anypinn.cli.scaffold.burgers_1d.ode_csv","title":"<code>anypinn.cli.scaffold.burgers_1d.ode_csv</code>","text":"<p>Burgers Equation 1D \u2014 nonlinear PDE inverse problem definition (CSV data).</p>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_csv/#anypinn.cli.scaffold.burgers_1d.ode_csv.GRID_SIZE","title":"<code>GRID_SIZE = 50</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_csv/#anypinn.cli.scaffold.burgers_1d.ode_csv.validation","title":"<code>validation: ValidationRegistry = {NU_KEY: lambda x: torch.full_like(x, TRUE_NU)}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_csv/#anypinn.cli.scaffold.burgers_1d.ode_csv.BurgersResidualScorer","title":"<code>BurgersResidualScorer</code>","text":"<p>ResidualScorer protocol implementation for adaptive collocation.</p> Source code in <code>src/anypinn/cli/scaffold/burgers_1d/ode_csv.py</code> <pre><code>class BurgersResidualScorer:\n    \"\"\"ResidualScorer protocol implementation for adaptive collocation.\"\"\"\n\n    def __init__(self, fields: FieldsRegistry, params: ParamsRegistry) -&gt; None:\n        self.fields = fields\n        self.params = params\n\n    def residual_score(self, x: Tensor) -&gt; Tensor:\n        device = next(iter(self.fields.values())).parameters().__next__().device\n        x = x.detach().to(device).requires_grad_(True)\n        with torch.enable_grad():\n            res = burgers_residual(x, self.fields, self.params)\n        # res may be (n, d) due to scalar param broadcasting; reduce to (n,)\n        return res.detach().cpu().abs().mean(dim=-1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_csv/#anypinn.cli.scaffold.burgers_1d.ode_csv.BurgersResidualScorer.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_csv/#anypinn.cli.scaffold.burgers_1d.ode_csv.BurgersResidualScorer.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_csv/#anypinn.cli.scaffold.burgers_1d.ode_csv.BurgersResidualScorer.__init__","title":"<code>__init__(fields: FieldsRegistry, params: ParamsRegistry) -&gt; None</code>","text":"Source code in <code>src/anypinn/cli/scaffold/burgers_1d/ode_csv.py</code> <pre><code>def __init__(self, fields: FieldsRegistry, params: ParamsRegistry) -&gt; None:\n    self.fields = fields\n    self.params = params\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_csv/#anypinn.cli.scaffold.burgers_1d.ode_csv.BurgersResidualScorer.residual_score","title":"<code>residual_score(x: Tensor) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/cli/scaffold/burgers_1d/ode_csv.py</code> <pre><code>def residual_score(self, x: Tensor) -&gt; Tensor:\n    device = next(iter(self.fields.values())).parameters().__next__().device\n    x = x.detach().to(device).requires_grad_(True)\n    with torch.enable_grad():\n        res = burgers_residual(x, self.fields, self.params)\n    # res may be (n, d) due to scalar param broadcasting; reduce to (n,)\n    return res.detach().cpu().abs().mean(dim=-1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_csv/#anypinn.cli.scaffold.burgers_1d.ode_csv.burgers_residual","title":"<code>burgers_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor</code>","text":"<p>PDE residual: du/dt + udu/dx - nud2u/dx2 = 0.</p> Source code in <code>src/anypinn/cli/scaffold/burgers_1d/ode_csv.py</code> <pre><code>def burgers_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor:\n    \"\"\"PDE residual: du/dt + u*du/dx - nu*d2u/dx2 = 0.\"\"\"\n    u = fields[U_KEY](x)\n    nu = params[NU_KEY](x)\n    du_dt = partial(u, x, dim=1, order=1)\n    du_dx = partial(u, x, dim=0, order=1)\n    d2u_dx2 = partial(u, x, dim=0, order=2)\n    return du_dt + u * du_dx - nu * d2u_dx2\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_csv/#anypinn.cli.scaffold.burgers_1d.ode_csv.create_data_module","title":"<code>create_data_module(hp: PINNHyperparameters, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Burgers1DDataModule</code>","text":"Source code in <code>src/anypinn/cli/scaffold/burgers_1d/ode_csv.py</code> <pre><code>def create_data_module(\n    hp: PINNHyperparameters,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n) -&gt; Burgers1DDataModule:\n    scorer = BurgersResidualScorer(fields, params)\n    return Burgers1DDataModule(\n        hp=hp,\n        true_nu=TRUE_NU,\n        grid_size=GRID_SIZE,\n        residual_scorer=scorer,\n        validation=validation,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_csv/#anypinn.cli.scaffold.burgers_1d.ode_csv.create_problem","title":"<code>create_problem(hp: PINNHyperparameters) -&gt; Problem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/burgers_1d/ode_csv.py</code> <pre><code>def create_problem(hp: PINNHyperparameters) -&gt; Problem:\n    rff = RandomFourierFeatures(in_dim=2, num_features=128, scale=1.0, seed=42)\n    field_u = Field(\n        config=MLPConfig(\n            in_dim=rff.out_dim,\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=hp.fields_config.output_activation,\n            encode=rff,\n        )\n    )\n    param_nu = Parameter(\n        config=ScalarConfig(init_value=hp.params_config.init_value),\n    )\n\n    fields = FieldsRegistry({U_KEY: field_u})\n    params = ParamsRegistry({NU_KEY: param_nu})\n\n    bcs = [\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_left_boundary, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_left\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_right_boundary, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_right\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_initial_condition, value=_ic_value, n_pts=100),\n            field_u,\n            log_key=\"loss/ic\",\n            weight=10.0,\n        ),\n    ]\n\n    pde = PDEResidualConstraint(\n        fields=fields,\n        params=params,\n        residual_fn=burgers_residual,\n        log_key=\"loss/pde_residual\",\n        weight=1.0,\n    )\n\n    data = DataConstraint(\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n        weight=5.0,\n    )\n\n    return Problem(\n        constraints=[pde, *bcs, data],\n        criterion=build_criterion(hp.criterion),\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_csv/#anypinn.cli.scaffold.burgers_1d.ode_csv.predict_data","title":"<code>predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/cli/scaffold/burgers_1d/ode_csv.py</code> <pre><code>def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n    return fields[U_KEY](x_data).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_synthetic/","title":"ode_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_synthetic/#anypinn.cli.scaffold.burgers_1d.ode_synthetic","title":"<code>anypinn.cli.scaffold.burgers_1d.ode_synthetic</code>","text":"<p>Burgers Equation 1D \u2014 nonlinear PDE inverse problem definition (synthetic).</p>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_synthetic/#anypinn.cli.scaffold.burgers_1d.ode_synthetic.GRID_SIZE","title":"<code>GRID_SIZE = 50</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_synthetic/#anypinn.cli.scaffold.burgers_1d.ode_synthetic.validation","title":"<code>validation: ValidationRegistry = {NU_KEY: lambda x: torch.full_like(x, TRUE_NU)}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_synthetic/#anypinn.cli.scaffold.burgers_1d.ode_synthetic.BurgersResidualScorer","title":"<code>BurgersResidualScorer</code>","text":"<p>ResidualScorer protocol implementation for adaptive collocation.</p> Source code in <code>src/anypinn/cli/scaffold/burgers_1d/ode_synthetic.py</code> <pre><code>class BurgersResidualScorer:\n    \"\"\"ResidualScorer protocol implementation for adaptive collocation.\"\"\"\n\n    def __init__(self, fields: FieldsRegistry, params: ParamsRegistry) -&gt; None:\n        self.fields = fields\n        self.params = params\n\n    def residual_score(self, x: Tensor) -&gt; Tensor:\n        device = next(iter(self.fields.values())).parameters().__next__().device\n        x = x.detach().to(device).requires_grad_(True)\n        with torch.enable_grad():\n            res = burgers_residual(x, self.fields, self.params)\n        # res may be (n, d) due to scalar param broadcasting; reduce to (n,)\n        return res.detach().cpu().abs().mean(dim=-1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_synthetic/#anypinn.cli.scaffold.burgers_1d.ode_synthetic.BurgersResidualScorer.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_synthetic/#anypinn.cli.scaffold.burgers_1d.ode_synthetic.BurgersResidualScorer.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_synthetic/#anypinn.cli.scaffold.burgers_1d.ode_synthetic.BurgersResidualScorer.__init__","title":"<code>__init__(fields: FieldsRegistry, params: ParamsRegistry) -&gt; None</code>","text":"Source code in <code>src/anypinn/cli/scaffold/burgers_1d/ode_synthetic.py</code> <pre><code>def __init__(self, fields: FieldsRegistry, params: ParamsRegistry) -&gt; None:\n    self.fields = fields\n    self.params = params\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_synthetic/#anypinn.cli.scaffold.burgers_1d.ode_synthetic.BurgersResidualScorer.residual_score","title":"<code>residual_score(x: Tensor) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/cli/scaffold/burgers_1d/ode_synthetic.py</code> <pre><code>def residual_score(self, x: Tensor) -&gt; Tensor:\n    device = next(iter(self.fields.values())).parameters().__next__().device\n    x = x.detach().to(device).requires_grad_(True)\n    with torch.enable_grad():\n        res = burgers_residual(x, self.fields, self.params)\n    # res may be (n, d) due to scalar param broadcasting; reduce to (n,)\n    return res.detach().cpu().abs().mean(dim=-1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_synthetic/#anypinn.cli.scaffold.burgers_1d.ode_synthetic.burgers_residual","title":"<code>burgers_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor</code>","text":"<p>PDE residual: du/dt + udu/dx - nud2u/dx2 = 0.</p> Source code in <code>src/anypinn/cli/scaffold/burgers_1d/ode_synthetic.py</code> <pre><code>def burgers_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor:\n    \"\"\"PDE residual: du/dt + u*du/dx - nu*d2u/dx2 = 0.\"\"\"\n    u = fields[U_KEY](x)\n    nu = params[NU_KEY](x)\n    du_dt = partial(u, x, dim=1, order=1)\n    du_dx = partial(u, x, dim=0, order=1)\n    d2u_dx2 = partial(u, x, dim=0, order=2)\n    return du_dt + u * du_dx - nu * d2u_dx2\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_synthetic/#anypinn.cli.scaffold.burgers_1d.ode_synthetic.create_data_module","title":"<code>create_data_module(hp: PINNHyperparameters, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Burgers1DDataModule</code>","text":"Source code in <code>src/anypinn/cli/scaffold/burgers_1d/ode_synthetic.py</code> <pre><code>def create_data_module(\n    hp: PINNHyperparameters,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n) -&gt; Burgers1DDataModule:\n    scorer = BurgersResidualScorer(fields, params)\n    return Burgers1DDataModule(\n        hp=hp,\n        true_nu=TRUE_NU,\n        grid_size=GRID_SIZE,\n        residual_scorer=scorer,\n        validation=validation,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_synthetic/#anypinn.cli.scaffold.burgers_1d.ode_synthetic.create_problem","title":"<code>create_problem(hp: PINNHyperparameters) -&gt; Problem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/burgers_1d/ode_synthetic.py</code> <pre><code>def create_problem(hp: PINNHyperparameters) -&gt; Problem:\n    rff = RandomFourierFeatures(in_dim=2, num_features=128, scale=1.0, seed=42)\n    field_u = Field(\n        config=MLPConfig(\n            in_dim=rff.out_dim,\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=hp.fields_config.output_activation,\n            encode=rff,\n        )\n    )\n    param_nu = Parameter(\n        config=ScalarConfig(init_value=hp.params_config.init_value),\n    )\n\n    fields = FieldsRegistry({U_KEY: field_u})\n    params = ParamsRegistry({NU_KEY: param_nu})\n\n    bcs = [\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_left_boundary, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_left\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_right_boundary, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_right\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_initial_condition, value=_ic_value, n_pts=100),\n            field_u,\n            log_key=\"loss/ic\",\n            weight=10.0,\n        ),\n    ]\n\n    pde = PDEResidualConstraint(\n        fields=fields,\n        params=params,\n        residual_fn=burgers_residual,\n        log_key=\"loss/pde_residual\",\n        weight=1.0,\n    )\n\n    data = DataConstraint(\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n        weight=5.0,\n    )\n\n    return Problem(\n        constraints=[pde, *bcs, data],\n        criterion=build_criterion(hp.criterion),\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/burgers_1d/ode_synthetic/#anypinn.cli.scaffold.burgers_1d.ode_synthetic.predict_data","title":"<code>predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/cli/scaffold/burgers_1d/ode_synthetic.py</code> <pre><code>def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n    return fields[U_KEY](x_data).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/custom/","title":"custom","text":""},{"location":"reference/anypinn/cli/scaffold/custom/#anypinn.cli.scaffold.custom","title":"<code>anypinn.cli.scaffold.custom</code>","text":""},{"location":"reference/anypinn/cli/scaffold/custom/config_csv/","title":"config_csv","text":""},{"location":"reference/anypinn/cli/scaffold/custom/config_csv/#anypinn.cli.scaffold.custom.config_csv","title":"<code>anypinn.cli.scaffold.custom.config_csv</code>","text":"<p>Custom ODE \u2014 training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/custom/config_csv/#anypinn.cli.scaffold.custom.config_csv.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/custom/config_csv/#anypinn.cli.scaffold.custom.config_csv.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/custom/config_csv/#anypinn.cli.scaffold.custom.config_csv.hp","title":"<code>hp = ODEHyperparameters(lr=0.001, max_epochs=2000, gradient_clip_val=0.1, training_data=(IngestionConfig(batch_size=100, data_ratio=2, collocations=6000, df_path=(Path('./data/data.csv')), y_columns=['y_obs'])), fields_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.5)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=55, threshold=0.005, min_lr=1e-06)), pde_weight=1, ic_weight=1, data_weight=1)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/custom/config_synthetic/","title":"config_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/custom/config_synthetic/#anypinn.cli.scaffold.custom.config_synthetic","title":"<code>anypinn.cli.scaffold.custom.config_synthetic</code>","text":"<p>Custom ODE \u2014 training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/custom/config_synthetic/#anypinn.cli.scaffold.custom.config_synthetic.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/custom/config_synthetic/#anypinn.cli.scaffold.custom.config_synthetic.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/custom/config_synthetic/#anypinn.cli.scaffold.custom.config_synthetic.hp","title":"<code>hp = ODEHyperparameters(lr=0.001, max_epochs=2000, gradient_clip_val=0.1, training_data=(GenerationConfig(batch_size=100, data_ratio=2, collocations=6000, x=(torch.linspace(start=0, end=10, steps=100)), args_to_train={}, noise_level=0)), fields_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.5)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=55, threshold=0.005, min_lr=1e-06)), pde_weight=1, ic_weight=1, data_weight=1)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/custom/ode_csv/","title":"ode_csv","text":""},{"location":"reference/anypinn/cli/scaffold/custom/ode_csv/#anypinn.cli.scaffold.custom.ode_csv","title":"<code>anypinn.cli.scaffold.custom.ode_csv</code>","text":"<p>Custom ODE \u2014 mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/custom/ode_csv/#anypinn.cli.scaffold.custom.ode_csv.validation","title":"<code>validation: ValidationRegistry = {}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/custom/ode_csv/#anypinn.cli.scaffold.custom.ode_csv.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/custom/ode_csv.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    # TODO: create and return your DataModule\n    # See anypinn.catalog for examples of DataModule subclasses.\n    raise NotImplementedError(\"TODO: implement create_data_module\")\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/custom/ode_csv/#anypinn.cli.scaffold.custom.ode_csv.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/custom/ode_csv.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    # TODO: define your problem\n    # props = ODEProperties(\n    #     ode=my_ode,\n    #     y0=torch.tensor([Y1_0, Y2_0]),\n    #     args={\n    #         # fixed arguments go here\n    #     },\n    # )\n    #\n    # fields = FieldsRegistry({\n    #     Y1_KEY: Field(config=hp.fields_config),\n    #     Y2_KEY: Field(config=hp.fields_config),\n    # })\n    # params = ParamsRegistry({\n    #     PARAM_KEY: Parameter(config=hp.params_config),\n    # })\n    #\n    # def predict_data(\n    #     x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry\n    # ) -&gt; Tensor:\n    #     return cast(Tensor, fields[Y1_KEY](x_data))\n    #\n    # return ODEInverseProblem(\n    #     props=props, hp=hp, fields=fields, params=params,\n    #     predict_data=predict_data,\n    # )\n    raise NotImplementedError(\"TODO: implement create_problem\")\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/custom/ode_csv/#anypinn.cli.scaffold.custom.ode_csv.my_ode","title":"<code>my_ode(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>TODO: implement your ODE system.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Independent variable (e.g. time). Shape: (N,)</p> required <code>y</code> <code>Tensor</code> <p>State variables. Each element has shape (N,)</p> required <code>args</code> <code>ArgsRegistry</code> <p>Parameters (both fixed and learnable).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>dy/dx stacked as a tensor.</p> Source code in <code>src/anypinn/cli/scaffold/custom/ode_csv.py</code> <pre><code>def my_ode(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"TODO: implement your ODE system.\n\n    Args:\n        x: Independent variable (e.g. time). Shape: (N,)\n        y: State variables. Each element has shape (N,)\n        args: Parameters (both fixed and learnable).\n\n    Returns:\n        dy/dx stacked as a tensor.\n    \"\"\"\n    # Example:\n    # y1, y2 = y\n    # p = args[PARAM_KEY]\n    # dy1 = -p(x) * y1\n    # dy2 = p(x) * y1 - y2\n    # return torch.stack([dy1, dy2])\n    raise NotImplementedError(\"TODO: implement your ODE\")\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/custom/ode_synthetic/","title":"ode_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/custom/ode_synthetic/#anypinn.cli.scaffold.custom.ode_synthetic","title":"<code>anypinn.cli.scaffold.custom.ode_synthetic</code>","text":"<p>Custom ODE \u2014 mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/custom/ode_synthetic/#anypinn.cli.scaffold.custom.ode_synthetic.validation","title":"<code>validation: ValidationRegistry = {}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/custom/ode_synthetic/#anypinn.cli.scaffold.custom.ode_synthetic.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/custom/ode_synthetic.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    # TODO: create and return your DataModule\n    # See anypinn.catalog for examples of DataModule subclasses.\n    raise NotImplementedError(\"TODO: implement create_data_module\")\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/custom/ode_synthetic/#anypinn.cli.scaffold.custom.ode_synthetic.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/custom/ode_synthetic.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    # TODO: define your problem\n    # props = ODEProperties(\n    #     ode=my_ode,\n    #     y0=torch.tensor([Y1_0, Y2_0]),\n    #     args={\n    #         # fixed arguments go here\n    #     },\n    # )\n    #\n    # fields = FieldsRegistry({\n    #     Y1_KEY: Field(config=hp.fields_config),\n    #     Y2_KEY: Field(config=hp.fields_config),\n    # })\n    # params = ParamsRegistry({\n    #     PARAM_KEY: Parameter(config=hp.params_config),\n    # })\n    #\n    # def predict_data(\n    #     x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry\n    # ) -&gt; Tensor:\n    #     return cast(Tensor, fields[Y1_KEY](x_data))\n    #\n    # return ODEInverseProblem(\n    #     props=props, hp=hp, fields=fields, params=params,\n    #     predict_data=predict_data,\n    # )\n    raise NotImplementedError(\"TODO: implement create_problem\")\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/custom/ode_synthetic/#anypinn.cli.scaffold.custom.ode_synthetic.my_ode","title":"<code>my_ode(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>TODO: implement your ODE system.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Independent variable (e.g. time). Shape: (N,)</p> required <code>y</code> <code>Tensor</code> <p>State variables. Each element has shape (N,)</p> required <code>args</code> <code>ArgsRegistry</code> <p>Parameters (both fixed and learnable).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>dy/dx stacked as a tensor.</p> Source code in <code>src/anypinn/cli/scaffold/custom/ode_synthetic.py</code> <pre><code>def my_ode(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"TODO: implement your ODE system.\n\n    Args:\n        x: Independent variable (e.g. time). Shape: (N,)\n        y: State variables. Each element has shape (N,)\n        args: Parameters (both fixed and learnable).\n\n    Returns:\n        dy/dx stacked as a tensor.\n    \"\"\"\n    # Example:\n    # y1, y2 = y\n    # p = args[PARAM_KEY]\n    # dy1 = -p(x) * y1\n    # dy2 = p(x) * y1 - y2\n    # return torch.stack([dy1, dy2])\n    raise NotImplementedError(\"TODO: implement your ODE\")\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/","title":"damped_oscillator","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/#anypinn.cli.scaffold.damped_oscillator","title":"<code>anypinn.cli.scaffold.damped_oscillator</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/config_csv/","title":"config_csv","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/config_csv/#anypinn.cli.scaffold.damped_oscillator.config_csv","title":"<code>anypinn.cli.scaffold.damped_oscillator.config_csv</code>","text":"<p>Damped oscillator \u2014 training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/config_csv/#anypinn.cli.scaffold.damped_oscillator.config_csv.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/config_csv/#anypinn.cli.scaffold.damped_oscillator.config_csv.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/config_csv/#anypinn.cli.scaffold.damped_oscillator.config_csv.hp","title":"<code>hp = ODEHyperparameters(lr=0.001, max_epochs=2000, gradient_clip_val=0.1, training_data=(IngestionConfig(batch_size=100, data_ratio=2, collocations=6000, df_path=(Path('./data/data.csv')), y_columns=['x_obs'])), fields_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.3)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=55, threshold=0.005, min_lr=1e-06)), pde_weight=0.0001, ic_weight=15, data_weight=5)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/config_synthetic/","title":"config_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/config_synthetic/#anypinn.cli.scaffold.damped_oscillator.config_synthetic","title":"<code>anypinn.cli.scaffold.damped_oscillator.config_synthetic</code>","text":"<p>Damped oscillator \u2014 training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/config_synthetic/#anypinn.cli.scaffold.damped_oscillator.config_synthetic.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/config_synthetic/#anypinn.cli.scaffold.damped_oscillator.config_synthetic.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/config_synthetic/#anypinn.cli.scaffold.damped_oscillator.config_synthetic.hp","title":"<code>hp = ODEHyperparameters(lr=0.001, max_epochs=2000, gradient_clip_val=0.1, training_data=(GenerationConfig(batch_size=100, data_ratio=2, collocations=6000, x=(torch.linspace(start=0, end=5, steps=200)), args_to_train={}, noise_level=0)), fields_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.3)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=55, threshold=0.005, min_lr=1e-06)), pde_weight=0.0001, ic_weight=15, data_weight=5)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_csv/","title":"ode_csv","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_csv/#anypinn.cli.scaffold.damped_oscillator.ode_csv","title":"<code>anypinn.cli.scaffold.damped_oscillator.ode_csv</code>","text":"<p>Damped oscillator \u2014 mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_csv/#anypinn.cli.scaffold.damped_oscillator.ode_csv.NOISE_STD","title":"<code>NOISE_STD = 0.02</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_csv/#anypinn.cli.scaffold.damped_oscillator.ode_csv.OMEGA_KEY","title":"<code>OMEGA_KEY = 'omega0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_csv/#anypinn.cli.scaffold.damped_oscillator.ode_csv.TRUE_OMEGA0","title":"<code>TRUE_OMEGA0 = 2 * math.pi</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_csv/#anypinn.cli.scaffold.damped_oscillator.ode_csv.TRUE_ZETA","title":"<code>TRUE_ZETA = 0.15</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_csv/#anypinn.cli.scaffold.damped_oscillator.ode_csv.T_TOTAL","title":"<code>T_TOTAL = 5</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_csv/#anypinn.cli.scaffold.damped_oscillator.ode_csv.V0","title":"<code>V0 = 0.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_csv/#anypinn.cli.scaffold.damped_oscillator.ode_csv.V_KEY","title":"<code>V_KEY = 'v'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_csv/#anypinn.cli.scaffold.damped_oscillator.ode_csv.X0","title":"<code>X0 = 1.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_csv/#anypinn.cli.scaffold.damped_oscillator.ode_csv.X_KEY","title":"<code>X_KEY = 'x'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_csv/#anypinn.cli.scaffold.damped_oscillator.ode_csv.ZETA_KEY","title":"<code>ZETA_KEY = 'zeta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_csv/#anypinn.cli.scaffold.damped_oscillator.ode_csv.validation","title":"<code>validation: ValidationRegistry = {}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_csv/#anypinn.cli.scaffold.damped_oscillator.ode_csv.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/damped_oscillator/ode_csv.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    from anypinn.catalog.damped_oscillator import DampedOscillatorDataModule\n\n    def oscillator_unscaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n        pos, vel = y\n        z = args[ZETA_KEY]\n        omega0 = args[OMEGA_KEY]\n        dx = vel\n        dv = -2 * z(x) * omega0(x) * vel - omega0(x) ** 2 * pos\n        return torch.stack([dx, dv])\n\n    gen_props = ODEProperties(\n        ode=oscillator_unscaled,\n        y0=torch.tensor([X0, V0]),\n        args={\n            ZETA_KEY: Argument(TRUE_ZETA),\n            OMEGA_KEY: Argument(TRUE_OMEGA0),\n        },\n    )\n\n    return DampedOscillatorDataModule(\n        hp=hp,\n        gen_props=gen_props,\n        noise_std=NOISE_STD,\n        validation=validation,\n        callbacks=[DataScaling(y_scale=1.0)],\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_csv/#anypinn.cli.scaffold.damped_oscillator.ode_csv.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/damped_oscillator/ode_csv.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    props = ODEProperties(\n        ode=oscillator,\n        y0=torch.tensor([X0, V0]),\n        args={\n            OMEGA_KEY: Argument(TRUE_OMEGA0),\n        },\n    )\n\n    fields = FieldsRegistry(\n        {\n            X_KEY: Field(config=hp.fields_config),\n            V_KEY: Field(config=hp.fields_config),\n        }\n    )\n    params = ParamsRegistry(\n        {\n            ZETA_KEY: Parameter(config=hp.params_config),\n        }\n    )\n\n    def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n        x_pred = fields[X_KEY](x_data)\n        return x_pred.unsqueeze(1)\n\n    return ODEInverseProblem(\n        props=props,\n        hp=hp,\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_csv/#anypinn.cli.scaffold.damped_oscillator.ode_csv.oscillator","title":"<code>oscillator(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>Scaled damped oscillator ODE: \\(dx/dt = v\\), \\(dv/dt = -2 zeta omega_0 v - omega_0^2 x\\).</p> Source code in <code>src/anypinn/cli/scaffold/damped_oscillator/ode_csv.py</code> <pre><code>def oscillator(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"Scaled damped oscillator ODE: $dx/dt = v$, $dv/dt = -2 zeta omega_0 v - omega_0^2 x$.\"\"\"\n    pos, vel = y\n    z = args[ZETA_KEY]\n    omega0 = args[OMEGA_KEY]\n\n    dx = vel\n    dv = -2 * z(x) * omega0(x) * vel - omega0(x) ** 2 * pos\n\n    dx = dx * T_TOTAL\n    dv = dv * T_TOTAL\n    return torch.stack([dx, dv])\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_synthetic/","title":"ode_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_synthetic/#anypinn.cli.scaffold.damped_oscillator.ode_synthetic","title":"<code>anypinn.cli.scaffold.damped_oscillator.ode_synthetic</code>","text":"<p>Damped oscillator \u2014 mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_synthetic/#anypinn.cli.scaffold.damped_oscillator.ode_synthetic.NOISE_STD","title":"<code>NOISE_STD = 0.02</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_synthetic/#anypinn.cli.scaffold.damped_oscillator.ode_synthetic.OMEGA_KEY","title":"<code>OMEGA_KEY = 'omega0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_synthetic/#anypinn.cli.scaffold.damped_oscillator.ode_synthetic.TRUE_OMEGA0","title":"<code>TRUE_OMEGA0 = 2 * math.pi</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_synthetic/#anypinn.cli.scaffold.damped_oscillator.ode_synthetic.TRUE_ZETA","title":"<code>TRUE_ZETA = 0.15</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_synthetic/#anypinn.cli.scaffold.damped_oscillator.ode_synthetic.T_TOTAL","title":"<code>T_TOTAL = 5</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_synthetic/#anypinn.cli.scaffold.damped_oscillator.ode_synthetic.V0","title":"<code>V0 = 0.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_synthetic/#anypinn.cli.scaffold.damped_oscillator.ode_synthetic.V_KEY","title":"<code>V_KEY = 'v'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_synthetic/#anypinn.cli.scaffold.damped_oscillator.ode_synthetic.X0","title":"<code>X0 = 1.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_synthetic/#anypinn.cli.scaffold.damped_oscillator.ode_synthetic.X_KEY","title":"<code>X_KEY = 'x'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_synthetic/#anypinn.cli.scaffold.damped_oscillator.ode_synthetic.ZETA_KEY","title":"<code>ZETA_KEY = 'zeta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_synthetic/#anypinn.cli.scaffold.damped_oscillator.ode_synthetic.validation","title":"<code>validation: ValidationRegistry = {ZETA_KEY: lambda x: torch.full_like(x, TRUE_ZETA)}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_synthetic/#anypinn.cli.scaffold.damped_oscillator.ode_synthetic.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/damped_oscillator/ode_synthetic.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    from anypinn.catalog.damped_oscillator import DampedOscillatorDataModule\n\n    def oscillator_unscaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n        pos, vel = y\n        z = args[ZETA_KEY]\n        omega0 = args[OMEGA_KEY]\n        dx = vel\n        dv = -2 * z(x) * omega0(x) * vel - omega0(x) ** 2 * pos\n        return torch.stack([dx, dv])\n\n    gen_props = ODEProperties(\n        ode=oscillator_unscaled,\n        y0=torch.tensor([X0, V0]),\n        args={\n            ZETA_KEY: Argument(TRUE_ZETA),\n            OMEGA_KEY: Argument(TRUE_OMEGA0),\n        },\n    )\n\n    return DampedOscillatorDataModule(\n        hp=hp,\n        gen_props=gen_props,\n        noise_std=NOISE_STD,\n        validation=validation,\n        callbacks=[DataScaling(y_scale=1.0)],\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_synthetic/#anypinn.cli.scaffold.damped_oscillator.ode_synthetic.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/damped_oscillator/ode_synthetic.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    props = ODEProperties(\n        ode=oscillator,\n        y0=torch.tensor([X0, V0]),\n        args={\n            OMEGA_KEY: Argument(TRUE_OMEGA0),\n        },\n    )\n\n    fields = FieldsRegistry(\n        {\n            X_KEY: Field(config=hp.fields_config),\n            V_KEY: Field(config=hp.fields_config),\n        }\n    )\n    params = ParamsRegistry(\n        {\n            ZETA_KEY: Parameter(config=hp.params_config),\n        }\n    )\n\n    def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n        x_pred = fields[X_KEY](x_data)\n        return x_pred.unsqueeze(1)\n\n    return ODEInverseProblem(\n        props=props,\n        hp=hp,\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/damped_oscillator/ode_synthetic/#anypinn.cli.scaffold.damped_oscillator.ode_synthetic.oscillator","title":"<code>oscillator(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>Scaled damped oscillator ODE: \\(dx/dt = v\\), \\(dv/dt = -2 zeta omega_0 v - omega_0^2 x\\).</p> Source code in <code>src/anypinn/cli/scaffold/damped_oscillator/ode_synthetic.py</code> <pre><code>def oscillator(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"Scaled damped oscillator ODE: $dx/dt = v$, $dv/dt = -2 zeta omega_0 v - omega_0^2 x$.\"\"\"\n    pos, vel = y\n    z = args[ZETA_KEY]\n    omega0 = args[OMEGA_KEY]\n\n    dx = vel\n    dv = -2 * z(x) * omega0(x) * vel - omega0(x) ** 2 * pos\n\n    dx = dx * T_TOTAL\n    dv = dv * T_TOTAL\n    return torch.stack([dx, dv])\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/","title":"fitzhugh_nagumo","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/#anypinn.cli.scaffold.fitzhugh_nagumo","title":"<code>anypinn.cli.scaffold.fitzhugh_nagumo</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/config_csv/","title":"config_csv","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/config_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.config_csv","title":"<code>anypinn.cli.scaffold.fitzhugh_nagumo.config_csv</code>","text":"<p>FitzHugh-Nagumo neuron model \u2014 training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/config_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.config_csv.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/config_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.config_csv.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/config_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.config_csv.hp","title":"<code>hp = ODEHyperparameters(lr=1.0, max_epochs=500, criterion='mse', optimizer=(LBFGSConfig(lr=1.0, max_iter=20, history_size=100)), training_data=(IngestionConfig(batch_size=300, data_ratio=2, collocations=5000, collocation_sampler='latin_hypercube', df_path=(Path('./data/data.csv')), y_columns=['v'])), fields_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.3)), smma_stopping=(SMMAStoppingConfig(window=50, threshold=1e-06, lookback=100)), pde_weight=0.001, ic_weight=10, data_weight=5)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/config_synthetic/","title":"config_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/config_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.config_synthetic","title":"<code>anypinn.cli.scaffold.fitzhugh_nagumo.config_synthetic</code>","text":"<p>FitzHugh-Nagumo neuron model \u2014 training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/config_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.config_synthetic.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/config_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.config_synthetic.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/config_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.config_synthetic.hp","title":"<code>hp = ODEHyperparameters(lr=0.001, max_epochs=3000, gradient_clip_val=0.5, criterion='mse', training_data=(GenerationConfig(batch_size=300, data_ratio=2, collocations=5000, collocation_sampler='latin_hypercube', x=(torch.linspace(start=0, end=50, steps=300)), args_to_train={}, noise_level=0)), fields_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.3)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=55, threshold=0.005, min_lr=1e-06)), smma_stopping=(SMMAStoppingConfig(window=50, threshold=1e-06, lookback=100)), pde_weight=0.001, ic_weight=10, data_weight=5)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/","title":"ode_csv","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv","title":"<code>anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv</code>","text":"<p>FitzHugh-Nagumo neuron model \u2014 mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv.A_KEY","title":"<code>A_KEY = 'a'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv.B","title":"<code>B = 0.8</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv.EPSILON_KEY","title":"<code>EPSILON_KEY = 'epsilon'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv.I_EXT","title":"<code>I_EXT = 0.5</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv.NOISE_STD","title":"<code>NOISE_STD = 0.05</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv.TRUE_A","title":"<code>TRUE_A = 0.7</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv.TRUE_EPSILON","title":"<code>TRUE_EPSILON = 0.08</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv.T_TOTAL","title":"<code>T_TOTAL = 50</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv.V0","title":"<code>V0 = -1.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv.V_KEY","title":"<code>V_KEY = 'v'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv.W0","title":"<code>W0 = 1.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv.W_KEY","title":"<code>W_KEY = 'w'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv.validation","title":"<code>validation: ValidationRegistry = {}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    from anypinn.catalog.fitzhugh_nagumo import FitzHughNagumoDataModule\n\n    def fhn_unscaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n        v, w = y\n        eps = args[EPSILON_KEY]\n        a = args[A_KEY]\n        dv = v - v**3 / 3 - w + I_EXT\n        dw = eps(x) * (v + a(x) - B * w)\n        return torch.stack([dv, dw])\n\n    gen_props = ODEProperties(\n        ode=fhn_unscaled,\n        y0=torch.tensor([V0, W0]),\n        args={\n            EPSILON_KEY: Argument(TRUE_EPSILON),\n            A_KEY: Argument(TRUE_A),\n        },\n    )\n\n    return FitzHughNagumoDataModule(\n        hp=hp,\n        gen_props=gen_props,\n        noise_std=NOISE_STD,\n        validation=validation,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    props = ODEProperties(\n        ode=fhn_scaled,\n        y0=torch.tensor([V0, W0]),\n        args={},\n    )\n\n    fields = FieldsRegistry(\n        {\n            V_KEY: Field(config=hp.fields_config),\n            W_KEY: Field(config=hp.fields_config),\n        }\n    )\n    params = ParamsRegistry(\n        {\n            EPSILON_KEY: Parameter(config=hp.params_config),\n            A_KEY: Parameter(config=hp.params_config),\n        }\n    )\n\n    def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n        v_pred = fields[V_KEY](x_data)\n        return v_pred.unsqueeze(1)  # (N, 1, 1) \u2014 only v is observed\n\n    return ODEInverseProblem(\n        props=props,\n        hp=hp,\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_csv.fhn_scaled","title":"<code>fhn_scaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>Scaled FHN ODE for training. Time scaled by T_TOTAL.</p> Source code in <code>src/anypinn/cli/scaffold/fitzhugh_nagumo/ode_csv.py</code> <pre><code>def fhn_scaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"Scaled FHN ODE for training. Time scaled by T_TOTAL.\"\"\"\n    v, w = y\n    eps = args[EPSILON_KEY]\n    a = args[A_KEY]\n\n    dv = (v - v**3 / 3 - w + I_EXT) * T_TOTAL\n    dw = eps(x) * (v + a(x) - B * w) * T_TOTAL\n    return torch.stack([dv, dw])\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/","title":"ode_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic","title":"<code>anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic</code>","text":"<p>FitzHugh-Nagumo neuron model \u2014 mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic.A_KEY","title":"<code>A_KEY = 'a'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic.B","title":"<code>B = 0.8</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic.EPSILON_KEY","title":"<code>EPSILON_KEY = 'epsilon'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic.I_EXT","title":"<code>I_EXT = 0.5</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic.NOISE_STD","title":"<code>NOISE_STD = 0.05</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic.TRUE_A","title":"<code>TRUE_A = 0.7</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic.TRUE_EPSILON","title":"<code>TRUE_EPSILON = 0.08</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic.T_TOTAL","title":"<code>T_TOTAL = 50</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic.V0","title":"<code>V0 = -1.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic.V_KEY","title":"<code>V_KEY = 'v'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic.W0","title":"<code>W0 = 1.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic.W_KEY","title":"<code>W_KEY = 'w'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic.validation","title":"<code>validation: ValidationRegistry = {EPSILON_KEY: lambda x: torch.full_like(x, TRUE_EPSILON), A_KEY: lambda x: torch.full_like(x, TRUE_A)}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    from anypinn.catalog.fitzhugh_nagumo import FitzHughNagumoDataModule\n\n    def fhn_unscaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n        v, w = y\n        eps = args[EPSILON_KEY]\n        a = args[A_KEY]\n        dv = v - v**3 / 3 - w + I_EXT\n        dw = eps(x) * (v + a(x) - B * w)\n        return torch.stack([dv, dw])\n\n    gen_props = ODEProperties(\n        ode=fhn_unscaled,\n        y0=torch.tensor([V0, W0]),\n        args={\n            EPSILON_KEY: Argument(TRUE_EPSILON),\n            A_KEY: Argument(TRUE_A),\n        },\n    )\n\n    return FitzHughNagumoDataModule(\n        hp=hp,\n        gen_props=gen_props,\n        noise_std=NOISE_STD,\n        validation=validation,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    props = ODEProperties(\n        ode=fhn_scaled,\n        y0=torch.tensor([V0, W0]),\n        args={},\n    )\n\n    fields = FieldsRegistry(\n        {\n            V_KEY: Field(config=hp.fields_config),\n            W_KEY: Field(config=hp.fields_config),\n        }\n    )\n    params = ParamsRegistry(\n        {\n            EPSILON_KEY: Parameter(config=hp.params_config),\n            A_KEY: Parameter(config=hp.params_config),\n        }\n    )\n\n    def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n        v_pred = fields[V_KEY](x_data)\n        return v_pred.unsqueeze(1)  # (N, 1, 1) \u2014 only v is observed\n\n    return ODEInverseProblem(\n        props=props,\n        hp=hp,\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic/#anypinn.cli.scaffold.fitzhugh_nagumo.ode_synthetic.fhn_scaled","title":"<code>fhn_scaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>Scaled FHN ODE for training. Time scaled by T_TOTAL.</p> Source code in <code>src/anypinn/cli/scaffold/fitzhugh_nagumo/ode_synthetic.py</code> <pre><code>def fhn_scaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"Scaled FHN ODE for training. Time scaled by T_TOTAL.\"\"\"\n    v, w = y\n    eps = args[EPSILON_KEY]\n    a = args[A_KEY]\n\n    dv = (v - v**3 / 3 - w + I_EXT) * T_TOTAL\n    dw = eps(x) * (v + a(x) - B * w) * T_TOTAL\n    return torch.stack([dv, dw])\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/","title":"gray_scott_2d","text":""},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/#anypinn.cli.scaffold.gray_scott_2d","title":"<code>anypinn.cli.scaffold.gray_scott_2d</code>","text":""},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/config_csv/","title":"config_csv","text":""},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/config_csv/#anypinn.cli.scaffold.gray_scott_2d.config_csv","title":"<code>anypinn.cli.scaffold.gray_scott_2d.config_csv</code>","text":"<p>Gray-Scott 2D Reaction-Diffusion \u2014 training configuration (CSV).</p>"},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/config_csv/#anypinn.cli.scaffold.gray_scott_2d.config_csv.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/config_csv/#anypinn.cli.scaffold.gray_scott_2d.config_csv.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/config_csv/#anypinn.cli.scaffold.gray_scott_2d.config_csv.hp","title":"<code>hp = PINNHyperparameters(lr=0.001, max_epochs=3000, gradient_clip_val=0.5, training_data=(IngestionConfig(batch_size=256, data_ratio=2, collocations=10000, collocation_sampler='latin_hypercube', df_path=(Path('./data/data.csv')), y_columns=['u', 'v'])), fields_config=(MLPConfig(in_dim=39, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.01)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=100, threshold=0.0001, min_lr=1e-06)))</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/config_synthetic/","title":"config_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/config_synthetic/#anypinn.cli.scaffold.gray_scott_2d.config_synthetic","title":"<code>anypinn.cli.scaffold.gray_scott_2d.config_synthetic</code>","text":"<p>Gray-Scott 2D Reaction-Diffusion \u2014 training configuration (synthetic).</p>"},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/config_synthetic/#anypinn.cli.scaffold.gray_scott_2d.config_synthetic.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/config_synthetic/#anypinn.cli.scaffold.gray_scott_2d.config_synthetic.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/config_synthetic/#anypinn.cli.scaffold.gray_scott_2d.config_synthetic.hp","title":"<code>hp = PINNHyperparameters(lr=0.001, max_epochs=3000, gradient_clip_val=0.5, training_data=(GenerationConfig(batch_size=256, data_ratio=2, collocations=10000, collocation_sampler='latin_hypercube', x=(torch.linspace(0, 1, steps=50)), args_to_train={}, noise_level=0)), fields_config=(MLPConfig(in_dim=39, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.01)), scheduler=(CosineAnnealingConfig(T_max=1500, eta_min=1e-06)), smma_stopping=(SMMAStoppingConfig(window=20, threshold=0.005, lookback=100)))</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/ode_csv/","title":"ode_csv","text":""},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/ode_csv/#anypinn.cli.scaffold.gray_scott_2d.ode_csv","title":"<code>anypinn.cli.scaffold.gray_scott_2d.ode_csv</code>","text":"<p>Gray-Scott 2D Reaction-Diffusion \u2014 inverse PDE problem definition (CSV data).</p>"},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/ode_csv/#anypinn.cli.scaffold.gray_scott_2d.ode_csv.validation","title":"<code>validation: ValidationRegistry = {DU_KEY: lambda x: torch.full_like(x, TRUE_DU), DV_KEY: lambda x: torch.full_like(x, TRUE_DV), F_KEY: lambda x: torch.full_like(x, TRUE_F), K_KEY: lambda x: torch.full_like(x, TRUE_K)}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/ode_csv/#anypinn.cli.scaffold.gray_scott_2d.ode_csv.create_data_module","title":"<code>create_data_module(hp: PINNHyperparameters) -&gt; GrayScott2DDataModule</code>","text":"Source code in <code>src/anypinn/cli/scaffold/gray_scott_2d/ode_csv.py</code> <pre><code>def create_data_module(hp: PINNHyperparameters) -&gt; GrayScott2DDataModule:\n    return GrayScott2DDataModule(\n        hp=hp,\n        true_du=TRUE_DU,\n        true_dv=TRUE_DV,\n        true_f=TRUE_F,\n        true_k=TRUE_K,\n        validation=validation,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/ode_csv/#anypinn.cli.scaffold.gray_scott_2d.ode_csv.create_problem","title":"<code>create_problem(hp: PINNHyperparameters) -&gt; Problem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/gray_scott_2d/ode_csv.py</code> <pre><code>def create_problem(hp: PINNHyperparameters) -&gt; Problem:\n    encode = FourierEncoding(num_frequencies=6)\n    field_u = Field(\n        config=MLPConfig(\n            in_dim=encode.out_dim(3),\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=hp.fields_config.output_activation,\n            encode=encode,\n        )\n    )\n    field_v = Field(\n        config=MLPConfig(\n            in_dim=encode.out_dim(3),\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=hp.fields_config.output_activation,\n            encode=encode,\n        )\n    )\n    param_du = Parameter(config=ScalarConfig(init_value=hp.params_config.init_value))\n    param_dv = Parameter(config=ScalarConfig(init_value=hp.params_config.init_value))\n    param_f = Parameter(config=ScalarConfig(init_value=hp.params_config.init_value))\n    param_k = Parameter(config=ScalarConfig(init_value=hp.params_config.init_value))\n\n    fields = FieldsRegistry({U_KEY: field_u, V_KEY: field_v})\n    params = ParamsRegistry(\n        {\n            DU_KEY: param_du,\n            DV_KEY: param_dv,\n            F_KEY: param_f,\n            K_KEY: param_k,\n        }\n    )\n\n    bcs = [\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_left_edge, value=_zero, n_pts=100),\n            field_u,\n            normal_dim=0,\n            log_key=\"loss/bc_left_u\",\n            weight=10.0,\n        ),\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_left_edge, value=_zero, n_pts=100),\n            field_v,\n            normal_dim=0,\n            log_key=\"loss/bc_left_v\",\n            weight=10.0,\n        ),\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_right_edge, value=_zero, n_pts=100),\n            field_u,\n            normal_dim=0,\n            log_key=\"loss/bc_right_u\",\n            weight=10.0,\n        ),\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_right_edge, value=_zero, n_pts=100),\n            field_v,\n            normal_dim=0,\n            log_key=\"loss/bc_right_v\",\n            weight=10.0,\n        ),\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_bottom_edge, value=_zero, n_pts=100),\n            field_u,\n            normal_dim=1,\n            log_key=\"loss/bc_bottom_u\",\n            weight=10.0,\n        ),\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_bottom_edge, value=_zero, n_pts=100),\n            field_v,\n            normal_dim=1,\n            log_key=\"loss/bc_bottom_v\",\n            weight=10.0,\n        ),\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_top_edge, value=_zero, n_pts=100),\n            field_u,\n            normal_dim=1,\n            log_key=\"loss/bc_top_u\",\n            weight=10.0,\n        ),\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_top_edge, value=_zero, n_pts=100),\n            field_v,\n            normal_dim=1,\n            log_key=\"loss/bc_top_v\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_initial_condition, value=_ic_u, n_pts=200),\n            field_u,\n            log_key=\"loss/ic_u\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_initial_condition, value=_ic_v, n_pts=200),\n            field_v,\n            log_key=\"loss/ic_v\",\n            weight=10.0,\n        ),\n    ]\n\n    pde_u = PDEResidualConstraint(\n        fields=fields,\n        params=params,\n        residual_fn=gs_residual_u,\n        log_key=\"loss/pde_u\",\n        weight=1.0,\n    )\n    pde_v = PDEResidualConstraint(\n        fields=fields,\n        params=params,\n        residual_fn=gs_residual_v,\n        log_key=\"loss/pde_v\",\n        weight=1.0,\n    )\n\n    data = DataConstraint(\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n        weight=5.0,\n    )\n\n    return Problem(\n        constraints=[pde_u, pde_v, *bcs, data],\n        criterion=build_criterion(hp.criterion),\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/ode_csv/#anypinn.cli.scaffold.gray_scott_2d.ode_csv.gs_residual_u","title":"<code>gs_residual_u(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor</code>","text":"<p>PDE residual for u: du/dt_norm - T * (D_u lap_u - uv^2 + F(1-u)) = 0.</p> Source code in <code>src/anypinn/cli/scaffold/gray_scott_2d/ode_csv.py</code> <pre><code>def gs_residual_u(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor:\n    \"\"\"PDE residual for u: du/dt_norm - T * (D_u lap_u - uv^2 + F(1-u)) = 0.\"\"\"\n    u = fields[U_KEY](x)\n    v = fields[V_KEY](x)\n    du = params[DU_KEY](x)\n    f = params[F_KEY](x)\n    du_dt = partial(u, x, dim=2, order=1)\n    lap_u = partial(u, x, dim=0, order=2) + partial(u, x, dim=1, order=2)\n    return du_dt - (du * lap_u - u * v**2 + f * (1 - u)) * T_TOTAL\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/ode_csv/#anypinn.cli.scaffold.gray_scott_2d.ode_csv.gs_residual_v","title":"<code>gs_residual_v(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor</code>","text":"<p>PDE residual for v: dv/dt_norm - T * (D_v lap_v + uv^2 - (F+k)v) = 0.</p> Source code in <code>src/anypinn/cli/scaffold/gray_scott_2d/ode_csv.py</code> <pre><code>def gs_residual_v(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor:\n    \"\"\"PDE residual for v: dv/dt_norm - T * (D_v lap_v + uv^2 - (F+k)v) = 0.\"\"\"\n    u = fields[U_KEY](x)\n    v = fields[V_KEY](x)\n    dv = params[DV_KEY](x)\n    f = params[F_KEY](x)\n    k = params[K_KEY](x)\n    dv_dt = partial(v, x, dim=2, order=1)\n    lap_v = partial(v, x, dim=0, order=2) + partial(v, x, dim=1, order=2)\n    return dv_dt - (dv * lap_v + u * v**2 - (f + k) * v) * T_TOTAL\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/ode_csv/#anypinn.cli.scaffold.gray_scott_2d.ode_csv.predict_data","title":"<code>predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/cli/scaffold/gray_scott_2d/ode_csv.py</code> <pre><code>def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n    u_pred = fields[U_KEY](x_data)\n    v_pred = fields[V_KEY](x_data)\n    return torch.stack([u_pred, v_pred], dim=1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/ode_synthetic/","title":"ode_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/ode_synthetic/#anypinn.cli.scaffold.gray_scott_2d.ode_synthetic","title":"<code>anypinn.cli.scaffold.gray_scott_2d.ode_synthetic</code>","text":"<p>Gray-Scott 2D Reaction-Diffusion \u2014 inverse PDE problem definition (synthetic).</p>"},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/ode_synthetic/#anypinn.cli.scaffold.gray_scott_2d.ode_synthetic.validation","title":"<code>validation: ValidationRegistry = {DU_KEY: lambda x: torch.full_like(x, TRUE_DU), DV_KEY: lambda x: torch.full_like(x, TRUE_DV), F_KEY: lambda x: torch.full_like(x, TRUE_F), K_KEY: lambda x: torch.full_like(x, TRUE_K)}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/ode_synthetic/#anypinn.cli.scaffold.gray_scott_2d.ode_synthetic.create_data_module","title":"<code>create_data_module(hp: PINNHyperparameters) -&gt; GrayScott2DDataModule</code>","text":"Source code in <code>src/anypinn/cli/scaffold/gray_scott_2d/ode_synthetic.py</code> <pre><code>def create_data_module(hp: PINNHyperparameters) -&gt; GrayScott2DDataModule:\n    return GrayScott2DDataModule(\n        hp=hp,\n        true_du=TRUE_DU,\n        true_dv=TRUE_DV,\n        true_f=TRUE_F,\n        true_k=TRUE_K,\n        validation=validation,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/ode_synthetic/#anypinn.cli.scaffold.gray_scott_2d.ode_synthetic.create_problem","title":"<code>create_problem(hp: PINNHyperparameters) -&gt; Problem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/gray_scott_2d/ode_synthetic.py</code> <pre><code>def create_problem(hp: PINNHyperparameters) -&gt; Problem:\n    encode = FourierEncoding(num_frequencies=6)\n    field_u = Field(\n        config=MLPConfig(\n            in_dim=encode.out_dim(3),\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=hp.fields_config.output_activation,\n            encode=encode,\n        )\n    )\n    field_v = Field(\n        config=MLPConfig(\n            in_dim=encode.out_dim(3),\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=hp.fields_config.output_activation,\n            encode=encode,\n        )\n    )\n    param_du = Parameter(config=ScalarConfig(init_value=hp.params_config.init_value))\n    param_dv = Parameter(config=ScalarConfig(init_value=hp.params_config.init_value))\n    param_f = Parameter(config=ScalarConfig(init_value=hp.params_config.init_value))\n    param_k = Parameter(config=ScalarConfig(init_value=hp.params_config.init_value))\n\n    fields = FieldsRegistry({U_KEY: field_u, V_KEY: field_v})\n    params = ParamsRegistry(\n        {\n            DU_KEY: param_du,\n            DV_KEY: param_dv,\n            F_KEY: param_f,\n            K_KEY: param_k,\n        }\n    )\n\n    bcs = [\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_left_edge, value=_zero, n_pts=100),\n            field_u,\n            normal_dim=0,\n            log_key=\"loss/bc_left_u\",\n            weight=10.0,\n        ),\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_left_edge, value=_zero, n_pts=100),\n            field_v,\n            normal_dim=0,\n            log_key=\"loss/bc_left_v\",\n            weight=10.0,\n        ),\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_right_edge, value=_zero, n_pts=100),\n            field_u,\n            normal_dim=0,\n            log_key=\"loss/bc_right_u\",\n            weight=10.0,\n        ),\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_right_edge, value=_zero, n_pts=100),\n            field_v,\n            normal_dim=0,\n            log_key=\"loss/bc_right_v\",\n            weight=10.0,\n        ),\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_bottom_edge, value=_zero, n_pts=100),\n            field_u,\n            normal_dim=1,\n            log_key=\"loss/bc_bottom_u\",\n            weight=10.0,\n        ),\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_bottom_edge, value=_zero, n_pts=100),\n            field_v,\n            normal_dim=1,\n            log_key=\"loss/bc_bottom_v\",\n            weight=10.0,\n        ),\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_top_edge, value=_zero, n_pts=100),\n            field_u,\n            normal_dim=1,\n            log_key=\"loss/bc_top_u\",\n            weight=10.0,\n        ),\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_top_edge, value=_zero, n_pts=100),\n            field_v,\n            normal_dim=1,\n            log_key=\"loss/bc_top_v\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_initial_condition, value=_ic_u, n_pts=200),\n            field_u,\n            log_key=\"loss/ic_u\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_initial_condition, value=_ic_v, n_pts=200),\n            field_v,\n            log_key=\"loss/ic_v\",\n            weight=10.0,\n        ),\n    ]\n\n    pde_u = PDEResidualConstraint(\n        fields=fields,\n        params=params,\n        residual_fn=gs_residual_u,\n        log_key=\"loss/pde_u\",\n        weight=1.0,\n    )\n    pde_v = PDEResidualConstraint(\n        fields=fields,\n        params=params,\n        residual_fn=gs_residual_v,\n        log_key=\"loss/pde_v\",\n        weight=1.0,\n    )\n\n    data = DataConstraint(\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n        weight=5.0,\n    )\n\n    return Problem(\n        constraints=[pde_u, pde_v, *bcs, data],\n        criterion=build_criterion(hp.criterion),\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/ode_synthetic/#anypinn.cli.scaffold.gray_scott_2d.ode_synthetic.gs_residual_u","title":"<code>gs_residual_u(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor</code>","text":"<p>PDE residual for u: du/dt_norm - T * (D_u lap_u - uv^2 + F(1-u)) = 0.</p> Source code in <code>src/anypinn/cli/scaffold/gray_scott_2d/ode_synthetic.py</code> <pre><code>def gs_residual_u(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor:\n    \"\"\"PDE residual for u: du/dt_norm - T * (D_u lap_u - uv^2 + F(1-u)) = 0.\"\"\"\n    u = fields[U_KEY](x)\n    v = fields[V_KEY](x)\n    du = params[DU_KEY](x)\n    f = params[F_KEY](x)\n    du_dt = partial(u, x, dim=2, order=1)\n    lap_u = partial(u, x, dim=0, order=2) + partial(u, x, dim=1, order=2)\n    return du_dt - (du * lap_u - u * v**2 + f * (1 - u)) * T_TOTAL\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/ode_synthetic/#anypinn.cli.scaffold.gray_scott_2d.ode_synthetic.gs_residual_v","title":"<code>gs_residual_v(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor</code>","text":"<p>PDE residual for v: dv/dt_norm - T * (D_v lap_v + uv^2 - (F+k)v) = 0.</p> Source code in <code>src/anypinn/cli/scaffold/gray_scott_2d/ode_synthetic.py</code> <pre><code>def gs_residual_v(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor:\n    \"\"\"PDE residual for v: dv/dt_norm - T * (D_v lap_v + uv^2 - (F+k)v) = 0.\"\"\"\n    u = fields[U_KEY](x)\n    v = fields[V_KEY](x)\n    dv = params[DV_KEY](x)\n    f = params[F_KEY](x)\n    k = params[K_KEY](x)\n    dv_dt = partial(v, x, dim=2, order=1)\n    lap_v = partial(v, x, dim=0, order=2) + partial(v, x, dim=1, order=2)\n    return dv_dt - (dv * lap_v + u * v**2 - (f + k) * v) * T_TOTAL\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/gray_scott_2d/ode_synthetic/#anypinn.cli.scaffold.gray_scott_2d.ode_synthetic.predict_data","title":"<code>predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/cli/scaffold/gray_scott_2d/ode_synthetic.py</code> <pre><code>def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n    u_pred = fields[U_KEY](x_data)\n    v_pred = fields[V_KEY](x_data)\n    return torch.stack([u_pred, v_pred], dim=1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/heat_1d/","title":"heat_1d","text":""},{"location":"reference/anypinn/cli/scaffold/heat_1d/#anypinn.cli.scaffold.heat_1d","title":"<code>anypinn.cli.scaffold.heat_1d</code>","text":""},{"location":"reference/anypinn/cli/scaffold/heat_1d/config_csv/","title":"config_csv","text":""},{"location":"reference/anypinn/cli/scaffold/heat_1d/config_csv/#anypinn.cli.scaffold.heat_1d.config_csv","title":"<code>anypinn.cli.scaffold.heat_1d.config_csv</code>","text":"<p>Heat Equation 1D \u2014 training configuration (CSV).</p>"},{"location":"reference/anypinn/cli/scaffold/heat_1d/config_csv/#anypinn.cli.scaffold.heat_1d.config_csv.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/heat_1d/config_csv/#anypinn.cli.scaffold.heat_1d.config_csv.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/heat_1d/config_csv/#anypinn.cli.scaffold.heat_1d.config_csv.hp","title":"<code>hp = PINNHyperparameters(lr=0.001, max_epochs=3000, gradient_clip_val=0.5, training_data=(IngestionConfig(batch_size=128, data_ratio=2, collocations=8000, collocation_sampler='uniform', df_path=(Path('./data/data.csv')), y_columns=['u'])), fields_config=(MLPConfig(in_dim=26, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.5)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=100, threshold=0.0001, min_lr=1e-06)))</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/heat_1d/config_synthetic/","title":"config_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/heat_1d/config_synthetic/#anypinn.cli.scaffold.heat_1d.config_synthetic","title":"<code>anypinn.cli.scaffold.heat_1d.config_synthetic</code>","text":"<p>Heat Equation 1D \u2014 training configuration (synthetic).</p>"},{"location":"reference/anypinn/cli/scaffold/heat_1d/config_synthetic/#anypinn.cli.scaffold.heat_1d.config_synthetic.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/heat_1d/config_synthetic/#anypinn.cli.scaffold.heat_1d.config_synthetic.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/heat_1d/config_synthetic/#anypinn.cli.scaffold.heat_1d.config_synthetic.hp","title":"<code>hp = PINNHyperparameters(lr=0.001, max_epochs=3000, gradient_clip_val=0.5, training_data=(GenerationConfig(batch_size=128, data_ratio=2, collocations=8000, collocation_sampler='uniform', x=(torch.linspace(0, 1, steps=50)), args_to_train={}, noise_level=0)), fields_config=(MLPConfig(in_dim=26, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.5)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=100, threshold=0.0001, min_lr=1e-06)))</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/heat_1d/ode_csv/","title":"ode_csv","text":""},{"location":"reference/anypinn/cli/scaffold/heat_1d/ode_csv/#anypinn.cli.scaffold.heat_1d.ode_csv","title":"<code>anypinn.cli.scaffold.heat_1d.ode_csv</code>","text":"<p>Heat Equation 1D \u2014 inverse PDE problem definition (CSV data).</p>"},{"location":"reference/anypinn/cli/scaffold/heat_1d/ode_csv/#anypinn.cli.scaffold.heat_1d.ode_csv.GRID_SIZE","title":"<code>GRID_SIZE = 50</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/heat_1d/ode_csv/#anypinn.cli.scaffold.heat_1d.ode_csv.validation","title":"<code>validation: ValidationRegistry = {ALPHA_KEY: lambda x: torch.full_like(x, TRUE_ALPHA)}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/heat_1d/ode_csv/#anypinn.cli.scaffold.heat_1d.ode_csv.create_data_module","title":"<code>create_data_module(hp: PINNHyperparameters) -&gt; Heat1DDataModule</code>","text":"Source code in <code>src/anypinn/cli/scaffold/heat_1d/ode_csv.py</code> <pre><code>def create_data_module(hp: PINNHyperparameters) -&gt; Heat1DDataModule:\n    return Heat1DDataModule(\n        hp=hp,\n        true_alpha=TRUE_ALPHA,\n        grid_size=GRID_SIZE,\n        validation=validation,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/heat_1d/ode_csv/#anypinn.cli.scaffold.heat_1d.ode_csv.create_problem","title":"<code>create_problem(hp: PINNHyperparameters) -&gt; Problem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/heat_1d/ode_csv.py</code> <pre><code>def create_problem(hp: PINNHyperparameters) -&gt; Problem:\n    encode = FourierEncoding(num_frequencies=6)\n    field_u = Field(\n        config=MLPConfig(\n            in_dim=encode.out_dim(2),\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=hp.fields_config.output_activation,\n            encode=encode,\n        )\n    )\n    param_alpha = Parameter(\n        config=ScalarConfig(init_value=hp.params_config.init_value),\n    )\n\n    fields = FieldsRegistry({U_KEY: field_u})\n    params = ParamsRegistry({ALPHA_KEY: param_alpha})\n\n    bcs = [\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_left_boundary, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_left\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_right_boundary, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_right\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_initial_condition, value=_ic_value, n_pts=100),\n            field_u,\n            log_key=\"loss/ic\",\n            weight=10.0,\n        ),\n    ]\n\n    pde = PDEResidualConstraint(\n        fields=fields,\n        params=params,\n        residual_fn=heat_residual,\n        log_key=\"loss/pde_residual\",\n        weight=1.0,\n    )\n\n    data = DataConstraint(\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n        weight=5.0,\n    )\n\n    return Problem(\n        constraints=[pde, *bcs, data],\n        criterion=build_criterion(hp.criterion),\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/heat_1d/ode_csv/#anypinn.cli.scaffold.heat_1d.ode_csv.heat_residual","title":"<code>heat_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor</code>","text":"<p>PDE residual: du/dt - alpha * d2u/dx2 = 0.</p> Source code in <code>src/anypinn/cli/scaffold/heat_1d/ode_csv.py</code> <pre><code>def heat_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor:\n    \"\"\"PDE residual: du/dt - alpha * d2u/dx2 = 0.\"\"\"\n    u = fields[U_KEY](x)\n    alpha = params[ALPHA_KEY](x)\n    du_dt = partial(u, x, dim=1, order=1)\n    d2u_dx2 = partial(u, x, dim=0, order=2)\n    return du_dt - alpha * d2u_dx2\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/heat_1d/ode_csv/#anypinn.cli.scaffold.heat_1d.ode_csv.predict_data","title":"<code>predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/cli/scaffold/heat_1d/ode_csv.py</code> <pre><code>def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n    return fields[U_KEY](x_data).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/heat_1d/ode_synthetic/","title":"ode_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/heat_1d/ode_synthetic/#anypinn.cli.scaffold.heat_1d.ode_synthetic","title":"<code>anypinn.cli.scaffold.heat_1d.ode_synthetic</code>","text":"<p>Heat Equation 1D \u2014 inverse PDE problem definition (synthetic).</p>"},{"location":"reference/anypinn/cli/scaffold/heat_1d/ode_synthetic/#anypinn.cli.scaffold.heat_1d.ode_synthetic.GRID_SIZE","title":"<code>GRID_SIZE = 50</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/heat_1d/ode_synthetic/#anypinn.cli.scaffold.heat_1d.ode_synthetic.validation","title":"<code>validation: ValidationRegistry = {ALPHA_KEY: lambda x: torch.full_like(x, TRUE_ALPHA)}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/heat_1d/ode_synthetic/#anypinn.cli.scaffold.heat_1d.ode_synthetic.create_data_module","title":"<code>create_data_module(hp: PINNHyperparameters) -&gt; Heat1DDataModule</code>","text":"Source code in <code>src/anypinn/cli/scaffold/heat_1d/ode_synthetic.py</code> <pre><code>def create_data_module(hp: PINNHyperparameters) -&gt; Heat1DDataModule:\n    return Heat1DDataModule(\n        hp=hp,\n        true_alpha=TRUE_ALPHA,\n        grid_size=GRID_SIZE,\n        validation=validation,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/heat_1d/ode_synthetic/#anypinn.cli.scaffold.heat_1d.ode_synthetic.create_problem","title":"<code>create_problem(hp: PINNHyperparameters) -&gt; Problem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/heat_1d/ode_synthetic.py</code> <pre><code>def create_problem(hp: PINNHyperparameters) -&gt; Problem:\n    encode = FourierEncoding(num_frequencies=6)\n    field_u = Field(\n        config=MLPConfig(\n            in_dim=encode.out_dim(2),\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=hp.fields_config.output_activation,\n            encode=encode,\n        )\n    )\n    param_alpha = Parameter(\n        config=ScalarConfig(init_value=hp.params_config.init_value),\n    )\n\n    fields = FieldsRegistry({U_KEY: field_u})\n    params = ParamsRegistry({ALPHA_KEY: param_alpha})\n\n    bcs = [\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_left_boundary, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_left\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_right_boundary, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_right\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_initial_condition, value=_ic_value, n_pts=100),\n            field_u,\n            log_key=\"loss/ic\",\n            weight=10.0,\n        ),\n    ]\n\n    pde = PDEResidualConstraint(\n        fields=fields,\n        params=params,\n        residual_fn=heat_residual,\n        log_key=\"loss/pde_residual\",\n        weight=1.0,\n    )\n\n    data = DataConstraint(\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n        weight=5.0,\n    )\n\n    return Problem(\n        constraints=[pde, *bcs, data],\n        criterion=build_criterion(hp.criterion),\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/heat_1d/ode_synthetic/#anypinn.cli.scaffold.heat_1d.ode_synthetic.heat_residual","title":"<code>heat_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor</code>","text":"<p>PDE residual: du/dt - alpha * d2u/dx2 = 0.</p> Source code in <code>src/anypinn/cli/scaffold/heat_1d/ode_synthetic.py</code> <pre><code>def heat_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor:\n    \"\"\"PDE residual: du/dt - alpha * d2u/dx2 = 0.\"\"\"\n    u = fields[U_KEY](x)\n    alpha = params[ALPHA_KEY](x)\n    du_dt = partial(u, x, dim=1, order=1)\n    d2u_dx2 = partial(u, x, dim=0, order=2)\n    return du_dt - alpha * d2u_dx2\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/heat_1d/ode_synthetic/#anypinn.cli.scaffold.heat_1d.ode_synthetic.predict_data","title":"<code>predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/cli/scaffold/heat_1d/ode_synthetic.py</code> <pre><code>def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n    return fields[U_KEY](x_data).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/","title":"inverse_diffusivity","text":""},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/#anypinn.cli.scaffold.inverse_diffusivity","title":"<code>anypinn.cli.scaffold.inverse_diffusivity</code>","text":""},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/config_csv/","title":"config_csv","text":""},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/config_csv/#anypinn.cli.scaffold.inverse_diffusivity.config_csv","title":"<code>anypinn.cli.scaffold.inverse_diffusivity.config_csv</code>","text":"<p>Inverse Diffusivity \u2014 training configuration (CSV).</p>"},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/config_csv/#anypinn.cli.scaffold.inverse_diffusivity.config_csv.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/config_csv/#anypinn.cli.scaffold.inverse_diffusivity.config_csv.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/config_csv/#anypinn.cli.scaffold.inverse_diffusivity.config_csv.hp","title":"<code>hp = PINNHyperparameters(lr=0.001, max_epochs=3000, gradient_clip_val=0.5, training_data=(IngestionConfig(batch_size=128, data_ratio=2, collocations=8000, collocation_sampler='uniform', df_path=(Path('./data/data.csv')), y_columns=['u'])), fields_config=(MLPConfig(in_dim=26, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.0)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=100, threshold=0.0001, min_lr=1e-06)))</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/config_synthetic/","title":"config_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/config_synthetic/#anypinn.cli.scaffold.inverse_diffusivity.config_synthetic","title":"<code>anypinn.cli.scaffold.inverse_diffusivity.config_synthetic</code>","text":"<p>Inverse Diffusivity \u2014 training configuration (synthetic).</p>"},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/config_synthetic/#anypinn.cli.scaffold.inverse_diffusivity.config_synthetic.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/config_synthetic/#anypinn.cli.scaffold.inverse_diffusivity.config_synthetic.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/config_synthetic/#anypinn.cli.scaffold.inverse_diffusivity.config_synthetic.hp","title":"<code>hp = PINNHyperparameters(lr=0.001, max_epochs=3000, gradient_clip_val=0.5, training_data=(GenerationConfig(batch_size=128, data_ratio=2, collocations=8000, collocation_sampler='uniform', x=(torch.linspace(0, 1, steps=50)), args_to_train={}, noise_level=0)), fields_config=(MLPConfig(in_dim=26, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.0)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=100, threshold=0.0001, min_lr=1e-06)))</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/ode_csv/","title":"ode_csv","text":""},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/ode_csv/#anypinn.cli.scaffold.inverse_diffusivity.ode_csv","title":"<code>anypinn.cli.scaffold.inverse_diffusivity.ode_csv</code>","text":"<p>Inverse Diffusivity \u2014 inverse PDE problem definition (CSV data).</p>"},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/ode_csv/#anypinn.cli.scaffold.inverse_diffusivity.ode_csv.GRID_SIZE","title":"<code>GRID_SIZE = 50</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/ode_csv/#anypinn.cli.scaffold.inverse_diffusivity.ode_csv.create_data_module","title":"<code>create_data_module(hp: PINNHyperparameters) -&gt; InverseDiffusivityDataModule</code>","text":"Source code in <code>src/anypinn/cli/scaffold/inverse_diffusivity/ode_csv.py</code> <pre><code>def create_data_module(hp: PINNHyperparameters) -&gt; InverseDiffusivityDataModule:\n    return InverseDiffusivityDataModule(\n        hp=hp,\n        grid_size=GRID_SIZE,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/ode_csv/#anypinn.cli.scaffold.inverse_diffusivity.ode_csv.create_problem","title":"<code>create_problem(hp: PINNHyperparameters) -&gt; Problem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/inverse_diffusivity/ode_csv.py</code> <pre><code>def create_problem(hp: PINNHyperparameters) -&gt; Problem:\n    encode = FourierEncoding(num_frequencies=6)\n    field_u = Field(\n        config=MLPConfig(\n            in_dim=encode.out_dim(2),\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=hp.fields_config.output_activation,\n            encode=encode,\n        )\n    )\n    field_d = Field(\n        config=MLPConfig(\n            in_dim=encode.out_dim(2),\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=\"softplus\",\n            encode=encode,\n        )\n    )\n\n    fields = FieldsRegistry({U_KEY: field_u, D_KEY: field_d})\n    params = ParamsRegistry({})\n\n    bcs = [\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_left_boundary, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_left\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_right_boundary, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_right\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_initial_condition, value=_ic_value, n_pts=100),\n            field_u,\n            log_key=\"loss/ic\",\n            weight=10.0,\n        ),\n    ]\n\n    pde = PDEResidualConstraint(\n        fields=fields,\n        params=params,\n        residual_fn=diffusivity_residual,\n        log_key=\"loss/pde_residual\",\n        weight=1.0,\n    )\n\n    data = DataConstraint(\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n        weight=5.0,\n    )\n\n    return Problem(\n        constraints=[pde, *bcs, data],\n        criterion=build_criterion(hp.criterion),\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/ode_csv/#anypinn.cli.scaffold.inverse_diffusivity.ode_csv.diffusivity_residual","title":"<code>diffusivity_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor</code>","text":"<p>PDE residual: du/dt - (dD/dx * du/dx + D * d2u/dx2) = 0.</p> Source code in <code>src/anypinn/cli/scaffold/inverse_diffusivity/ode_csv.py</code> <pre><code>def diffusivity_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor:\n    \"\"\"PDE residual: du/dt - (dD/dx * du/dx + D * d2u/dx2) = 0.\"\"\"\n    u = fields[U_KEY](x)\n    D = fields[D_KEY](x)\n    du_dt = partial(u, x, dim=1, order=1)\n    du_dx = partial(u, x, dim=0, order=1)\n    d2u_dx2 = partial(u, x, dim=0, order=2)\n    dD_dx = partial(D, x, dim=0, order=1)\n    return du_dt - (dD_dx * du_dx + D * d2u_dx2)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/ode_csv/#anypinn.cli.scaffold.inverse_diffusivity.ode_csv.predict_data","title":"<code>predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/cli/scaffold/inverse_diffusivity/ode_csv.py</code> <pre><code>def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n    return fields[U_KEY](x_data).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/ode_synthetic/","title":"ode_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/ode_synthetic/#anypinn.cli.scaffold.inverse_diffusivity.ode_synthetic","title":"<code>anypinn.cli.scaffold.inverse_diffusivity.ode_synthetic</code>","text":"<p>Inverse Diffusivity \u2014 inverse PDE problem definition (synthetic).</p>"},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/ode_synthetic/#anypinn.cli.scaffold.inverse_diffusivity.ode_synthetic.GRID_SIZE","title":"<code>GRID_SIZE = 50</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/ode_synthetic/#anypinn.cli.scaffold.inverse_diffusivity.ode_synthetic.create_data_module","title":"<code>create_data_module(hp: PINNHyperparameters) -&gt; InverseDiffusivityDataModule</code>","text":"Source code in <code>src/anypinn/cli/scaffold/inverse_diffusivity/ode_synthetic.py</code> <pre><code>def create_data_module(hp: PINNHyperparameters) -&gt; InverseDiffusivityDataModule:\n    return InverseDiffusivityDataModule(\n        hp=hp,\n        grid_size=GRID_SIZE,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/ode_synthetic/#anypinn.cli.scaffold.inverse_diffusivity.ode_synthetic.create_problem","title":"<code>create_problem(hp: PINNHyperparameters) -&gt; Problem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/inverse_diffusivity/ode_synthetic.py</code> <pre><code>def create_problem(hp: PINNHyperparameters) -&gt; Problem:\n    encode = FourierEncoding(num_frequencies=6)\n    field_u = Field(\n        config=MLPConfig(\n            in_dim=encode.out_dim(2),\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=hp.fields_config.output_activation,\n            encode=encode,\n        )\n    )\n    field_d = Field(\n        config=MLPConfig(\n            in_dim=encode.out_dim(2),\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=\"softplus\",\n            encode=encode,\n        )\n    )\n\n    fields = FieldsRegistry({U_KEY: field_u, D_KEY: field_d})\n    params = ParamsRegistry({})\n\n    bcs = [\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_left_boundary, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_left\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_right_boundary, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_right\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_initial_condition, value=_ic_value, n_pts=100),\n            field_u,\n            log_key=\"loss/ic\",\n            weight=10.0,\n        ),\n    ]\n\n    pde = PDEResidualConstraint(\n        fields=fields,\n        params=params,\n        residual_fn=diffusivity_residual,\n        log_key=\"loss/pde_residual\",\n        weight=1.0,\n    )\n\n    data = DataConstraint(\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n        weight=5.0,\n    )\n\n    return Problem(\n        constraints=[pde, *bcs, data],\n        criterion=build_criterion(hp.criterion),\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/ode_synthetic/#anypinn.cli.scaffold.inverse_diffusivity.ode_synthetic.diffusivity_residual","title":"<code>diffusivity_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor</code>","text":"<p>PDE residual: du/dt - (dD/dx * du/dx + D * d2u/dx2) = 0.</p> Source code in <code>src/anypinn/cli/scaffold/inverse_diffusivity/ode_synthetic.py</code> <pre><code>def diffusivity_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor:\n    \"\"\"PDE residual: du/dt - (dD/dx * du/dx + D * d2u/dx2) = 0.\"\"\"\n    u = fields[U_KEY](x)\n    D = fields[D_KEY](x)\n    du_dt = partial(u, x, dim=1, order=1)\n    du_dx = partial(u, x, dim=0, order=1)\n    d2u_dx2 = partial(u, x, dim=0, order=2)\n    dD_dx = partial(D, x, dim=0, order=1)\n    return du_dt - (dD_dx * du_dx + D * d2u_dx2)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/inverse_diffusivity/ode_synthetic/#anypinn.cli.scaffold.inverse_diffusivity.ode_synthetic.predict_data","title":"<code>predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/cli/scaffold/inverse_diffusivity/ode_synthetic.py</code> <pre><code>def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n    return fields[U_KEY](x_data).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/lorenz/","title":"lorenz","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/#anypinn.cli.scaffold.lorenz","title":"<code>anypinn.cli.scaffold.lorenz</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/config_csv/","title":"config_csv","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/config_csv/#anypinn.cli.scaffold.lorenz.config_csv","title":"<code>anypinn.cli.scaffold.lorenz.config_csv</code>","text":"<p>Lorenz system \u2014 training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/lorenz/config_csv/#anypinn.cli.scaffold.lorenz.config_csv.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/config_csv/#anypinn.cli.scaffold.lorenz.config_csv.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/config_csv/#anypinn.cli.scaffold.lorenz.config_csv.hp","title":"<code>hp = ODEHyperparameters(lr=0.001, max_epochs=3000, gradient_clip_val=0.5, criterion='huber', training_data=(IngestionConfig(batch_size=100, data_ratio=2, collocations=8000, collocation_sampler='latin_hypercube', df_path=(Path('./data/data.csv')), y_columns=['x_obs', 'y_obs', 'z_obs'])), fields_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=15.0)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=55, threshold=0.005, min_lr=1e-06)), pde_weight=0.001, ic_weight=10, data_weight=5)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/config_synthetic/","title":"config_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/config_synthetic/#anypinn.cli.scaffold.lorenz.config_synthetic","title":"<code>anypinn.cli.scaffold.lorenz.config_synthetic</code>","text":"<p>Lorenz system \u2014 training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/lorenz/config_synthetic/#anypinn.cli.scaffold.lorenz.config_synthetic.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/config_synthetic/#anypinn.cli.scaffold.lorenz.config_synthetic.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/config_synthetic/#anypinn.cli.scaffold.lorenz.config_synthetic.hp","title":"<code>hp = ODEHyperparameters(lr=0.001, max_epochs=3000, gradient_clip_val=0.5, criterion='huber', training_data=(GenerationConfig(batch_size=100, data_ratio=2, collocations=8000, collocation_sampler='latin_hypercube', x=(torch.linspace(start=0, end=3, steps=300)), args_to_train={}, noise_level=0)), fields_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=15.0)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=55, threshold=0.005, min_lr=1e-06)), pde_weight=0.001, ic_weight=10, data_weight=5)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/","title":"ode_csv","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv","title":"<code>anypinn.cli.scaffold.lorenz.ode_csv</code>","text":"<p>Lorenz system \u2014 mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.BETA_KEY","title":"<code>BETA_KEY = 'beta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.NOISE_STD","title":"<code>NOISE_STD = 0.5</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.RHO_KEY","title":"<code>RHO_KEY = 'rho'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.SCALE","title":"<code>SCALE = 20.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.SIGMA_KEY","title":"<code>SIGMA_KEY = 'sigma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.TRUE_BETA","title":"<code>TRUE_BETA = 8.0 / 3.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.TRUE_RHO","title":"<code>TRUE_RHO = 28.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.TRUE_SIGMA","title":"<code>TRUE_SIGMA = 10.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.T_TOTAL","title":"<code>T_TOTAL = 3</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.X0","title":"<code>X0 = -8.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.X_KEY","title":"<code>X_KEY = 'x'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.Y0","title":"<code>Y0 = 7.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.Y_KEY","title":"<code>Y_KEY = 'y'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.Z0","title":"<code>Z0 = 27.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.Z_KEY","title":"<code>Z_KEY = 'z'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.validation","title":"<code>validation: ValidationRegistry = {}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/lorenz/ode_csv.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    from anypinn.catalog.lorenz import LorenzDataModule\n\n    def lorenz_unscaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n        lx, ly, lz = y\n        sigma = args[SIGMA_KEY]\n        rho = args[RHO_KEY]\n        beta = args[BETA_KEY]\n        dx = sigma(x) * (ly - lx)\n        dy = lx * (rho(x) - lz) - ly\n        dz = lx * ly - beta(x) * lz\n        return torch.stack([dx, dy, dz])\n\n    gen_props = ODEProperties(\n        ode=lorenz_unscaled,\n        y0=torch.tensor([X0, Y0, Z0]),\n        args={\n            SIGMA_KEY: Argument(TRUE_SIGMA),\n            RHO_KEY: Argument(TRUE_RHO),\n            BETA_KEY: Argument(TRUE_BETA),\n        },\n    )\n\n    return LorenzDataModule(\n        hp=hp,\n        gen_props=gen_props,\n        noise_std=NOISE_STD,\n        validation=validation,\n        callbacks=[DataScaling(y_scale=[1 / SCALE, 1 / SCALE, 1 / SCALE])],\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/lorenz/ode_csv.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    props = ODEProperties(\n        ode=lorenz_scaled,\n        y0=torch.tensor([X0, Y0, Z0]) / SCALE,\n        args={},\n    )\n\n    fields = FieldsRegistry(\n        {\n            X_KEY: Field(config=hp.fields_config),\n            Y_KEY: Field(config=hp.fields_config),\n            Z_KEY: Field(config=hp.fields_config),\n        }\n    )\n    params = ParamsRegistry(\n        {\n            SIGMA_KEY: Parameter(config=hp.params_config),\n            RHO_KEY: Parameter(config=hp.params_config),\n            BETA_KEY: Parameter(config=hp.params_config),\n        }\n    )\n\n    def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n        x_pred = fields[X_KEY](x_data)\n        y_pred = fields[Y_KEY](x_data)\n        z_pred = fields[Z_KEY](x_data)\n        return torch.stack([x_pred, y_pred, z_pred], dim=1)\n\n    return ODEInverseProblem(\n        props=props,\n        hp=hp,\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_csv/#anypinn.cli.scaffold.lorenz.ode_csv.lorenz_scaled","title":"<code>lorenz_scaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>Scaled Lorenz ODE. States pre-divided by SCALE, time by T_TOTAL.</p> Source code in <code>src/anypinn/cli/scaffold/lorenz/ode_csv.py</code> <pre><code>def lorenz_scaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"Scaled Lorenz ODE. States pre-divided by SCALE, time by T_TOTAL.\"\"\"\n    lx, ly, lz = y\n    sigma = args[SIGMA_KEY]\n    rho = args[RHO_KEY]\n    beta = args[BETA_KEY]\n\n    dx = sigma(x) * (ly - lx)\n    dy = lx * (rho(x) / SCALE - lz) - ly\n    dz = lx * ly * SCALE - beta(x) * lz\n\n    dx = dx * T_TOTAL\n    dy = dy * T_TOTAL\n    dz = dz * T_TOTAL\n    return torch.stack([dx, dy, dz])\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/","title":"ode_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic","title":"<code>anypinn.cli.scaffold.lorenz.ode_synthetic</code>","text":"<p>Lorenz system \u2014 mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.BETA_KEY","title":"<code>BETA_KEY = 'beta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.NOISE_STD","title":"<code>NOISE_STD = 0.5</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.RHO_KEY","title":"<code>RHO_KEY = 'rho'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.SCALE","title":"<code>SCALE = 20.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.SIGMA_KEY","title":"<code>SIGMA_KEY = 'sigma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.TRUE_BETA","title":"<code>TRUE_BETA = 8.0 / 3.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.TRUE_RHO","title":"<code>TRUE_RHO = 28.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.TRUE_SIGMA","title":"<code>TRUE_SIGMA = 10.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.T_TOTAL","title":"<code>T_TOTAL = 3</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.X0","title":"<code>X0 = -8.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.X_KEY","title":"<code>X_KEY = 'x'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.Y0","title":"<code>Y0 = 7.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.Y_KEY","title":"<code>Y_KEY = 'y'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.Z0","title":"<code>Z0 = 27.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.Z_KEY","title":"<code>Z_KEY = 'z'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.validation","title":"<code>validation: ValidationRegistry = {SIGMA_KEY: lambda x: torch.full_like(x, TRUE_SIGMA), RHO_KEY: lambda x: torch.full_like(x, TRUE_RHO), BETA_KEY: lambda x: torch.full_like(x, TRUE_BETA)}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/lorenz/ode_synthetic.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    from anypinn.catalog.lorenz import LorenzDataModule\n\n    def lorenz_unscaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n        lx, ly, lz = y\n        sigma = args[SIGMA_KEY]\n        rho = args[RHO_KEY]\n        beta = args[BETA_KEY]\n        dx = sigma(x) * (ly - lx)\n        dy = lx * (rho(x) - lz) - ly\n        dz = lx * ly - beta(x) * lz\n        return torch.stack([dx, dy, dz])\n\n    gen_props = ODEProperties(\n        ode=lorenz_unscaled,\n        y0=torch.tensor([X0, Y0, Z0]),\n        args={\n            SIGMA_KEY: Argument(TRUE_SIGMA),\n            RHO_KEY: Argument(TRUE_RHO),\n            BETA_KEY: Argument(TRUE_BETA),\n        },\n    )\n\n    return LorenzDataModule(\n        hp=hp,\n        gen_props=gen_props,\n        noise_std=NOISE_STD,\n        validation=validation,\n        callbacks=[DataScaling(y_scale=[1 / SCALE, 1 / SCALE, 1 / SCALE])],\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/lorenz/ode_synthetic.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    props = ODEProperties(\n        ode=lorenz_scaled,\n        y0=torch.tensor([X0, Y0, Z0]) / SCALE,\n        args={},\n    )\n\n    fields = FieldsRegistry(\n        {\n            X_KEY: Field(config=hp.fields_config),\n            Y_KEY: Field(config=hp.fields_config),\n            Z_KEY: Field(config=hp.fields_config),\n        }\n    )\n    params = ParamsRegistry(\n        {\n            SIGMA_KEY: Parameter(config=hp.params_config),\n            RHO_KEY: Parameter(config=hp.params_config),\n            BETA_KEY: Parameter(config=hp.params_config),\n        }\n    )\n\n    def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n        x_pred = fields[X_KEY](x_data)\n        y_pred = fields[Y_KEY](x_data)\n        z_pred = fields[Z_KEY](x_data)\n        return torch.stack([x_pred, y_pred, z_pred], dim=1)\n\n    return ODEInverseProblem(\n        props=props,\n        hp=hp,\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/lorenz/ode_synthetic/#anypinn.cli.scaffold.lorenz.ode_synthetic.lorenz_scaled","title":"<code>lorenz_scaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>Scaled Lorenz ODE. States pre-divided by SCALE, time by T_TOTAL.</p> Source code in <code>src/anypinn/cli/scaffold/lorenz/ode_synthetic.py</code> <pre><code>def lorenz_scaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"Scaled Lorenz ODE. States pre-divided by SCALE, time by T_TOTAL.\"\"\"\n    lx, ly, lz = y\n    sigma = args[SIGMA_KEY]\n    rho = args[RHO_KEY]\n    beta = args[BETA_KEY]\n\n    dx = sigma(x) * (ly - lx)\n    dy = lx * (rho(x) / SCALE - lz) - ly\n    dz = lx * ly * SCALE - beta(x) * lz\n\n    dx = dx * T_TOTAL\n    dy = dy * T_TOTAL\n    dz = dz * T_TOTAL\n    return torch.stack([dx, dy, dz])\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/","title":"lotka_volterra","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/#anypinn.cli.scaffold.lotka_volterra","title":"<code>anypinn.cli.scaffold.lotka_volterra</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/config_csv/","title":"config_csv","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/config_csv/#anypinn.cli.scaffold.lotka_volterra.config_csv","title":"<code>anypinn.cli.scaffold.lotka_volterra.config_csv</code>","text":"<p>Lotka-Volterra predator-prey \u2014 training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/config_csv/#anypinn.cli.scaffold.lotka_volterra.config_csv.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/config_csv/#anypinn.cli.scaffold.lotka_volterra.config_csv.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/config_csv/#anypinn.cli.scaffold.lotka_volterra.config_csv.hp","title":"<code>hp = ODEHyperparameters(lr=0.001, max_epochs=5000, gradient_clip_val=1.0, training_data=(IngestionConfig(batch_size=100, data_ratio=2, collocations=10000, df_path=(Path('./data/data.csv')), y_columns=['x_obs', 'y_obs'])), fields_config=(MLPConfig(in_dim=7, out_dim=1, hidden_layers=[64, 64, 64, 64, 64, 64], activation='tanh', output_activation=None, encode=fourier_encode)), params_config=(ScalarConfig(init_value=0.05)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=200, threshold=0.0001, min_lr=1e-05)), pde_weight=1.0, ic_weight=10, data_weight=1.0)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/config_synthetic/","title":"config_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/config_synthetic/#anypinn.cli.scaffold.lotka_volterra.config_synthetic","title":"<code>anypinn.cli.scaffold.lotka_volterra.config_synthetic</code>","text":"<p>Lotka-Volterra predator-prey \u2014 training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/config_synthetic/#anypinn.cli.scaffold.lotka_volterra.config_synthetic.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/config_synthetic/#anypinn.cli.scaffold.lotka_volterra.config_synthetic.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/config_synthetic/#anypinn.cli.scaffold.lotka_volterra.config_synthetic.hp","title":"<code>hp = ODEHyperparameters(lr=0.001, max_epochs=5000, gradient_clip_val=1.0, training_data=(GenerationConfig(batch_size=100, data_ratio=2, collocations=10000, x=(torch.linspace(start=0, end=50, steps=200)), args_to_train={}, noise_level=0)), fields_config=(MLPConfig(in_dim=7, out_dim=1, hidden_layers=[64, 64, 64, 64, 64, 64], activation='tanh', output_activation=None, encode=fourier_encode)), params_config=(ScalarConfig(init_value=0.05)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=200, threshold=0.0001, min_lr=1e-05)), pde_weight=1.0, ic_weight=10, data_weight=1.0)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/","title":"ode_csv","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv","title":"<code>anypinn.cli.scaffold.lotka_volterra.ode_csv</code>","text":"<p>Lotka-Volterra predator-prey \u2014 mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.ALPHA_KEY","title":"<code>ALPHA_KEY = 'alpha'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.BETA_KEY","title":"<code>BETA_KEY = 'beta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.DELTA_KEY","title":"<code>DELTA_KEY = 'delta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.GAMMA_KEY","title":"<code>GAMMA_KEY = 'gamma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.NOISE_FRAC","title":"<code>NOISE_FRAC = 0.02</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.POP_SCALE","title":"<code>POP_SCALE = 100.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.TRUE_ALPHA","title":"<code>TRUE_ALPHA = 0.5</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.TRUE_BETA","title":"<code>TRUE_BETA = 0.02</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.TRUE_DELTA","title":"<code>TRUE_DELTA = 0.01</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.TRUE_GAMMA","title":"<code>TRUE_GAMMA = 0.5</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.T_TOTAL","title":"<code>T_TOTAL = 50</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.X0","title":"<code>X0 = 40.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.X_KEY","title":"<code>X_KEY = 'x'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.Y0","title":"<code>Y0 = 9.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.Y_KEY","title":"<code>Y_KEY = 'y'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.validation","title":"<code>validation: ValidationRegistry = {}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/lotka_volterra/ode_csv.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    from anypinn.catalog.lotka_volterra import LotkaVolterraDataModule\n\n    def LV_unscaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n        prey, predator = y\n        b = args[BETA_KEY]\n        alpha = args[ALPHA_KEY]\n        delta = args[DELTA_KEY]\n        gamma = args[GAMMA_KEY]\n        dx = alpha(x) * prey - b(x) * prey * predator\n        dy = delta(x) * prey * predator - gamma(x) * predator\n        return torch.stack([dx, dy])\n\n    gen_props = ODEProperties(\n        ode=LV_unscaled,\n        y0=torch.tensor([X0, Y0]),\n        args={\n            ALPHA_KEY: Argument(TRUE_ALPHA),\n            BETA_KEY: Argument(TRUE_BETA),\n            DELTA_KEY: Argument(TRUE_DELTA),\n            GAMMA_KEY: Argument(TRUE_GAMMA),\n        },\n    )\n\n    return LotkaVolterraDataModule(\n        hp=hp,\n        gen_props=gen_props,\n        noise_frac=NOISE_FRAC,\n        validation=validation,\n        callbacks=[DataScaling(y_scale=[1 / POP_SCALE, 1 / POP_SCALE])],\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/lotka_volterra/ode_csv.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    props = ODEProperties(\n        ode=lotka_volterra,\n        y0=torch.tensor([X0, Y0]) / POP_SCALE,\n        args={\n            ALPHA_KEY: Argument(TRUE_ALPHA),\n            DELTA_KEY: Argument(TRUE_DELTA),\n            GAMMA_KEY: Argument(TRUE_GAMMA),\n        },\n    )\n\n    fields = FieldsRegistry(\n        {\n            X_KEY: Field(config=hp.fields_config),\n            Y_KEY: Field(config=hp.fields_config),\n        }\n    )\n    params = ParamsRegistry(\n        {\n            BETA_KEY: Parameter(config=hp.params_config),\n        }\n    )\n\n    def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n        x_pred = fields[X_KEY](x_data)\n        y_pred = fields[Y_KEY](x_data)\n        return torch.stack([x_pred, y_pred], dim=1)\n\n    return ODEInverseProblem(\n        props=props,\n        hp=hp,\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.fourier_encode","title":"<code>fourier_encode(t: Tensor) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/cli/scaffold/lotka_volterra/ode_csv.py</code> <pre><code>def fourier_encode(t: Tensor) -&gt; Tensor:\n    features = [t]\n    for k in range(1, 7):\n        features.append(torch.sin(k * t))\n    return torch.cat(features, dim=-1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_csv/#anypinn.cli.scaffold.lotka_volterra.ode_csv.lotka_volterra","title":"<code>lotka_volterra(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>Scaled Lotka-Volterra ODE. Populations scaled by POP_SCALE, time by T_TOTAL.</p> Source code in <code>src/anypinn/cli/scaffold/lotka_volterra/ode_csv.py</code> <pre><code>def lotka_volterra(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"Scaled Lotka-Volterra ODE. Populations scaled by POP_SCALE, time by T_TOTAL.\"\"\"\n    prey, predator = y\n    b = args[BETA_KEY]\n    alpha = args[ALPHA_KEY]\n    delta = args[DELTA_KEY]\n    gamma = args[GAMMA_KEY]\n\n    dx = alpha(x) * prey - b(x) * prey * predator * POP_SCALE\n    dy = delta(x) * prey * predator * POP_SCALE - gamma(x) * predator\n\n    dx = dx * T_TOTAL\n    dy = dy * T_TOTAL\n    return torch.stack([dx, dy])\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/","title":"ode_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic","title":"<code>anypinn.cli.scaffold.lotka_volterra.ode_synthetic</code>","text":"<p>Lotka-Volterra predator-prey \u2014 mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.ALPHA_KEY","title":"<code>ALPHA_KEY = 'alpha'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.BETA_KEY","title":"<code>BETA_KEY = 'beta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.DELTA_KEY","title":"<code>DELTA_KEY = 'delta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.GAMMA_KEY","title":"<code>GAMMA_KEY = 'gamma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.NOISE_FRAC","title":"<code>NOISE_FRAC = 0.02</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.POP_SCALE","title":"<code>POP_SCALE = 100.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.TRUE_ALPHA","title":"<code>TRUE_ALPHA = 0.5</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.TRUE_BETA","title":"<code>TRUE_BETA = 0.02</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.TRUE_DELTA","title":"<code>TRUE_DELTA = 0.01</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.TRUE_GAMMA","title":"<code>TRUE_GAMMA = 0.5</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.T_TOTAL","title":"<code>T_TOTAL = 50</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.X0","title":"<code>X0 = 40.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.X_KEY","title":"<code>X_KEY = 'x'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.Y0","title":"<code>Y0 = 9.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.Y_KEY","title":"<code>Y_KEY = 'y'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.validation","title":"<code>validation: ValidationRegistry = {BETA_KEY: lambda x: torch.full_like(x, TRUE_BETA)}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/lotka_volterra/ode_synthetic.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    from anypinn.catalog.lotka_volterra import LotkaVolterraDataModule\n\n    def LV_unscaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n        prey, predator = y\n        b = args[BETA_KEY]\n        alpha = args[ALPHA_KEY]\n        delta = args[DELTA_KEY]\n        gamma = args[GAMMA_KEY]\n        dx = alpha(x) * prey - b(x) * prey * predator\n        dy = delta(x) * prey * predator - gamma(x) * predator\n        return torch.stack([dx, dy])\n\n    gen_props = ODEProperties(\n        ode=LV_unscaled,\n        y0=torch.tensor([X0, Y0]),\n        args={\n            ALPHA_KEY: Argument(TRUE_ALPHA),\n            BETA_KEY: Argument(TRUE_BETA),\n            DELTA_KEY: Argument(TRUE_DELTA),\n            GAMMA_KEY: Argument(TRUE_GAMMA),\n        },\n    )\n\n    return LotkaVolterraDataModule(\n        hp=hp,\n        gen_props=gen_props,\n        noise_frac=NOISE_FRAC,\n        validation=validation,\n        callbacks=[DataScaling(y_scale=[1 / POP_SCALE, 1 / POP_SCALE])],\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/lotka_volterra/ode_synthetic.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    props = ODEProperties(\n        ode=lotka_volterra,\n        y0=torch.tensor([X0, Y0]) / POP_SCALE,\n        args={\n            ALPHA_KEY: Argument(TRUE_ALPHA),\n            DELTA_KEY: Argument(TRUE_DELTA),\n            GAMMA_KEY: Argument(TRUE_GAMMA),\n        },\n    )\n\n    fields = FieldsRegistry(\n        {\n            X_KEY: Field(config=hp.fields_config),\n            Y_KEY: Field(config=hp.fields_config),\n        }\n    )\n    params = ParamsRegistry(\n        {\n            BETA_KEY: Parameter(config=hp.params_config),\n        }\n    )\n\n    def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n        x_pred = fields[X_KEY](x_data)\n        y_pred = fields[Y_KEY](x_data)\n        return torch.stack([x_pred, y_pred], dim=1)\n\n    return ODEInverseProblem(\n        props=props,\n        hp=hp,\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.fourier_encode","title":"<code>fourier_encode(t: Tensor) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/cli/scaffold/lotka_volterra/ode_synthetic.py</code> <pre><code>def fourier_encode(t: Tensor) -&gt; Tensor:\n    features = [t]\n    for k in range(1, 7):\n        features.append(torch.sin(k * t))\n    return torch.cat(features, dim=-1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/lotka_volterra/ode_synthetic/#anypinn.cli.scaffold.lotka_volterra.ode_synthetic.lotka_volterra","title":"<code>lotka_volterra(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>Scaled Lotka-Volterra ODE. Populations scaled by POP_SCALE, time by T_TOTAL.</p> Source code in <code>src/anypinn/cli/scaffold/lotka_volterra/ode_synthetic.py</code> <pre><code>def lotka_volterra(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"Scaled Lotka-Volterra ODE. Populations scaled by POP_SCALE, time by T_TOTAL.\"\"\"\n    prey, predator = y\n    b = args[BETA_KEY]\n    alpha = args[ALPHA_KEY]\n    delta = args[DELTA_KEY]\n    gamma = args[GAMMA_KEY]\n\n    dx = alpha(x) * prey - b(x) * prey * predator * POP_SCALE\n    dy = delta(x) * prey * predator * POP_SCALE - gamma(x) * predator\n\n    dx = dx * T_TOTAL\n    dy = dy * T_TOTAL\n    return torch.stack([dx, dy])\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/poisson_2d/","title":"poisson_2d","text":""},{"location":"reference/anypinn/cli/scaffold/poisson_2d/#anypinn.cli.scaffold.poisson_2d","title":"<code>anypinn.cli.scaffold.poisson_2d</code>","text":""},{"location":"reference/anypinn/cli/scaffold/poisson_2d/config_csv/","title":"config_csv","text":""},{"location":"reference/anypinn/cli/scaffold/poisson_2d/config_csv/#anypinn.cli.scaffold.poisson_2d.config_csv","title":"<code>anypinn.cli.scaffold.poisson_2d.config_csv</code>","text":"<p>Poisson 2D \u2014 training configuration (CSV).</p>"},{"location":"reference/anypinn/cli/scaffold/poisson_2d/config_csv/#anypinn.cli.scaffold.poisson_2d.config_csv.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/poisson_2d/config_csv/#anypinn.cli.scaffold.poisson_2d.config_csv.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/poisson_2d/config_csv/#anypinn.cli.scaffold.poisson_2d.config_csv.hp","title":"<code>hp = PINNHyperparameters(lr=0.001, max_epochs=2000, gradient_clip_val=0.5, training_data=(IngestionConfig(batch_size=128, data_ratio=2, collocations=8000, collocation_sampler='uniform', df_path=(Path('./data/data.csv')), y_columns=['u'])), fields_config=(MLPConfig(in_dim=26, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.0)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=100, threshold=0.0001, min_lr=1e-06)))</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/poisson_2d/config_synthetic/","title":"config_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/poisson_2d/config_synthetic/#anypinn.cli.scaffold.poisson_2d.config_synthetic","title":"<code>anypinn.cli.scaffold.poisson_2d.config_synthetic</code>","text":"<p>Poisson 2D \u2014 training configuration (synthetic).</p>"},{"location":"reference/anypinn/cli/scaffold/poisson_2d/config_synthetic/#anypinn.cli.scaffold.poisson_2d.config_synthetic.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/poisson_2d/config_synthetic/#anypinn.cli.scaffold.poisson_2d.config_synthetic.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/poisson_2d/config_synthetic/#anypinn.cli.scaffold.poisson_2d.config_synthetic.hp","title":"<code>hp = PINNHyperparameters(lr=0.001, max_epochs=2000, gradient_clip_val=0.5, training_data=(GenerationConfig(batch_size=128, data_ratio=2, collocations=8000, collocation_sampler='uniform', x=(torch.linspace(0, 1, steps=30)), args_to_train={}, noise_level=0)), fields_config=(MLPConfig(in_dim=26, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.0)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=100, threshold=0.0001, min_lr=1e-06)))</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/poisson_2d/ode_csv/","title":"ode_csv","text":""},{"location":"reference/anypinn/cli/scaffold/poisson_2d/ode_csv/#anypinn.cli.scaffold.poisson_2d.ode_csv","title":"<code>anypinn.cli.scaffold.poisson_2d.ode_csv</code>","text":"<p>Poisson 2D \u2014 PDE problem definition (CSV data).</p>"},{"location":"reference/anypinn/cli/scaffold/poisson_2d/ode_csv/#anypinn.cli.scaffold.poisson_2d.ode_csv.GRID_SIZE","title":"<code>GRID_SIZE = 30</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/poisson_2d/ode_csv/#anypinn.cli.scaffold.poisson_2d.ode_csv.create_data_module","title":"<code>create_data_module(hp: PINNHyperparameters) -&gt; Poisson2DDataModule</code>","text":"Source code in <code>src/anypinn/cli/scaffold/poisson_2d/ode_csv.py</code> <pre><code>def create_data_module(hp: PINNHyperparameters) -&gt; Poisson2DDataModule:\n    return Poisson2DDataModule(hp=hp, grid_size=GRID_SIZE)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/poisson_2d/ode_csv/#anypinn.cli.scaffold.poisson_2d.ode_csv.create_problem","title":"<code>create_problem(hp: PINNHyperparameters) -&gt; Problem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/poisson_2d/ode_csv.py</code> <pre><code>def create_problem(hp: PINNHyperparameters) -&gt; Problem:\n    encode = FourierEncoding(num_frequencies=6)\n    field_u = Field(\n        config=MLPConfig(\n            in_dim=encode.out_dim(2),\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=hp.fields_config.output_activation,\n            encode=encode,\n        )\n    )\n    fields = FieldsRegistry({U_KEY: field_u})\n    params = ParamsRegistry({})\n\n    bcs = [\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_left_edge, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_left\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_right_edge, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_right\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_bottom_edge, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_bottom\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_top_edge, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_top\",\n            weight=10.0,\n        ),\n    ]\n\n    pde = PDEResidualConstraint(\n        fields=fields,\n        params=params,\n        residual_fn=poisson_residual,\n        log_key=\"loss/pde_residual\",\n        weight=1.0,\n    )\n\n    return Problem(\n        constraints=[pde, *bcs],\n        criterion=build_criterion(hp.criterion),\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/poisson_2d/ode_csv/#anypinn.cli.scaffold.poisson_2d.ode_csv.poisson_residual","title":"<code>poisson_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor</code>","text":"<p>PDE residual: laplacian(u) - f = 0.</p> Source code in <code>src/anypinn/cli/scaffold/poisson_2d/ode_csv.py</code> <pre><code>def poisson_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor:\n    \"\"\"PDE residual: laplacian(u) - f = 0.\"\"\"\n    u = fields[U_KEY](x)\n    return laplacian(u, x) - source_fn(x)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/poisson_2d/ode_csv/#anypinn.cli.scaffold.poisson_2d.ode_csv.source_fn","title":"<code>source_fn(x: Tensor) -&gt; Tensor</code>","text":"<p>Source term f(x,y). TODO: define your source term.</p> Source code in <code>src/anypinn/cli/scaffold/poisson_2d/ode_csv.py</code> <pre><code>def source_fn(x: Tensor) -&gt; Tensor:\n    \"\"\"Source term f(x,y). TODO: define your source term.\"\"\"\n    # Example: f(x,y) = -2 pi^2 sin(pi*x) sin(pi*y)\n    return -2 * math.pi**2 * torch.sin(math.pi * x[:, 0:1]) * torch.sin(math.pi * x[:, 1:2])\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/poisson_2d/ode_synthetic/","title":"ode_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/poisson_2d/ode_synthetic/#anypinn.cli.scaffold.poisson_2d.ode_synthetic","title":"<code>anypinn.cli.scaffold.poisson_2d.ode_synthetic</code>","text":"<p>Poisson 2D \u2014 PDE problem definition (synthetic).</p>"},{"location":"reference/anypinn/cli/scaffold/poisson_2d/ode_synthetic/#anypinn.cli.scaffold.poisson_2d.ode_synthetic.GRID_SIZE","title":"<code>GRID_SIZE = 30</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/poisson_2d/ode_synthetic/#anypinn.cli.scaffold.poisson_2d.ode_synthetic.create_data_module","title":"<code>create_data_module(hp: PINNHyperparameters) -&gt; Poisson2DDataModule</code>","text":"Source code in <code>src/anypinn/cli/scaffold/poisson_2d/ode_synthetic.py</code> <pre><code>def create_data_module(hp: PINNHyperparameters) -&gt; Poisson2DDataModule:\n    return Poisson2DDataModule(hp=hp, grid_size=GRID_SIZE)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/poisson_2d/ode_synthetic/#anypinn.cli.scaffold.poisson_2d.ode_synthetic.create_problem","title":"<code>create_problem(hp: PINNHyperparameters) -&gt; Problem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/poisson_2d/ode_synthetic.py</code> <pre><code>def create_problem(hp: PINNHyperparameters) -&gt; Problem:\n    encode = FourierEncoding(num_frequencies=6)\n    field_u = Field(\n        config=MLPConfig(\n            in_dim=encode.out_dim(2),\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=hp.fields_config.output_activation,\n            encode=encode,\n        )\n    )\n    fields = FieldsRegistry({U_KEY: field_u})\n    params = ParamsRegistry({})\n\n    bcs = [\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_left_edge, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_left\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_right_edge, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_right\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_bottom_edge, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_bottom\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_top_edge, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_top\",\n            weight=10.0,\n        ),\n    ]\n\n    pde = PDEResidualConstraint(\n        fields=fields,\n        params=params,\n        residual_fn=poisson_residual,\n        log_key=\"loss/pde_residual\",\n        weight=1.0,\n    )\n\n    return Problem(\n        constraints=[pde, *bcs],\n        criterion=build_criterion(hp.criterion),\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/poisson_2d/ode_synthetic/#anypinn.cli.scaffold.poisson_2d.ode_synthetic.poisson_residual","title":"<code>poisson_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor</code>","text":"<p>PDE residual: laplacian(u) - f = 0.</p> Source code in <code>src/anypinn/cli/scaffold/poisson_2d/ode_synthetic.py</code> <pre><code>def poisson_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor:\n    \"\"\"PDE residual: laplacian(u) - f = 0.\"\"\"\n    u = fields[U_KEY](x)\n    return laplacian(u, x) - source_fn(x)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/poisson_2d/ode_synthetic/#anypinn.cli.scaffold.poisson_2d.ode_synthetic.source_fn","title":"<code>source_fn(x: Tensor) -&gt; Tensor</code>","text":"<p>Source term f(x,y) = -2 pi^2 sin(pix) sin(piy).</p> Source code in <code>src/anypinn/cli/scaffold/poisson_2d/ode_synthetic.py</code> <pre><code>def source_fn(x: Tensor) -&gt; Tensor:\n    \"\"\"Source term f(x,y) = -2 pi^2 sin(pi*x) sin(pi*y).\"\"\"\n    return -2 * math.pi**2 * torch.sin(math.pi * x[:, 0:1]) * torch.sin(math.pi * x[:, 1:2])\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/seir/","title":"seir","text":""},{"location":"reference/anypinn/cli/scaffold/seir/#anypinn.cli.scaffold.seir","title":"<code>anypinn.cli.scaffold.seir</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/config_csv/","title":"config_csv","text":""},{"location":"reference/anypinn/cli/scaffold/seir/config_csv/#anypinn.cli.scaffold.seir.config_csv","title":"<code>anypinn.cli.scaffold.seir.config_csv</code>","text":"<p>SEIR epidemic model \u2014 training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/seir/config_csv/#anypinn.cli.scaffold.seir.config_csv.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/config_csv/#anypinn.cli.scaffold.seir.config_csv.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/config_csv/#anypinn.cli.scaffold.seir.config_csv.hp","title":"<code>hp = ODEHyperparameters(lr=0.0005, max_epochs=2000, gradient_clip_val=0.1, training_data=(IngestionConfig(batch_size=100, data_ratio=2, collocations=6000, df_path=(Path('./data/data.csv')), y_columns=['I_obs'])), fields_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation='softplus')), params_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation='softplus')), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=55, threshold=0.005, min_lr=1e-06)), pde_weight=1, ic_weight=1, data_weight=1)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/config_synthetic/","title":"config_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/seir/config_synthetic/#anypinn.cli.scaffold.seir.config_synthetic","title":"<code>anypinn.cli.scaffold.seir.config_synthetic</code>","text":"<p>SEIR epidemic model \u2014 training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/seir/config_synthetic/#anypinn.cli.scaffold.seir.config_synthetic.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/config_synthetic/#anypinn.cli.scaffold.seir.config_synthetic.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/config_synthetic/#anypinn.cli.scaffold.seir.config_synthetic.hp","title":"<code>hp = ODEHyperparameters(lr=0.0005, max_epochs=2000, gradient_clip_val=0.1, training_data=(GenerationConfig(batch_size=100, data_ratio=2, collocations=6000, x=(torch.linspace(start=0, end=160, steps=161)), args_to_train={}, noise_level=0)), fields_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation='softplus')), params_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation='softplus')), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=55, threshold=0.005, min_lr=1e-06)), pde_weight=1, ic_weight=1, data_weight=1)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/","title":"ode_csv","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv","title":"<code>anypinn.cli.scaffold.seir.ode_csv</code>","text":"<p>SEIR epidemic model \u2014 mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.BETA_KEY","title":"<code>BETA_KEY = 'beta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.E0","title":"<code>E0 = 0.01</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.E_KEY","title":"<code>E_KEY = 'E'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.GAMMA_KEY","title":"<code>GAMMA_KEY = 'gamma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.I0","title":"<code>I0 = 0.001</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.I_KEY","title":"<code>I_KEY = 'I'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.NOISE_STD","title":"<code>NOISE_STD = 0.0005</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.S0","title":"<code>S0 = 0.99</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.SIGMA_KEY","title":"<code>SIGMA_KEY = 'sigma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.S_KEY","title":"<code>S_KEY = 'S'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.TRUE_BETA","title":"<code>TRUE_BETA = 0.5</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.TRUE_GAMMA","title":"<code>TRUE_GAMMA = 1 / 10</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.TRUE_SIGMA","title":"<code>TRUE_SIGMA = 1 / 5.2</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.T_DAYS","title":"<code>T_DAYS = 160</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.validation","title":"<code>validation: ValidationRegistry = {}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.SEIR","title":"<code>SEIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>Scaled SEIR ODE system.</p> Source code in <code>src/anypinn/cli/scaffold/seir/ode_csv.py</code> <pre><code>def SEIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"Scaled SEIR ODE system.\"\"\"\n    S, E, I = y\n    b = args[BETA_KEY]\n    sigma = args[SIGMA_KEY]\n    gamma = args[GAMMA_KEY]\n\n    dS = -b(x) * S * I\n    dE = b(x) * S * I - sigma(x) * E\n    dI = sigma(x) * E - gamma(x) * I\n\n    dS = dS * T_DAYS\n    dE = dE * T_DAYS\n    dI = dI * T_DAYS\n    return torch.stack([dS, dE, dI])\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/seir/ode_csv.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    from anypinn.catalog.seir import SEIRDataModule\n\n    # Unscaled SEIR for data generation\n    def SEIR_unscaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n        S, E, I = y\n        b = args[BETA_KEY]\n        sigma = args[SIGMA_KEY]\n        gamma = args[GAMMA_KEY]\n        dS = -b(x) * S * I\n        dE = b(x) * S * I - sigma(x) * E\n        dI = sigma(x) * E - gamma(x) * I\n        return torch.stack([dS, dE, dI])\n\n    gen_props = ODEProperties(\n        ode=SEIR_unscaled,\n        y0=torch.tensor([S0, E0, I0]),\n        args={\n            BETA_KEY: Argument(TRUE_BETA),\n            SIGMA_KEY: Argument(TRUE_SIGMA),\n            GAMMA_KEY: Argument(TRUE_GAMMA),\n        },\n    )\n\n    return SEIRDataModule(\n        hp=hp,\n        gen_props=gen_props,\n        noise_std=NOISE_STD,\n        validation=validation,\n        callbacks=[DataScaling(y_scale=1.0)],\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/seir/ode_csv/#anypinn.cli.scaffold.seir.ode_csv.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/seir/ode_csv.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    props = ODEProperties(\n        ode=SEIR,\n        y0=torch.tensor([S0, E0, I0]),\n        args={\n            SIGMA_KEY: Argument(TRUE_SIGMA),\n            GAMMA_KEY: Argument(TRUE_GAMMA),\n        },\n    )\n\n    fields = FieldsRegistry(\n        {\n            S_KEY: Field(config=hp.fields_config),\n            E_KEY: Field(config=hp.fields_config),\n            I_KEY: Field(config=hp.fields_config),\n        }\n    )\n    params = ParamsRegistry(\n        {\n            BETA_KEY: Parameter(config=hp.params_config),\n        }\n    )\n\n    def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n        I_pred = fields[I_KEY](x_data)\n        return I_pred.unsqueeze(1)\n\n    return ODEInverseProblem(\n        props=props,\n        hp=hp,\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/","title":"ode_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic","title":"<code>anypinn.cli.scaffold.seir.ode_synthetic</code>","text":"<p>SEIR epidemic model \u2014 mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.BETA_KEY","title":"<code>BETA_KEY = 'beta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.E0","title":"<code>E0 = 0.01</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.E_KEY","title":"<code>E_KEY = 'E'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.GAMMA_KEY","title":"<code>GAMMA_KEY = 'gamma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.I0","title":"<code>I0 = 0.001</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.I_KEY","title":"<code>I_KEY = 'I'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.NOISE_STD","title":"<code>NOISE_STD = 0.0005</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.S0","title":"<code>S0 = 0.99</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.SIGMA_KEY","title":"<code>SIGMA_KEY = 'sigma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.S_KEY","title":"<code>S_KEY = 'S'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.TRUE_BETA","title":"<code>TRUE_BETA = 0.5</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.TRUE_GAMMA","title":"<code>TRUE_GAMMA = 1 / 10</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.TRUE_SIGMA","title":"<code>TRUE_SIGMA = 1 / 5.2</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.T_DAYS","title":"<code>T_DAYS = 160</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.validation","title":"<code>validation: ValidationRegistry = {BETA_KEY: lambda x: torch.full_like(x, TRUE_BETA)}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.SEIR","title":"<code>SEIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>Scaled SEIR ODE system.</p> Source code in <code>src/anypinn/cli/scaffold/seir/ode_synthetic.py</code> <pre><code>def SEIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"Scaled SEIR ODE system.\"\"\"\n    S, E, I = y\n    b = args[BETA_KEY]\n    sigma = args[SIGMA_KEY]\n    gamma = args[GAMMA_KEY]\n\n    dS = -b(x) * S * I\n    dE = b(x) * S * I - sigma(x) * E\n    dI = sigma(x) * E - gamma(x) * I\n\n    dS = dS * T_DAYS\n    dE = dE * T_DAYS\n    dI = dI * T_DAYS\n    return torch.stack([dS, dE, dI])\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/seir/ode_synthetic.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    from anypinn.catalog.seir import SEIRDataModule\n\n    # Unscaled SEIR for data generation\n    def SEIR_unscaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n        S, E, I = y\n        b = args[BETA_KEY]\n        sigma = args[SIGMA_KEY]\n        gamma = args[GAMMA_KEY]\n        dS = -b(x) * S * I\n        dE = b(x) * S * I - sigma(x) * E\n        dI = sigma(x) * E - gamma(x) * I\n        return torch.stack([dS, dE, dI])\n\n    gen_props = ODEProperties(\n        ode=SEIR_unscaled,\n        y0=torch.tensor([S0, E0, I0]),\n        args={\n            BETA_KEY: Argument(TRUE_BETA),\n            SIGMA_KEY: Argument(TRUE_SIGMA),\n            GAMMA_KEY: Argument(TRUE_GAMMA),\n        },\n    )\n\n    return SEIRDataModule(\n        hp=hp,\n        gen_props=gen_props,\n        noise_std=NOISE_STD,\n        validation=validation,\n        callbacks=[DataScaling(y_scale=1.0)],\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/seir/ode_synthetic/#anypinn.cli.scaffold.seir.ode_synthetic.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/seir/ode_synthetic.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    props = ODEProperties(\n        ode=SEIR,\n        y0=torch.tensor([S0, E0, I0]),\n        args={\n            SIGMA_KEY: Argument(TRUE_SIGMA),\n            GAMMA_KEY: Argument(TRUE_GAMMA),\n        },\n    )\n\n    fields = FieldsRegistry(\n        {\n            S_KEY: Field(config=hp.fields_config),\n            E_KEY: Field(config=hp.fields_config),\n            I_KEY: Field(config=hp.fields_config),\n        }\n    )\n    params = ParamsRegistry(\n        {\n            BETA_KEY: Parameter(config=hp.params_config),\n        }\n    )\n\n    def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n        I_pred = fields[I_KEY](x_data)\n        return I_pred.unsqueeze(1)\n\n    return ODEInverseProblem(\n        props=props,\n        hp=hp,\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/sir/","title":"sir","text":""},{"location":"reference/anypinn/cli/scaffold/sir/#anypinn.cli.scaffold.sir","title":"<code>anypinn.cli.scaffold.sir</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/config_csv/","title":"config_csv","text":""},{"location":"reference/anypinn/cli/scaffold/sir/config_csv/#anypinn.cli.scaffold.sir.config_csv","title":"<code>anypinn.cli.scaffold.sir.config_csv</code>","text":"<p>SIR epidemic model \u2014 training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/sir/config_csv/#anypinn.cli.scaffold.sir.config_csv.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/config_csv/#anypinn.cli.scaffold.sir.config_csv.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/config_csv/#anypinn.cli.scaffold.sir.config_csv.hp","title":"<code>hp = ODEHyperparameters(lr=0.0005, max_epochs=2000, gradient_clip_val=0.1, training_data=(IngestionConfig(batch_size=100, data_ratio=2, collocations=6000, df_path=(Path('./data/data.csv')), y_columns=['I_obs'])), fields_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation='softplus')), params_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation='softplus')), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=55, threshold=0.005, min_lr=1e-06)), pde_weight=1, ic_weight=1, data_weight=1)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/config_synthetic/","title":"config_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/sir/config_synthetic/#anypinn.cli.scaffold.sir.config_synthetic","title":"<code>anypinn.cli.scaffold.sir.config_synthetic</code>","text":"<p>SIR epidemic model \u2014 training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/sir/config_synthetic/#anypinn.cli.scaffold.sir.config_synthetic.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/config_synthetic/#anypinn.cli.scaffold.sir.config_synthetic.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/config_synthetic/#anypinn.cli.scaffold.sir.config_synthetic.hp","title":"<code>hp = ODEHyperparameters(lr=0.0005, max_epochs=2000, gradient_clip_val=0.1, training_data=(GenerationConfig(batch_size=100, data_ratio=2, collocations=6000, x=(torch.linspace(start=0, end=90, steps=91)), args_to_train={}, noise_level=0)), fields_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation='softplus')), params_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation='softplus')), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=55, threshold=0.005, min_lr=1e-06)), pde_weight=1, ic_weight=1, data_weight=1)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_csv/","title":"ode_csv","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_csv/#anypinn.cli.scaffold.sir.ode_csv","title":"<code>anypinn.cli.scaffold.sir.ode_csv</code>","text":"<p>SIR epidemic model \u2014 mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/sir/ode_csv/#anypinn.cli.scaffold.sir.ode_csv.BETA_KEY","title":"<code>BETA_KEY = 'beta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_csv/#anypinn.cli.scaffold.sir.ode_csv.C","title":"<code>C = 1000000.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_csv/#anypinn.cli.scaffold.sir.ode_csv.DELTA","title":"<code>DELTA = 1 / 5</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_csv/#anypinn.cli.scaffold.sir.ode_csv.DELTA_KEY","title":"<code>DELTA_KEY = 'delta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_csv/#anypinn.cli.scaffold.sir.ode_csv.I_KEY","title":"<code>I_KEY = 'I'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_csv/#anypinn.cli.scaffold.sir.ode_csv.N_KEY","title":"<code>N_KEY = 'N'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_csv/#anypinn.cli.scaffold.sir.ode_csv.N_POP","title":"<code>N_POP = 56000000.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_csv/#anypinn.cli.scaffold.sir.ode_csv.S_KEY","title":"<code>S_KEY = 'S'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_csv/#anypinn.cli.scaffold.sir.ode_csv.T","title":"<code>T = 90</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_csv/#anypinn.cli.scaffold.sir.ode_csv.TRUE_BETA","title":"<code>TRUE_BETA = 0.6</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_csv/#anypinn.cli.scaffold.sir.ode_csv.validation","title":"<code>validation: ValidationRegistry = {BETA_KEY: ColumnRef(column='Rt', transform=(lambda rt: rt * DELTA))}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_csv/#anypinn.cli.scaffold.sir.ode_csv.SIR","title":"<code>SIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>Scaled SIR ODE system.</p> Source code in <code>src/anypinn/cli/scaffold/sir/ode_csv.py</code> <pre><code>def SIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"Scaled SIR ODE system.\"\"\"\n    S, I = y\n    b, d, N = args[BETA_KEY], args[DELTA_KEY], args[N_KEY]\n\n    dS = -b(x) * I * S * C / N(x)\n    dI = b(x) * I * S * C / N(x) - d(x) * I\n\n    dS = dS * T\n    dI = dI * T\n    return torch.stack([dS, dI])\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/sir/ode_csv/#anypinn.cli.scaffold.sir.ode_csv.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/sir/ode_csv.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    from anypinn.catalog.sir import SIRInvDataModule\n\n    return SIRInvDataModule(\n        hp=hp,\n        validation=validation,\n        callbacks=[DataScaling(y_scale=1 / C)],\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/sir/ode_csv/#anypinn.cli.scaffold.sir.ode_csv.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/sir/ode_csv.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    props = ODEProperties(\n        ode=SIR,\n        y0=torch.tensor([N_POP - 1, 1]) / C,\n        args={\n            DELTA_KEY: Argument(DELTA),\n            N_KEY: Argument(N_POP),\n        },\n    )\n\n    fields = FieldsRegistry(\n        {\n            S_KEY: Field(config=hp.fields_config),\n            I_KEY: Field(config=hp.fields_config),\n        }\n    )\n    params = ParamsRegistry(\n        {\n            BETA_KEY: Parameter(config=hp.params_config),\n        }\n    )\n\n    def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n        I_pred = fields[I_KEY](x_data)\n        return I_pred.unsqueeze(1)\n\n    return ODEInverseProblem(\n        props=props,\n        hp=hp,\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/sir/ode_synthetic/","title":"ode_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_synthetic/#anypinn.cli.scaffold.sir.ode_synthetic","title":"<code>anypinn.cli.scaffold.sir.ode_synthetic</code>","text":"<p>SIR epidemic model \u2014 mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/sir/ode_synthetic/#anypinn.cli.scaffold.sir.ode_synthetic.BETA_KEY","title":"<code>BETA_KEY = 'beta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_synthetic/#anypinn.cli.scaffold.sir.ode_synthetic.C","title":"<code>C = 1000000.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_synthetic/#anypinn.cli.scaffold.sir.ode_synthetic.DELTA","title":"<code>DELTA = 1 / 5</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_synthetic/#anypinn.cli.scaffold.sir.ode_synthetic.DELTA_KEY","title":"<code>DELTA_KEY = 'delta'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_synthetic/#anypinn.cli.scaffold.sir.ode_synthetic.I_KEY","title":"<code>I_KEY = 'I'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_synthetic/#anypinn.cli.scaffold.sir.ode_synthetic.N_KEY","title":"<code>N_KEY = 'N'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_synthetic/#anypinn.cli.scaffold.sir.ode_synthetic.N_POP","title":"<code>N_POP = 56000000.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_synthetic/#anypinn.cli.scaffold.sir.ode_synthetic.S_KEY","title":"<code>S_KEY = 'S'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_synthetic/#anypinn.cli.scaffold.sir.ode_synthetic.T","title":"<code>T = 90</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_synthetic/#anypinn.cli.scaffold.sir.ode_synthetic.TRUE_BETA","title":"<code>TRUE_BETA = 0.6</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_synthetic/#anypinn.cli.scaffold.sir.ode_synthetic.validation","title":"<code>validation: ValidationRegistry = {BETA_KEY: lambda x: torch.full_like(x, TRUE_BETA)}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/sir/ode_synthetic/#anypinn.cli.scaffold.sir.ode_synthetic.SIR","title":"<code>SIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"<p>Scaled SIR ODE system.</p> Source code in <code>src/anypinn/cli/scaffold/sir/ode_synthetic.py</code> <pre><code>def SIR(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n    \"\"\"Scaled SIR ODE system.\"\"\"\n    S, I = y\n    b, d, N = args[BETA_KEY], args[DELTA_KEY], args[N_KEY]\n\n    dS = -b(x) * I * S * C / N(x)\n    dI = b(x) * I * S * C / N(x) - d(x) * I\n\n    dS = dS * T\n    dI = dI * T\n    return torch.stack([dS, dI])\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/sir/ode_synthetic/#anypinn.cli.scaffold.sir.ode_synthetic.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/sir/ode_synthetic.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    from anypinn.catalog.sir import SIRInvDataModule\n\n    return SIRInvDataModule(\n        hp=hp,\n        validation=validation,\n        callbacks=[DataScaling(y_scale=1 / C)],\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/sir/ode_synthetic/#anypinn.cli.scaffold.sir.ode_synthetic.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/sir/ode_synthetic.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    props = ODEProperties(\n        ode=SIR,\n        y0=torch.tensor([N_POP - 1, 1]) / C,\n        args={\n            DELTA_KEY: Argument(DELTA),\n            N_KEY: Argument(N_POP),\n        },\n    )\n\n    fields = FieldsRegistry(\n        {\n            S_KEY: Field(config=hp.fields_config),\n            I_KEY: Field(config=hp.fields_config),\n        }\n    )\n    params = ParamsRegistry(\n        {\n            BETA_KEY: Parameter(config=hp.params_config),\n        }\n    )\n\n    def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n        I_pred = fields[I_KEY](x_data)\n        return I_pred.unsqueeze(1)\n\n    return ODEInverseProblem(\n        props=props,\n        hp=hp,\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/van_der_pol/","title":"van_der_pol","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/#anypinn.cli.scaffold.van_der_pol","title":"<code>anypinn.cli.scaffold.van_der_pol</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/config_csv/","title":"config_csv","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/config_csv/#anypinn.cli.scaffold.van_der_pol.config_csv","title":"<code>anypinn.cli.scaffold.van_der_pol.config_csv</code>","text":"<p>Van der Pol oscillator \u2014 training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/van_der_pol/config_csv/#anypinn.cli.scaffold.van_der_pol.config_csv.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/config_csv/#anypinn.cli.scaffold.van_der_pol.config_csv.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/config_csv/#anypinn.cli.scaffold.van_der_pol.config_csv.hp","title":"<code>hp = ODEHyperparameters(lr=0.001, max_epochs=3000, gradient_clip_val=0.1, training_data=(IngestionConfig(batch_size=100, data_ratio=2, collocations=8000, df_path=(Path('./data/data.csv')), y_columns=['u_obs'])), fields_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=2.0)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=55, threshold=0.005, min_lr=1e-06)), pde_weight=0.0001, ic_weight=15, data_weight=5)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/config_synthetic/","title":"config_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/config_synthetic/#anypinn.cli.scaffold.van_der_pol.config_synthetic","title":"<code>anypinn.cli.scaffold.van_der_pol.config_synthetic</code>","text":"<p>Van der Pol oscillator \u2014 training configuration.</p>"},{"location":"reference/anypinn/cli/scaffold/van_der_pol/config_synthetic/#anypinn.cli.scaffold.van_der_pol.config_synthetic.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/config_synthetic/#anypinn.cli.scaffold.van_der_pol.config_synthetic.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/config_synthetic/#anypinn.cli.scaffold.van_der_pol.config_synthetic.hp","title":"<code>hp = ODEHyperparameters(lr=0.001, max_epochs=3000, gradient_clip_val=0.1, training_data=(GenerationConfig(batch_size=100, data_ratio=2, collocations=8000, x=(torch.linspace(start=0, end=20, steps=400)), args_to_train={}, noise_level=0)), fields_config=(MLPConfig(in_dim=1, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=2.0)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=55, threshold=0.005, min_lr=1e-06)), pde_weight=0.0001, ic_weight=15, data_weight=5)</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_csv/","title":"ode_csv","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_csv/#anypinn.cli.scaffold.van_der_pol.ode_csv","title":"<code>anypinn.cli.scaffold.van_der_pol.ode_csv</code>","text":"<p>Van der Pol oscillator \u2014 mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_csv/#anypinn.cli.scaffold.van_der_pol.ode_csv.DU0","title":"<code>DU0 = 0.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_csv/#anypinn.cli.scaffold.van_der_pol.ode_csv.MU_KEY","title":"<code>MU_KEY = 'mu'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_csv/#anypinn.cli.scaffold.van_der_pol.ode_csv.NOISE_STD","title":"<code>NOISE_STD = 0.05</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_csv/#anypinn.cli.scaffold.van_der_pol.ode_csv.TRUE_MU","title":"<code>TRUE_MU = 1.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_csv/#anypinn.cli.scaffold.van_der_pol.ode_csv.T_TOTAL","title":"<code>T_TOTAL = 20</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_csv/#anypinn.cli.scaffold.van_der_pol.ode_csv.U0","title":"<code>U0 = 2.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_csv/#anypinn.cli.scaffold.van_der_pol.ode_csv.U_KEY","title":"<code>U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_csv/#anypinn.cli.scaffold.van_der_pol.ode_csv.validation","title":"<code>validation: ValidationRegistry = {}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_csv/#anypinn.cli.scaffold.van_der_pol.ode_csv.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/van_der_pol/ode_csv.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    from anypinn.catalog.van_der_pol import VanDerPolDataModule\n\n    def vdp_unscaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n        u, v = y\n        mu = args[MU_KEY]\n        du = v\n        dv = mu(x) * (1 - u**2) * v - u\n        return torch.stack([du, dv])\n\n    gen_props = ODEProperties(\n        ode=vdp_unscaled,\n        y0=torch.tensor([U0, DU0]),\n        args={\n            MU_KEY: Argument(TRUE_MU),\n        },\n    )\n\n    return VanDerPolDataModule(\n        hp=hp,\n        gen_props=gen_props,\n        noise_std=NOISE_STD,\n        validation=validation,\n        callbacks=[DataScaling(y_scale=1.0)],\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_csv/#anypinn.cli.scaffold.van_der_pol.ode_csv.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/van_der_pol/ode_csv.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    props = ODEProperties(\n        ode=vdp_scaled,\n        y0=torch.tensor([U0]),\n        order=2,\n        dy0=[torch.tensor([DU0])],\n        args={},\n    )\n\n    fields = FieldsRegistry(\n        {\n            U_KEY: Field(config=hp.fields_config),\n        }\n    )\n    params = ParamsRegistry(\n        {\n            MU_KEY: Parameter(config=hp.params_config),\n        }\n    )\n\n    def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n        u_pred = fields[U_KEY](x_data)\n        return u_pred.unsqueeze(1)\n\n    return ODEInverseProblem(\n        props=props,\n        hp=hp,\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_csv/#anypinn.cli.scaffold.van_der_pol.ode_csv.vdp_scaled","title":"<code>vdp_scaled(x: Tensor, y: Tensor, args: ArgsRegistry, derivs: list[Tensor] | None = None) -&gt; Tensor</code>","text":"<p>Native second-order Van der Pol ODE (scaled time).</p> Source code in <code>src/anypinn/cli/scaffold/van_der_pol/ode_csv.py</code> <pre><code>def vdp_scaled(\n    x: Tensor,\n    y: Tensor,\n    args: ArgsRegistry,\n    derivs: list[Tensor] | None = None,\n) -&gt; Tensor:\n    \"\"\"Native second-order Van der Pol ODE (scaled time).\"\"\"\n    assert derivs is not None\n    u = y[0]  # (m, 1)\n    du_dtau = derivs[0][0]  # (m, 1) \u2014 derivative w.r.t. scaled time tau in [0,1]\n    mu = args[MU_KEY]\n    # Physical ODE: d2u/dt2 = mu*(1-u^2)*du/dt - u\n    # With tau = t/T: d2u/dtau2 = T*mu*(1-u^2)*du/dtau - T^2*u\n    return (T_TOTAL * mu(x) * (1 - u**2) * du_dtau - T_TOTAL**2 * u).unsqueeze(0)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_synthetic/","title":"ode_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_synthetic/#anypinn.cli.scaffold.van_der_pol.ode_synthetic","title":"<code>anypinn.cli.scaffold.van_der_pol.ode_synthetic</code>","text":"<p>Van der Pol oscillator \u2014 mathematical definition.</p>"},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_synthetic/#anypinn.cli.scaffold.van_der_pol.ode_synthetic.DU0","title":"<code>DU0 = 0.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_synthetic/#anypinn.cli.scaffold.van_der_pol.ode_synthetic.MU_KEY","title":"<code>MU_KEY = 'mu'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_synthetic/#anypinn.cli.scaffold.van_der_pol.ode_synthetic.NOISE_STD","title":"<code>NOISE_STD = 0.05</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_synthetic/#anypinn.cli.scaffold.van_der_pol.ode_synthetic.TRUE_MU","title":"<code>TRUE_MU = 1.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_synthetic/#anypinn.cli.scaffold.van_der_pol.ode_synthetic.T_TOTAL","title":"<code>T_TOTAL = 20</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_synthetic/#anypinn.cli.scaffold.van_der_pol.ode_synthetic.U0","title":"<code>U0 = 2.0</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_synthetic/#anypinn.cli.scaffold.van_der_pol.ode_synthetic.U_KEY","title":"<code>U_KEY = 'u'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_synthetic/#anypinn.cli.scaffold.van_der_pol.ode_synthetic.validation","title":"<code>validation: ValidationRegistry = {MU_KEY: lambda x: torch.full_like(x, TRUE_MU)}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_synthetic/#anypinn.cli.scaffold.van_der_pol.ode_synthetic.create_data_module","title":"<code>create_data_module(hp: ODEHyperparameters)</code>","text":"Source code in <code>src/anypinn/cli/scaffold/van_der_pol/ode_synthetic.py</code> <pre><code>def create_data_module(hp: ODEHyperparameters):\n    from anypinn.catalog.van_der_pol import VanDerPolDataModule\n\n    def vdp_unscaled(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor:\n        u, v = y\n        mu = args[MU_KEY]\n        du = v\n        dv = mu(x) * (1 - u**2) * v - u\n        return torch.stack([du, dv])\n\n    gen_props = ODEProperties(\n        ode=vdp_unscaled,\n        y0=torch.tensor([U0, DU0]),\n        args={\n            MU_KEY: Argument(TRUE_MU),\n        },\n    )\n\n    return VanDerPolDataModule(\n        hp=hp,\n        gen_props=gen_props,\n        noise_std=NOISE_STD,\n        validation=validation,\n        callbacks=[DataScaling(y_scale=1.0)],\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_synthetic/#anypinn.cli.scaffold.van_der_pol.ode_synthetic.create_problem","title":"<code>create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/van_der_pol/ode_synthetic.py</code> <pre><code>def create_problem(hp: ODEHyperparameters) -&gt; ODEInverseProblem:\n    props = ODEProperties(\n        ode=vdp_scaled,\n        y0=torch.tensor([U0]),\n        order=2,\n        dy0=[torch.tensor([DU0])],\n        args={},\n    )\n\n    fields = FieldsRegistry(\n        {\n            U_KEY: Field(config=hp.fields_config),\n        }\n    )\n    params = ParamsRegistry(\n        {\n            MU_KEY: Parameter(config=hp.params_config),\n        }\n    )\n\n    def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n        u_pred = fields[U_KEY](x_data)\n        return u_pred.unsqueeze(1)\n\n    return ODEInverseProblem(\n        props=props,\n        hp=hp,\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/van_der_pol/ode_synthetic/#anypinn.cli.scaffold.van_der_pol.ode_synthetic.vdp_scaled","title":"<code>vdp_scaled(x: Tensor, y: Tensor, args: ArgsRegistry, derivs: list[Tensor] | None = None) -&gt; Tensor</code>","text":"<p>Native second-order Van der Pol ODE (scaled time).</p> Source code in <code>src/anypinn/cli/scaffold/van_der_pol/ode_synthetic.py</code> <pre><code>def vdp_scaled(\n    x: Tensor,\n    y: Tensor,\n    args: ArgsRegistry,\n    derivs: list[Tensor] | None = None,\n) -&gt; Tensor:\n    \"\"\"Native second-order Van der Pol ODE (scaled time).\"\"\"\n    assert derivs is not None\n    u = y[0]  # (m, 1)\n    du_dtau = derivs[0][0]  # (m, 1) \u2014 derivative w.r.t. scaled time tau in [0,1]\n    mu = args[MU_KEY]\n    # Physical ODE: d2u/dt2 = mu*(1-u^2)*du/dt - u\n    # With tau = t/T: d2u/dtau2 = T*mu*(1-u^2)*du/dtau - T^2*u\n    return (T_TOTAL * mu(x) * (1 - u**2) * du_dtau - T_TOTAL**2 * u).unsqueeze(0)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/wave_1d/","title":"wave_1d","text":""},{"location":"reference/anypinn/cli/scaffold/wave_1d/#anypinn.cli.scaffold.wave_1d","title":"<code>anypinn.cli.scaffold.wave_1d</code>","text":""},{"location":"reference/anypinn/cli/scaffold/wave_1d/config_csv/","title":"config_csv","text":""},{"location":"reference/anypinn/cli/scaffold/wave_1d/config_csv/#anypinn.cli.scaffold.wave_1d.config_csv","title":"<code>anypinn.cli.scaffold.wave_1d.config_csv</code>","text":"<p>Wave Equation 1D \u2014 training configuration (CSV).</p>"},{"location":"reference/anypinn/cli/scaffold/wave_1d/config_csv/#anypinn.cli.scaffold.wave_1d.config_csv.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/wave_1d/config_csv/#anypinn.cli.scaffold.wave_1d.config_csv.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/wave_1d/config_csv/#anypinn.cli.scaffold.wave_1d.config_csv.hp","title":"<code>hp = PINNHyperparameters(lr=0.001, max_epochs=3000, gradient_clip_val=0.5, training_data=(IngestionConfig(batch_size=128, data_ratio=2, collocations=8000, collocation_sampler='uniform', df_path=(Path('./data/data.csv')), y_columns=['u'])), fields_config=(MLPConfig(in_dim=26, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.5)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=100, threshold=0.0001, min_lr=1e-06)))</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/wave_1d/config_synthetic/","title":"config_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/wave_1d/config_synthetic/#anypinn.cli.scaffold.wave_1d.config_synthetic","title":"<code>anypinn.cli.scaffold.wave_1d.config_synthetic</code>","text":"<p>Wave Equation 1D \u2014 training configuration (synthetic).</p>"},{"location":"reference/anypinn/cli/scaffold/wave_1d/config_synthetic/#anypinn.cli.scaffold.wave_1d.config_synthetic.EXPERIMENT_NAME","title":"<code>EXPERIMENT_NAME = '__EXPERIMENT_NAME__'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/wave_1d/config_synthetic/#anypinn.cli.scaffold.wave_1d.config_synthetic.RUN_NAME","title":"<code>RUN_NAME = 'v0'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/wave_1d/config_synthetic/#anypinn.cli.scaffold.wave_1d.config_synthetic.hp","title":"<code>hp = PINNHyperparameters(lr=0.001, max_epochs=3000, gradient_clip_val=0.5, training_data=(GenerationConfig(batch_size=128, data_ratio=2, collocations=8000, collocation_sampler='uniform', x=(torch.linspace(0, 1, steps=50)), args_to_train={}, noise_level=0)), fields_config=(MLPConfig(in_dim=26, out_dim=1, hidden_layers=[64, 128, 128, 64], activation='tanh', output_activation=None)), params_config=(ScalarConfig(init_value=0.5)), scheduler=(ReduceLROnPlateauConfig(mode='min', factor=0.5, patience=100, threshold=0.0001, min_lr=1e-06)))</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/wave_1d/ode_csv/","title":"ode_csv","text":""},{"location":"reference/anypinn/cli/scaffold/wave_1d/ode_csv/#anypinn.cli.scaffold.wave_1d.ode_csv","title":"<code>anypinn.cli.scaffold.wave_1d.ode_csv</code>","text":"<p>Wave Equation 1D \u2014 inverse PDE problem definition (CSV data).</p>"},{"location":"reference/anypinn/cli/scaffold/wave_1d/ode_csv/#anypinn.cli.scaffold.wave_1d.ode_csv.GRID_SIZE","title":"<code>GRID_SIZE = 50</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/wave_1d/ode_csv/#anypinn.cli.scaffold.wave_1d.ode_csv.validation","title":"<code>validation: ValidationRegistry = {C_KEY: lambda x: torch.full_like(x, TRUE_C)}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/wave_1d/ode_csv/#anypinn.cli.scaffold.wave_1d.ode_csv.create_data_module","title":"<code>create_data_module(hp: PINNHyperparameters) -&gt; Wave1DDataModule</code>","text":"Source code in <code>src/anypinn/cli/scaffold/wave_1d/ode_csv.py</code> <pre><code>def create_data_module(hp: PINNHyperparameters) -&gt; Wave1DDataModule:\n    return Wave1DDataModule(\n        hp=hp,\n        true_c=TRUE_C,\n        grid_size=GRID_SIZE,\n        validation=validation,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/wave_1d/ode_csv/#anypinn.cli.scaffold.wave_1d.ode_csv.create_problem","title":"<code>create_problem(hp: PINNHyperparameters) -&gt; Problem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/wave_1d/ode_csv.py</code> <pre><code>def create_problem(hp: PINNHyperparameters) -&gt; Problem:\n    encode = FourierEncoding(num_frequencies=6)\n    field_u = Field(\n        config=MLPConfig(\n            in_dim=encode.out_dim(2),\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=hp.fields_config.output_activation,\n            encode=encode,\n        )\n    )\n    param_c = Parameter(\n        config=ScalarConfig(init_value=hp.params_config.init_value),\n    )\n\n    fields = FieldsRegistry({U_KEY: field_u})\n    params = ParamsRegistry({C_KEY: param_c})\n\n    bcs = [\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_left_boundary, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_left\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_right_boundary, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_right\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_initial_condition, value=_ic_value, n_pts=100),\n            field_u,\n            log_key=\"loss/ic\",\n            weight=10.0,\n        ),\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_initial_condition, value=_zero, n_pts=100),\n            field_u,\n            normal_dim=1,\n            log_key=\"loss/ic_velocity\",\n            weight=10.0,\n        ),\n    ]\n\n    pde = PDEResidualConstraint(\n        fields=fields,\n        params=params,\n        residual_fn=wave_residual,\n        log_key=\"loss/pde_residual\",\n        weight=1.0,\n    )\n\n    data = DataConstraint(\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n        weight=5.0,\n    )\n\n    return Problem(\n        constraints=[pde, *bcs, data],\n        criterion=build_criterion(hp.criterion),\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/wave_1d/ode_csv/#anypinn.cli.scaffold.wave_1d.ode_csv.predict_data","title":"<code>predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/cli/scaffold/wave_1d/ode_csv.py</code> <pre><code>def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n    return fields[U_KEY](x_data).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/wave_1d/ode_csv/#anypinn.cli.scaffold.wave_1d.ode_csv.wave_residual","title":"<code>wave_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor</code>","text":"<p>PDE residual: d2u/dt2 - c^2 * d2u/dx2 = 0.</p> Source code in <code>src/anypinn/cli/scaffold/wave_1d/ode_csv.py</code> <pre><code>def wave_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor:\n    \"\"\"PDE residual: d2u/dt2 - c^2 * d2u/dx2 = 0.\"\"\"\n    u = fields[U_KEY](x)\n    c = params[C_KEY](x)\n    d2u_dt2 = partial(u, x, dim=1, order=2)\n    d2u_dx2 = partial(u, x, dim=0, order=2)\n    return d2u_dt2 - c * c * d2u_dx2\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/wave_1d/ode_synthetic/","title":"ode_synthetic","text":""},{"location":"reference/anypinn/cli/scaffold/wave_1d/ode_synthetic/#anypinn.cli.scaffold.wave_1d.ode_synthetic","title":"<code>anypinn.cli.scaffold.wave_1d.ode_synthetic</code>","text":"<p>Wave Equation 1D \u2014 inverse PDE problem definition (synthetic).</p>"},{"location":"reference/anypinn/cli/scaffold/wave_1d/ode_synthetic/#anypinn.cli.scaffold.wave_1d.ode_synthetic.GRID_SIZE","title":"<code>GRID_SIZE = 50</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/wave_1d/ode_synthetic/#anypinn.cli.scaffold.wave_1d.ode_synthetic.validation","title":"<code>validation: ValidationRegistry = {C_KEY: lambda x: torch.full_like(x, TRUE_C)}</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/cli/scaffold/wave_1d/ode_synthetic/#anypinn.cli.scaffold.wave_1d.ode_synthetic.create_data_module","title":"<code>create_data_module(hp: PINNHyperparameters) -&gt; Wave1DDataModule</code>","text":"Source code in <code>src/anypinn/cli/scaffold/wave_1d/ode_synthetic.py</code> <pre><code>def create_data_module(hp: PINNHyperparameters) -&gt; Wave1DDataModule:\n    return Wave1DDataModule(\n        hp=hp,\n        true_c=TRUE_C,\n        grid_size=GRID_SIZE,\n        validation=validation,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/wave_1d/ode_synthetic/#anypinn.cli.scaffold.wave_1d.ode_synthetic.create_problem","title":"<code>create_problem(hp: PINNHyperparameters) -&gt; Problem</code>","text":"Source code in <code>src/anypinn/cli/scaffold/wave_1d/ode_synthetic.py</code> <pre><code>def create_problem(hp: PINNHyperparameters) -&gt; Problem:\n    encode = FourierEncoding(num_frequencies=6)\n    field_u = Field(\n        config=MLPConfig(\n            in_dim=encode.out_dim(2),\n            out_dim=1,\n            hidden_layers=hp.fields_config.hidden_layers,\n            activation=hp.fields_config.activation,\n            output_activation=hp.fields_config.output_activation,\n            encode=encode,\n        )\n    )\n    param_c = Parameter(\n        config=ScalarConfig(init_value=hp.params_config.init_value),\n    )\n\n    fields = FieldsRegistry({U_KEY: field_u})\n    params = ParamsRegistry({C_KEY: param_c})\n\n    bcs = [\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_left_boundary, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_left\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_right_boundary, value=_zero, n_pts=100),\n            field_u,\n            log_key=\"loss/bc_right\",\n            weight=10.0,\n        ),\n        DirichletBCConstraint(\n            BoundaryCondition(sampler=_initial_condition, value=_ic_value, n_pts=100),\n            field_u,\n            log_key=\"loss/ic\",\n            weight=10.0,\n        ),\n        NeumannBCConstraint(\n            BoundaryCondition(sampler=_initial_condition, value=_zero, n_pts=100),\n            field_u,\n            normal_dim=1,\n            log_key=\"loss/ic_velocity\",\n            weight=10.0,\n        ),\n    ]\n\n    pde = PDEResidualConstraint(\n        fields=fields,\n        params=params,\n        residual_fn=wave_residual,\n        log_key=\"loss/pde_residual\",\n        weight=1.0,\n    )\n\n    data = DataConstraint(\n        fields=fields,\n        params=params,\n        predict_data=predict_data,\n        weight=5.0,\n    )\n\n    return Problem(\n        constraints=[pde, *bcs, data],\n        criterion=build_criterion(hp.criterion),\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/wave_1d/ode_synthetic/#anypinn.cli.scaffold.wave_1d.ode_synthetic.predict_data","title":"<code>predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/cli/scaffold/wave_1d/ode_synthetic.py</code> <pre><code>def predict_data(x_data: Tensor, fields: FieldsRegistry, _params: ParamsRegistry) -&gt; Tensor:\n    return fields[U_KEY](x_data).unsqueeze(1)\n</code></pre>"},{"location":"reference/anypinn/cli/scaffold/wave_1d/ode_synthetic/#anypinn.cli.scaffold.wave_1d.ode_synthetic.wave_residual","title":"<code>wave_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor</code>","text":"<p>PDE residual: d2u/dt2 - c^2 * d2u/dx2 = 0.</p> Source code in <code>src/anypinn/cli/scaffold/wave_1d/ode_synthetic.py</code> <pre><code>def wave_residual(x: Tensor, fields: FieldsRegistry, params: ParamsRegistry) -&gt; Tensor:\n    \"\"\"PDE residual: d2u/dt2 - c^2 * d2u/dx2 = 0.\"\"\"\n    u = fields[U_KEY](x)\n    c = params[C_KEY](x)\n    d2u_dt2 = partial(u, x, dim=1, order=2)\n    d2u_dx2 = partial(u, x, dim=0, order=2)\n    return d2u_dt2 - c * c * d2u_dx2\n</code></pre>"},{"location":"reference/anypinn/core/","title":"core","text":""},{"location":"reference/anypinn/core/#anypinn.core","title":"<code>anypinn.core</code>","text":"<p>Core PINN building blocks.</p>"},{"location":"reference/anypinn/core/#anypinn.core.Activations","title":"<code>Activations: TypeAlias = Literal['tanh', 'relu', 'leaky_relu', 'sigmoid', 'selu', 'softplus', 'identity']</code>  <code>module-attribute</code>","text":"<p>Supported activation functions.</p>"},{"location":"reference/anypinn/core/#anypinn.core.ArgsRegistry","title":"<code>ArgsRegistry: TypeAlias = dict[str, Argument]</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.CollocationStrategies","title":"<code>CollocationStrategies: TypeAlias = Literal['uniform', 'random', 'latin_hypercube', 'log_uniform_1d', 'adaptive']</code>  <code>module-attribute</code>","text":"<p>Supported collocation sampling strategies.</p>"},{"location":"reference/anypinn/core/#anypinn.core.Criteria","title":"<code>Criteria: TypeAlias = Literal['mse', 'huber', 'l1']</code>  <code>module-attribute</code>","text":"<p>Supported loss criteria.</p>"},{"location":"reference/anypinn/core/#anypinn.core.DataBatch","title":"<code>DataBatch: TypeAlias = tuple[Tensor, Tensor]</code>  <code>module-attribute</code>","text":"<p>Type alias for data batch: (x, y).</p>"},{"location":"reference/anypinn/core/#anypinn.core.FieldsRegistry","title":"<code>FieldsRegistry: TypeAlias = dict[str, Field]</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.LOSS_KEY","title":"<code>LOSS_KEY = 'loss'</code>  <code>module-attribute</code>","text":"<p>Key used for logging the total loss.</p>"},{"location":"reference/anypinn/core/#anypinn.core.ParamsRegistry","title":"<code>ParamsRegistry: TypeAlias = dict[str, Parameter]</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.Predictions","title":"<code>Predictions: TypeAlias = tuple[DataBatch, dict[str, Tensor], dict[str, Tensor] | None]</code>  <code>module-attribute</code>","text":"<p>Type alias for model predictions: (input_batch, predictions_dictionary, true_values_dictionary)  where predictions_dictionary is a dictionary of {[field_name | param_name]: prediction} and where true_values_dictionary is a dictionary of {[field_name | param_name]: true_value}. If no validation source is configured, true_values_dictionary is None.</p>"},{"location":"reference/anypinn/core/#anypinn.core.ResolvedValidation","title":"<code>ResolvedValidation: TypeAlias = dict[str, Callable[[Tensor], Tensor]]</code>  <code>module-attribute</code>","text":"<p>Validation registry after ColumnRef entries have been resolved to callables.</p>"},{"location":"reference/anypinn/core/#anypinn.core.TrainingBatch","title":"<code>TrainingBatch: TypeAlias = tuple[DataBatch, Tensor]</code>  <code>module-attribute</code>","text":"<p>Training batch tuple: ((x_data, y_data), x_coll).</p>"},{"location":"reference/anypinn/core/#anypinn.core.ValidationRegistry","title":"<code>ValidationRegistry: TypeAlias = dict[str, ValidationSource]</code>  <code>module-attribute</code>","text":"<p>Registry mapping parameter names to their validation sources.</p> Example <p>validation: ValidationRegistry = { ...     \"beta\": lambda x: torch.sin(x),  # Pure function ...     \"gamma\": ColumnRef(column=\"gamma_true\"),  # From data ...     \"delta\": None,  # No validation ... }</p>"},{"location":"reference/anypinn/core/#anypinn.core.ValidationSource","title":"<code>ValidationSource: TypeAlias = Callable[[Tensor], Tensor] | ColumnRef | None</code>  <code>module-attribute</code>","text":"<p>A source for ground truth values. Can be: - A callable that takes x coordinates and returns true values - A ColumnRef that references a column in loaded data - None if no validation is needed for this parameter</p>"},{"location":"reference/anypinn/core/#anypinn.core.__all__","title":"<code>__all__ = ['LOSS_KEY', 'Activations', 'AdamConfig', 'AdaptiveSampler', 'ArgsRegistry', 'Argument', 'CollocationSampler', 'CollocationStrategies', 'ColumnRef', 'Constraint', 'CosineAnnealingConfig', 'Criteria', 'DataBatch', 'DataCallback', 'Domain', 'EarlyStoppingConfig', 'Field', 'FieldsRegistry', 'FourierEncoding', 'GenerationConfig', 'InferredContext', 'IngestionConfig', 'LBFGSConfig', 'LatinHypercubeSampler', 'LogFn', 'LogUniform1DSampler', 'MLPConfig', 'PINNDataModule', 'PINNDataset', 'PINNHyperparameters', 'Parameter', 'ParamsRegistry', 'Predictions', 'Problem', 'RandomFourierFeatures', 'RandomSampler', 'ReduceLROnPlateauConfig', 'ResidualScorer', 'ResolvedValidation', 'SMMAStoppingConfig', 'ScalarConfig', 'TrainingBatch', 'TrainingDataConfig', 'UniformSampler', 'ValidationRegistry', 'ValidationSource', 'build_criterion', 'build_sampler', 'get_activation', 'resolve_validation']</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.AdamConfig","title":"<code>AdamConfig</code>  <code>dataclass</code>","text":"<p>Configuration for the Adam optimizer.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass AdamConfig:\n    \"\"\"\n    Configuration for the Adam optimizer.\n    \"\"\"\n\n    lr: float = 1e-3\n    betas: tuple[float, float] = (0.9, 0.999)\n    weight_decay: float = 0.0\n\n    def __post_init__(self) -&gt; None:\n        if self.lr &lt;= 0:\n            raise ValueError(f\"lr must be positive, got {self.lr}.\")\n        if self.weight_decay &lt; 0:\n            raise ValueError(f\"weight_decay must be non-negative, got {self.weight_decay}.\")\n        if not (0 &lt; self.betas[0] &lt; 1):\n            raise ValueError(f\"betas[0] must be in (0, 1), got {self.betas[0]}.\")\n        if not (0 &lt; self.betas[1] &lt; 1):\n            raise ValueError(f\"betas[1] must be in (0, 1), got {self.betas[1]}.\")\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.AdamConfig.betas","title":"<code>betas: tuple[float, float] = (0.9, 0.999)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.AdamConfig.lr","title":"<code>lr: float = 0.001</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.AdamConfig.weight_decay","title":"<code>weight_decay: float = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.AdamConfig.__init__","title":"<code>__init__(*, lr: float = 0.001, betas: tuple[float, float] = (0.9, 0.999), weight_decay: float = 0.0) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.AdamConfig.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/core/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.lr &lt;= 0:\n        raise ValueError(f\"lr must be positive, got {self.lr}.\")\n    if self.weight_decay &lt; 0:\n        raise ValueError(f\"weight_decay must be non-negative, got {self.weight_decay}.\")\n    if not (0 &lt; self.betas[0] &lt; 1):\n        raise ValueError(f\"betas[0] must be in (0, 1), got {self.betas[0]}.\")\n    if not (0 &lt; self.betas[1] &lt; 1):\n        raise ValueError(f\"betas[1] must be in (0, 1), got {self.betas[1]}.\")\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.AdaptiveSampler","title":"<code>AdaptiveSampler</code>","text":"<p>Residual-weighted adaptive collocation sampler.</p> <p>Draws an oversample of candidate points, scores them using a <code>ResidualScorer</code>, and retains the top-scoring subset. A configurable <code>exploration_ratio</code> ensures a fraction of purely random points to prevent mode collapse.</p> <p>Parameters:</p> Name Type Description Default <code>scorer</code> <code>ResidualScorer</code> <p>Callable returning per-point residual scores <code>(n,)</code>.</p> required <code>oversample_factor</code> <code>int</code> <p>Multiplier on <code>n</code> for candidate generation.</p> <code>4</code> <code>exploration_ratio</code> <code>float</code> <p>Fraction of the budget reserved for random points.</p> <code>0.2</code> <code>seed</code> <code>int | None</code> <p>Optional seed for reproducible sampling.</p> <code>None</code> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>class AdaptiveSampler:\n    \"\"\"Residual-weighted adaptive collocation sampler.\n\n    Draws an oversample of candidate points, scores them using a\n    ``ResidualScorer``, and retains the top-scoring subset. A configurable\n    ``exploration_ratio`` ensures a fraction of purely random points to prevent\n    mode collapse.\n\n    Args:\n        scorer: Callable returning per-point residual scores ``(n,)``.\n        oversample_factor: Multiplier on ``n`` for candidate generation.\n        exploration_ratio: Fraction of the budget reserved for random points.\n        seed: Optional seed for reproducible sampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        scorer: ResidualScorer,\n        oversample_factor: int = 4,\n        exploration_ratio: float = 0.2,\n        seed: int | None = None,\n    ) -&gt; None:\n        if oversample_factor &lt; 1:\n            raise ValueError(f\"oversample_factor must be &gt;= 1, got {oversample_factor}.\")\n        if not (0.0 &lt;= exploration_ratio &lt;= 1.0):\n            raise ValueError(f\"exploration_ratio must be in [0, 1], got {exploration_ratio}.\")\n        self._scorer = scorer\n        self._oversample = oversample_factor\n        self._explore = exploration_ratio\n        self._random = RandomSampler(seed=seed)\n\n    def sample(self, n: int, domain: Domain) -&gt; Tensor:\n        n_explore = max(1, int(n * self._explore))\n        n_exploit = n - n_explore\n\n        explore_pts = self._random.sample(n_explore, domain)\n\n        if n_exploit &lt;= 0:\n            return explore_pts\n\n        n_candidates = n_exploit * self._oversample\n        candidates = self._random.sample(n_candidates, domain)\n\n        with torch.no_grad():\n            scores = self._scorer.residual_score(candidates)\n\n        _, top_idx = scores.topk(min(n_exploit, len(scores)))\n        exploit_pts = candidates[top_idx]\n\n        return torch.cat([explore_pts, exploit_pts], dim=0)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.AdaptiveSampler.__init__","title":"<code>__init__(scorer: ResidualScorer, oversample_factor: int = 4, exploration_ratio: float = 0.2, seed: int | None = None) -&gt; None</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def __init__(\n    self,\n    scorer: ResidualScorer,\n    oversample_factor: int = 4,\n    exploration_ratio: float = 0.2,\n    seed: int | None = None,\n) -&gt; None:\n    if oversample_factor &lt; 1:\n        raise ValueError(f\"oversample_factor must be &gt;= 1, got {oversample_factor}.\")\n    if not (0.0 &lt;= exploration_ratio &lt;= 1.0):\n        raise ValueError(f\"exploration_ratio must be in [0, 1], got {exploration_ratio}.\")\n    self._scorer = scorer\n    self._oversample = oversample_factor\n    self._explore = exploration_ratio\n    self._random = RandomSampler(seed=seed)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.AdaptiveSampler.sample","title":"<code>sample(n: int, domain: Domain) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def sample(self, n: int, domain: Domain) -&gt; Tensor:\n    n_explore = max(1, int(n * self._explore))\n    n_exploit = n - n_explore\n\n    explore_pts = self._random.sample(n_explore, domain)\n\n    if n_exploit &lt;= 0:\n        return explore_pts\n\n    n_candidates = n_exploit * self._oversample\n    candidates = self._random.sample(n_candidates, domain)\n\n    with torch.no_grad():\n        scores = self._scorer.residual_score(candidates)\n\n    _, top_idx = scores.topk(min(n_exploit, len(scores)))\n    exploit_pts = candidates[top_idx]\n\n    return torch.cat([explore_pts, exploit_pts], dim=0)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Argument","title":"<code>Argument</code>","text":"<p>Represents an argument that can be passed to an ODE/PDE function. Can be a fixed float value or a callable function.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float | Callable[[Tensor], Tensor]</code> <p>The value (float) or function (callable).</p> required Source code in <code>src/anypinn/core/nn.py</code> <pre><code>class Argument:\n    \"\"\"\n    Represents an argument that can be passed to an ODE/PDE function.\n    Can be a fixed float value or a callable function.\n\n    Args:\n        value: The value (float) or function (callable).\n    \"\"\"\n\n    def __init__(self, value: float | Callable[[Tensor], Tensor]):\n        self._value = value\n        self._tensor_cache: dict[torch.device, Tensor] = {}\n\n    def __call__(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Evaluate the argument.\n\n        Args:\n            x: Input tensor (context).\n\n        Returns:\n            The value of the argument, broadcasted if necessary.\n        \"\"\"\n        if callable(self._value):\n            return self._value(x)\n        device = x.device\n        if device not in self._tensor_cache:\n            self._tensor_cache[device] = torch.tensor(self._value, device=device)\n        return self._tensor_cache[device]\n\n    @override\n    def __repr__(self) -&gt; str:\n        return f\"Argument(value={self._value})\"\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Argument.__call__","title":"<code>__call__(x: Tensor) -&gt; Tensor</code>","text":"<p>Evaluate the argument.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor (context).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The value of the argument, broadcasted if necessary.</p> Source code in <code>src/anypinn/core/nn.py</code> <pre><code>def __call__(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Evaluate the argument.\n\n    Args:\n        x: Input tensor (context).\n\n    Returns:\n        The value of the argument, broadcasted if necessary.\n    \"\"\"\n    if callable(self._value):\n        return self._value(x)\n    device = x.device\n    if device not in self._tensor_cache:\n        self._tensor_cache[device] = torch.tensor(self._value, device=device)\n    return self._tensor_cache[device]\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Argument.__init__","title":"<code>__init__(value: float | Callable[[Tensor], Tensor])</code>","text":"Source code in <code>src/anypinn/core/nn.py</code> <pre><code>def __init__(self, value: float | Callable[[Tensor], Tensor]):\n    self._value = value\n    self._tensor_cache: dict[torch.device, Tensor] = {}\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Argument.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"Source code in <code>src/anypinn/core/nn.py</code> <pre><code>@override\ndef __repr__(self) -&gt; str:\n    return f\"Argument(value={self._value})\"\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.CollocationSampler","title":"<code>CollocationSampler</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for collocation point samplers.</p> <p>Implementations must return a tensor of shape <code>(n, domain.ndim)</code> with all points inside the domain bounds.</p> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>class CollocationSampler(Protocol):\n    \"\"\"Protocol for collocation point samplers.\n\n    Implementations must return a tensor of shape ``(n, domain.ndim)`` with all\n    points inside the domain bounds.\n    \"\"\"\n\n    def sample(self, n: int, domain: Domain) -&gt; Tensor: ...\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.CollocationSampler.sample","title":"<code>sample(n: int, domain: Domain) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def sample(self, n: int, domain: Domain) -&gt; Tensor: ...\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.ColumnRef","title":"<code>ColumnRef</code>  <code>dataclass</code>","text":"<p>Reference to a column in loaded data for ground truth comparison.</p> <p>This allows practitioners to specify validation data by column name without writing custom functions. The column is resolved lazily when data is loaded.</p> <p>Attributes:</p> Name Type Description <code>column</code> <code>str</code> <p>Name of the column in the loaded DataFrame.</p> <code>transform</code> <code>Callable[[Tensor], Tensor] | None</code> <p>Optional transformation to apply to the column values.</p> Example <p>validation = { ...     \"beta\": ColumnRef(column=\"Rt\", transform=lambda rt: rt * delta), ... }</p> Source code in <code>src/anypinn/core/validation.py</code> <pre><code>@dataclass\nclass ColumnRef:\n    \"\"\"\n    Reference to a column in loaded data for ground truth comparison.\n\n    This allows practitioners to specify validation data by column name\n    without writing custom functions. The column is resolved lazily when\n    data is loaded.\n\n    Attributes:\n        column: Name of the column in the loaded DataFrame.\n        transform: Optional transformation to apply to the column values.\n\n    Example:\n        &gt;&gt;&gt; validation = {\n        ...     \"beta\": ColumnRef(column=\"Rt\", transform=lambda rt: rt * delta),\n        ... }\n    \"\"\"\n\n    column: str\n    transform: Callable[[Tensor], Tensor] | None = None\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.ColumnRef.column","title":"<code>column: str</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.ColumnRef.transform","title":"<code>transform: Callable[[Tensor], Tensor] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.ColumnRef.__init__","title":"<code>__init__(column: str, transform: Callable[[Tensor], Tensor] | None = None) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.Constraint","title":"<code>Constraint</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for a constraint (loss term) in the PINN. Returns a loss value for the given batch.</p> Source code in <code>src/anypinn/core/problem.py</code> <pre><code>class Constraint(ABC):\n    \"\"\"\n    Abstract base class for a constraint (loss term) in the PINN.\n    Returns a loss value for the given batch.\n    \"\"\"\n\n    def inject_context(self, context: InferredContext) -&gt; None:\n        \"\"\"\n        Inject the context into the constraint. This can be used by the constraint to access the\n        data used to compute the loss.\n\n        Args:\n            context: The context to inject.\n        \"\"\"\n        return None\n\n    @abstractmethod\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        \"\"\"\n        Calculate the loss for this constraint.\n\n        Args:\n            batch: The current batch of data/collocation points.\n            criterion: The loss function (e.g. MSE).\n            log: Optional logging function.\n\n        Returns:\n            The calculated loss tensor.\n        \"\"\"\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Constraint.inject_context","title":"<code>inject_context(context: InferredContext) -&gt; None</code>","text":"<p>Inject the context into the constraint. This can be used by the constraint to access the data used to compute the loss.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>InferredContext</code> <p>The context to inject.</p> required Source code in <code>src/anypinn/core/problem.py</code> <pre><code>def inject_context(self, context: InferredContext) -&gt; None:\n    \"\"\"\n    Inject the context into the constraint. This can be used by the constraint to access the\n    data used to compute the loss.\n\n    Args:\n        context: The context to inject.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Constraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>  <code>abstractmethod</code>","text":"<p>Calculate the loss for this constraint.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>TrainingBatch</code> <p>The current batch of data/collocation points.</p> required <code>criterion</code> <code>Module</code> <p>The loss function (e.g. MSE).</p> required <code>log</code> <code>LogFn | None</code> <p>Optional logging function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The calculated loss tensor.</p> Source code in <code>src/anypinn/core/problem.py</code> <pre><code>@abstractmethod\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    \"\"\"\n    Calculate the loss for this constraint.\n\n    Args:\n        batch: The current batch of data/collocation points.\n        criterion: The loss function (e.g. MSE).\n        log: Optional logging function.\n\n    Returns:\n        The calculated loss tensor.\n    \"\"\"\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.CosineAnnealingConfig","title":"<code>CosineAnnealingConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Cosine Annealing LR Scheduler.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass CosineAnnealingConfig:\n    \"\"\"\n    Configuration for Cosine Annealing LR Scheduler.\n    \"\"\"\n\n    T_max: int\n    eta_min: float = 0.0\n\n    def __post_init__(self) -&gt; None:\n        if self.T_max &lt;= 0:\n            raise ValueError(f\"T_max must be positive, got {self.T_max}.\")\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.CosineAnnealingConfig.T_max","title":"<code>T_max: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.CosineAnnealingConfig.eta_min","title":"<code>eta_min: float = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.CosineAnnealingConfig.__init__","title":"<code>__init__(*, T_max: int, eta_min: float = 0.0) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.CosineAnnealingConfig.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/core/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.T_max &lt;= 0:\n        raise ValueError(f\"T_max must be positive, got {self.T_max}.\")\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.DataCallback","title":"<code>DataCallback</code>","text":"<p>Abstract base class for building new data callbacks.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>class DataCallback:\n    \"\"\"Abstract base class for building new data callbacks.\"\"\"\n\n    def transform_data(self, data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]:\n        \"\"\"Transform the data and collocation points.\"\"\"\n        return data, coll\n\n    def on_after_setup(self, dm: \"PINNDataModule\") -&gt; None:\n        \"\"\"Called after setup is complete.\"\"\"\n        return None\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.DataCallback.on_after_setup","title":"<code>on_after_setup(dm: PINNDataModule) -&gt; None</code>","text":"<p>Called after setup is complete.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>def on_after_setup(self, dm: \"PINNDataModule\") -&gt; None:\n    \"\"\"Called after setup is complete.\"\"\"\n    return None\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.DataCallback.transform_data","title":"<code>transform_data(data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]</code>","text":"<p>Transform the data and collocation points.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>def transform_data(self, data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]:\n    \"\"\"Transform the data and collocation points.\"\"\"\n    return data, coll\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Domain","title":"<code>Domain</code>  <code>dataclass</code>","text":"<p>N-dimensional rectangular domain.</p> <p>Attributes:</p> Name Type Description <code>bounds</code> <code>list[tuple[float, float]]</code> <p>Per-dimension (min, max) pairs. <code>bounds[i]</code> covers dimension i.</p> <code>dx</code> <code>list[float] | None</code> <p>Per-dimension step size (<code>None</code> when not applicable).</p> Source code in <code>src/anypinn/core/nn.py</code> <pre><code>@dataclass\nclass Domain:\n    \"\"\"\n    N-dimensional rectangular domain.\n\n    Attributes:\n        bounds: Per-dimension (min, max) pairs. ``bounds[i]`` covers dimension i.\n        dx: Per-dimension step size (``None`` when not applicable).\n    \"\"\"\n\n    bounds: list[tuple[float, float]]\n    dx: list[float] | None = None\n\n    @property\n    def ndim(self) -&gt; int:\n        \"\"\"Number of spatial dimensions.\"\"\"\n        return len(self.bounds)\n\n    @property\n    def x0(self) -&gt; float:\n        \"\"\"Lower bound of the first dimension (convenience for 1-D / time-axis access).\"\"\"\n        return self.bounds[0][0]\n\n    @property\n    def x1(self) -&gt; float:\n        \"\"\"Upper bound of the first dimension.\"\"\"\n        return self.bounds[0][1]\n\n    @classmethod\n    def from_x(cls, x: Tensor) -&gt; Domain:\n        \"\"\"\n        Infer domain bounds and step sizes from a coordinate tensor of shape (N, d).\n\n        Args:\n            x: Coordinate tensor of shape ``(N, d)``.\n\n        Returns:\n            Domain with bounds and dx inferred from the data.\n        \"\"\"\n        if x.ndim != 2:\n            raise ValueError(f\"Expected 2-D coordinate tensor (N, d), got shape {tuple(x.shape)}.\")\n        if x.shape[0] &lt; 2:\n            raise ValueError(\n                f\"At least two points are required to infer the domain, got {x.shape[0]}.\"\n            )\n\n        d = x.shape[1]\n        bounds = [(x[:, i].min().item(), x[:, i].max().item()) for i in range(d)]\n        dx = [(x[1, i] - x[0, i]).item() for i in range(d)]\n        return cls(bounds=bounds, dx=dx)\n\n    @override\n    def __repr__(self) -&gt; str:\n        return f\"Domain(ndim={self.ndim}, bounds={self.bounds}, dx={self.dx})\"\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Domain.bounds","title":"<code>bounds: list[tuple[float, float]]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.Domain.dx","title":"<code>dx: list[float] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.Domain.ndim","title":"<code>ndim: int</code>  <code>property</code>","text":"<p>Number of spatial dimensions.</p>"},{"location":"reference/anypinn/core/#anypinn.core.Domain.x0","title":"<code>x0: float</code>  <code>property</code>","text":"<p>Lower bound of the first dimension (convenience for 1-D / time-axis access).</p>"},{"location":"reference/anypinn/core/#anypinn.core.Domain.x1","title":"<code>x1: float</code>  <code>property</code>","text":"<p>Upper bound of the first dimension.</p>"},{"location":"reference/anypinn/core/#anypinn.core.Domain.__init__","title":"<code>__init__(bounds: list[tuple[float, float]], dx: list[float] | None = None) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.Domain.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"Source code in <code>src/anypinn/core/nn.py</code> <pre><code>@override\ndef __repr__(self) -&gt; str:\n    return f\"Domain(ndim={self.ndim}, bounds={self.bounds}, dx={self.dx})\"\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Domain.from_x","title":"<code>from_x(x: Tensor) -&gt; Domain</code>  <code>classmethod</code>","text":"<p>Infer domain bounds and step sizes from a coordinate tensor of shape (N, d).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Coordinate tensor of shape <code>(N, d)</code>.</p> required <p>Returns:</p> Type Description <code>Domain</code> <p>Domain with bounds and dx inferred from the data.</p> Source code in <code>src/anypinn/core/nn.py</code> <pre><code>@classmethod\ndef from_x(cls, x: Tensor) -&gt; Domain:\n    \"\"\"\n    Infer domain bounds and step sizes from a coordinate tensor of shape (N, d).\n\n    Args:\n        x: Coordinate tensor of shape ``(N, d)``.\n\n    Returns:\n        Domain with bounds and dx inferred from the data.\n    \"\"\"\n    if x.ndim != 2:\n        raise ValueError(f\"Expected 2-D coordinate tensor (N, d), got shape {tuple(x.shape)}.\")\n    if x.shape[0] &lt; 2:\n        raise ValueError(\n            f\"At least two points are required to infer the domain, got {x.shape[0]}.\"\n        )\n\n    d = x.shape[1]\n    bounds = [(x[:, i].min().item(), x[:, i].max().item()) for i in range(d)]\n    dx = [(x[1, i] - x[0, i]).item() for i in range(d)]\n    return cls(bounds=bounds, dx=dx)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.EarlyStoppingConfig","title":"<code>EarlyStoppingConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Early Stopping callback.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass EarlyStoppingConfig:\n    \"\"\"\n    Configuration for Early Stopping callback.\n    \"\"\"\n\n    patience: int\n    mode: Literal[\"min\", \"max\"]\n\n    def __post_init__(self) -&gt; None:\n        if self.patience &lt;= 0:\n            raise ValueError(f\"patience must be positive, got {self.patience}.\")\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.EarlyStoppingConfig.mode","title":"<code>mode: Literal['min', 'max']</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.EarlyStoppingConfig.patience","title":"<code>patience: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.EarlyStoppingConfig.__init__","title":"<code>__init__(*, patience: int, mode: Literal['min', 'max']) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.EarlyStoppingConfig.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/core/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.patience &lt;= 0:\n        raise ValueError(f\"patience must be positive, got {self.patience}.\")\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Field","title":"<code>Field</code>","text":"<p>               Bases: <code>Module</code></p> <p>A neural field mapping coordinates -&gt; vector of state variables. Example (ODE): t -&gt; [S, I, R].</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MLPConfig</code> <p>Configuration for the MLP backing this field.</p> required Source code in <code>src/anypinn/core/nn.py</code> <pre><code>class Field(nn.Module):\n    \"\"\"\n    A neural field mapping coordinates -&gt; vector of state variables.\n    Example (ODE): t -&gt; [S, I, R].\n\n    Args:\n        config: Configuration for the MLP backing this field.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: MLPConfig,\n    ):\n        super().__init__()\n        encode = config.encode\n        if isinstance(encode, nn.Module):\n            # registers \u2192 participates in .to(), .state_dict()\n            self.encoder: nn.Module | None = encode\n        else:\n            self.encoder = None\n        self._encode_fn = encode  # callable reference (module or plain fn)\n        dims = [config.in_dim] + config.hidden_layers + [config.out_dim]\n        act = get_activation(config.activation)\n\n        layers: list[nn.Module] = []\n        for i in range(len(dims) - 1):\n            layers.append(nn.Linear(dims[i], dims[i + 1]))\n            if i &lt; len(dims) - 2:\n                layers.append(act)\n\n        if config.output_activation is not None:\n            out_act = get_activation(config.output_activation)\n            layers.append(out_act)\n\n        self.net = nn.Sequential(*layers)\n        self.apply(self._init)\n\n    @staticmethod\n    def _init(m: nn.Module) -&gt; None:\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            nn.init.zeros_(m.bias)\n\n    @override\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Forward pass of the field.\n\n        Args:\n            x: Input coordinates (e.g. time, space).\n\n        Returns:\n            The values of the field at input coordinates.\n        \"\"\"\n        if self._encode_fn is not None:\n            x = self._encode_fn(x)\n        return cast(Tensor, self.net(x))\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Field.encoder","title":"<code>encoder: nn.Module | None = encode</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.Field.net","title":"<code>net = nn.Sequential(*layers)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.Field.__init__","title":"<code>__init__(config: MLPConfig)</code>","text":"Source code in <code>src/anypinn/core/nn.py</code> <pre><code>def __init__(\n    self,\n    config: MLPConfig,\n):\n    super().__init__()\n    encode = config.encode\n    if isinstance(encode, nn.Module):\n        # registers \u2192 participates in .to(), .state_dict()\n        self.encoder: nn.Module | None = encode\n    else:\n        self.encoder = None\n    self._encode_fn = encode  # callable reference (module or plain fn)\n    dims = [config.in_dim] + config.hidden_layers + [config.out_dim]\n    act = get_activation(config.activation)\n\n    layers: list[nn.Module] = []\n    for i in range(len(dims) - 1):\n        layers.append(nn.Linear(dims[i], dims[i + 1]))\n        if i &lt; len(dims) - 2:\n            layers.append(act)\n\n    if config.output_activation is not None:\n        out_act = get_activation(config.output_activation)\n        layers.append(out_act)\n\n    self.net = nn.Sequential(*layers)\n    self.apply(self._init)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Field.forward","title":"<code>forward(x: Tensor) -&gt; Tensor</code>","text":"<p>Forward pass of the field.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input coordinates (e.g. time, space).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The values of the field at input coordinates.</p> Source code in <code>src/anypinn/core/nn.py</code> <pre><code>@override\ndef forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Forward pass of the field.\n\n    Args:\n        x: Input coordinates (e.g. time, space).\n\n    Returns:\n        The values of the field at input coordinates.\n    \"\"\"\n    if self._encode_fn is not None:\n        x = self._encode_fn(x)\n    return cast(Tensor, self.net(x))\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.FourierEncoding","title":"<code>FourierEncoding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Sinusoidal positional encoding for periodic or high-frequency signals.</p> <p>For input \\(\\mathbf{x} \\in \\mathbb{R}^{n \\times d}\\) and <code>num_frequencies</code> \\(K\\), the encoding is:</p> \\[ \\gamma(\\mathbf{x}) = [\\mathbf{x},\\,     \\sin(\\mathbf{x}),\\, \\cos(\\mathbf{x}),\\,     \\sin(2\\mathbf{x}),\\, \\cos(2\\mathbf{x}),\\,     \\ldots,\\,     \\sin(K\\mathbf{x}),\\, \\cos(K\\mathbf{x})] \\] <p>producing shape \\((n,\\, d\\,(1 + 2K))\\) when <code>include_input=True</code>, or \\((n,\\, 2dK)\\) when <code>include_input=False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>num_frequencies</code> <code>int</code> <p>Number of frequency bands \\(K \\geq 1\\).</p> <code>6</code> <code>include_input</code> <code>bool</code> <p>Prepend original coordinates to the encoded output.</p> <code>True</code> Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>class FourierEncoding(nn.Module):\n    \"\"\"Sinusoidal positional encoding for periodic or high-frequency signals.\n\n    For input $\\\\mathbf{x} \\\\in \\\\mathbb{R}^{n \\\\times d}$ and\n    `num_frequencies` $K$, the encoding is:\n\n    $$\n    \\\\gamma(\\\\mathbf{x}) = [\\\\mathbf{x},\\\\,\n        \\\\sin(\\\\mathbf{x}),\\\\, \\\\cos(\\\\mathbf{x}),\\\\,\n        \\\\sin(2\\\\mathbf{x}),\\\\, \\\\cos(2\\\\mathbf{x}),\\\\,\n        \\\\ldots,\\\\,\n        \\\\sin(K\\\\mathbf{x}),\\\\, \\\\cos(K\\\\mathbf{x})]\n    $$\n\n    producing shape $(n,\\\\, d\\\\,(1 + 2K))$ when `include_input=True`,\n    or $(n,\\\\, 2dK)$ when `include_input=False`.\n\n    Args:\n        num_frequencies: Number of frequency bands $K \\\\geq 1$.\n        include_input:   Prepend original coordinates to the encoded output.\n    \"\"\"\n\n    def __init__(self, num_frequencies: int = 6, include_input: bool = True) -&gt; None:\n        if num_frequencies &lt; 1:\n            raise ValueError(f\"num_frequencies must be &gt;= 1, got {num_frequencies}.\")\n        super().__init__()\n        self.num_frequencies = num_frequencies\n        self.include_input = include_input\n\n    def out_dim(self, in_dim: int) -&gt; int:\n        \"\"\"Compute output dimension given input dimension.\"\"\"\n        factor = 1 + 2 * self.num_frequencies if self.include_input else 2 * self.num_frequencies\n        return in_dim * factor\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        parts = [x] if self.include_input else []\n        for k in range(1, self.num_frequencies + 1):\n            parts.append(torch.sin(k * x))\n            parts.append(torch.cos(k * x))\n        return torch.cat(parts, dim=-1)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.FourierEncoding.include_input","title":"<code>include_input = include_input</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.FourierEncoding.num_frequencies","title":"<code>num_frequencies = num_frequencies</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.FourierEncoding.__init__","title":"<code>__init__(num_frequencies: int = 6, include_input: bool = True) -&gt; None</code>","text":"Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>def __init__(self, num_frequencies: int = 6, include_input: bool = True) -&gt; None:\n    if num_frequencies &lt; 1:\n        raise ValueError(f\"num_frequencies must be &gt;= 1, got {num_frequencies}.\")\n    super().__init__()\n    self.num_frequencies = num_frequencies\n    self.include_input = include_input\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.FourierEncoding.forward","title":"<code>forward(x: Tensor) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    parts = [x] if self.include_input else []\n    for k in range(1, self.num_frequencies + 1):\n        parts.append(torch.sin(k * x))\n        parts.append(torch.cos(k * x))\n    return torch.cat(parts, dim=-1)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.FourierEncoding.out_dim","title":"<code>out_dim(in_dim: int) -&gt; int</code>","text":"<p>Compute output dimension given input dimension.</p> Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>def out_dim(self, in_dim: int) -&gt; int:\n    \"\"\"Compute output dimension given input dimension.\"\"\"\n    factor = 1 + 2 * self.num_frequencies if self.include_input else 2 * self.num_frequencies\n    return in_dim * factor\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.GenerationConfig","title":"<code>GenerationConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrainingDataConfig</code></p> <p>Configuration for data generation.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass GenerationConfig(TrainingDataConfig):\n    \"\"\"\n    Configuration for data generation.\n    \"\"\"\n\n    x: Tensor\n    noise_level: float\n    args_to_train: ArgsRegistry\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.GenerationConfig.args_to_train","title":"<code>args_to_train: ArgsRegistry</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.GenerationConfig.noise_level","title":"<code>noise_level: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.GenerationConfig.x","title":"<code>x: Tensor</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.GenerationConfig.__init__","title":"<code>__init__(*, batch_size: int, data_ratio: int | float, collocations: int, collocation_sampler: CollocationStrategies = 'random', collocation_seed: int | None = None, x: Tensor, noise_level: float, args_to_train: ArgsRegistry) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.InferredContext","title":"<code>InferredContext</code>  <code>dataclass</code>","text":"<p>Runtime context inferred from training data.</p> <p>This holds the data that is either explicitly provided in props or inferred from training data.</p> Source code in <code>src/anypinn/core/context.py</code> <pre><code>@dataclass\nclass InferredContext:\n    \"\"\"\n    Runtime context inferred from training data.\n\n    This holds the data that is either explicitly provided in props or inferred from training data.\n    \"\"\"\n\n    def __init__(\n        self,\n        x: Tensor,\n        y: Tensor,\n        validation: ResolvedValidation,\n    ):\n        \"\"\"\n        Infer context from either generated or loaded data.\n\n        Args:\n            x: x coordinates.\n            y: observations.\n            validation: Resolved validation dictionary.\n        \"\"\"\n\n        self.domain = Domain.from_x(x)\n        self.validation = validation\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.InferredContext.domain","title":"<code>domain = Domain.from_x(x)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.InferredContext.validation","title":"<code>validation = validation</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.InferredContext.__init__","title":"<code>__init__(x: Tensor, y: Tensor, validation: ResolvedValidation)</code>","text":"<p>Infer context from either generated or loaded data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>x coordinates.</p> required <code>y</code> <code>Tensor</code> <p>observations.</p> required <code>validation</code> <code>ResolvedValidation</code> <p>Resolved validation dictionary.</p> required Source code in <code>src/anypinn/core/context.py</code> <pre><code>def __init__(\n    self,\n    x: Tensor,\n    y: Tensor,\n    validation: ResolvedValidation,\n):\n    \"\"\"\n    Infer context from either generated or loaded data.\n\n    Args:\n        x: x coordinates.\n        y: observations.\n        validation: Resolved validation dictionary.\n    \"\"\"\n\n    self.domain = Domain.from_x(x)\n    self.validation = validation\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.IngestionConfig","title":"<code>IngestionConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrainingDataConfig</code></p> <p>Configuration for data ingestion from files. If x_column is None, the data is assumed to be evenly spaced.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass IngestionConfig(TrainingDataConfig):\n    \"\"\"\n    Configuration for data ingestion from files.\n    If x_column is None, the data is assumed to be evenly spaced.\n    \"\"\"\n\n    df_path: Path\n    x_transform: Callable[[Any], Any] | None = None\n    x_column: str | None = None\n    y_columns: list[str]\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.IngestionConfig.df_path","title":"<code>df_path: Path</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.IngestionConfig.x_column","title":"<code>x_column: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.IngestionConfig.x_transform","title":"<code>x_transform: Callable[[Any], Any] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.IngestionConfig.y_columns","title":"<code>y_columns: list[str]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.IngestionConfig.__init__","title":"<code>__init__(*, batch_size: int, data_ratio: int | float, collocations: int, collocation_sampler: CollocationStrategies = 'random', collocation_seed: int | None = None, df_path: Path, x_transform: Callable[[Any], Any] | None = None, x_column: str | None = None, y_columns: list[str]) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.LBFGSConfig","title":"<code>LBFGSConfig</code>  <code>dataclass</code>","text":"<p>Configuration for the L-BFGS optimizer.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass LBFGSConfig:\n    \"\"\"\n    Configuration for the L-BFGS optimizer.\n    \"\"\"\n\n    lr: float = 1.0\n    max_iter: int = 20\n    max_eval: int | None = None\n    history_size: int = 100\n    line_search_fn: str | None = \"strong_wolfe\"\n\n    def __post_init__(self) -&gt; None:\n        if self.lr &lt;= 0:\n            raise ValueError(f\"lr must be positive, got {self.lr}.\")\n        if self.max_iter &lt;= 0:\n            raise ValueError(f\"max_iter must be positive, got {self.max_iter}.\")\n        if self.history_size &lt;= 0:\n            raise ValueError(f\"history_size must be positive, got {self.history_size}.\")\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.LBFGSConfig.history_size","title":"<code>history_size: int = 100</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.LBFGSConfig.line_search_fn","title":"<code>line_search_fn: str | None = 'strong_wolfe'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.LBFGSConfig.lr","title":"<code>lr: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.LBFGSConfig.max_eval","title":"<code>max_eval: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.LBFGSConfig.max_iter","title":"<code>max_iter: int = 20</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.LBFGSConfig.__init__","title":"<code>__init__(*, lr: float = 1.0, max_iter: int = 20, max_eval: int | None = None, history_size: int = 100, line_search_fn: str | None = 'strong_wolfe') -&gt; None</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.LBFGSConfig.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/core/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.lr &lt;= 0:\n        raise ValueError(f\"lr must be positive, got {self.lr}.\")\n    if self.max_iter &lt;= 0:\n        raise ValueError(f\"max_iter must be positive, got {self.max_iter}.\")\n    if self.history_size &lt;= 0:\n        raise ValueError(f\"history_size must be positive, got {self.history_size}.\")\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.LatinHypercubeSampler","title":"<code>LatinHypercubeSampler</code>","text":"<p>Latin Hypercube sampler (pure-PyTorch, no SciPy dependency).</p> <p>Stratifies each dimension into <code>n</code> equal intervals and places one sample per interval, then shuffles columns independently.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Optional seed for reproducible sampling.</p> <code>None</code> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>class LatinHypercubeSampler:\n    \"\"\"Latin Hypercube sampler (pure-PyTorch, no SciPy dependency).\n\n    Stratifies each dimension into ``n`` equal intervals and places one sample\n    per interval, then shuffles columns independently.\n\n    Args:\n        seed: Optional seed for reproducible sampling.\n    \"\"\"\n\n    def __init__(self, seed: int | None = None) -&gt; None:\n        self._gen = torch.Generator()\n        if seed is not None:\n            self._gen.manual_seed(seed)\n\n    def sample(self, n: int, domain: Domain) -&gt; Tensor:\n        d = domain.ndim\n        result = torch.empty(n, d)\n\n        for i, (lo, hi) in enumerate(domain.bounds):\n            perm = torch.randperm(n, generator=self._gen)\n            base = (perm.float() + torch.rand(n, generator=self._gen)) / n\n            result[:, i] = base * (hi - lo) + lo\n\n        return result\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.LatinHypercubeSampler.__init__","title":"<code>__init__(seed: int | None = None) -&gt; None</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def __init__(self, seed: int | None = None) -&gt; None:\n    self._gen = torch.Generator()\n    if seed is not None:\n        self._gen.manual_seed(seed)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.LatinHypercubeSampler.sample","title":"<code>sample(n: int, domain: Domain) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def sample(self, n: int, domain: Domain) -&gt; Tensor:\n    d = domain.ndim\n    result = torch.empty(n, d)\n\n    for i, (lo, hi) in enumerate(domain.bounds):\n        perm = torch.randperm(n, generator=self._gen)\n        base = (perm.float() + torch.rand(n, generator=self._gen)) / n\n        result[:, i] = base * (hi - lo) + lo\n\n    return result\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.LogFn","title":"<code>LogFn</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>A function that logs a value to a dictionary.</p> Source code in <code>src/anypinn/core/types.py</code> <pre><code>class LogFn(Protocol):\n    \"\"\"\n    A function that logs a value to a dictionary.\n    \"\"\"\n\n    def __call__(self, name: str, value: Tensor, progress_bar: bool = False) -&gt; None:\n        \"\"\"\n        Log a value.\n\n        Args:\n            name: The name to log the value under.\n            value: The value to log.\n            progress_bar: Whether the value should be logged to the progress bar.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.LogFn.__call__","title":"<code>__call__(name: str, value: Tensor, progress_bar: bool = False) -&gt; None</code>","text":"<p>Log a value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to log the value under.</p> required <code>value</code> <code>Tensor</code> <p>The value to log.</p> required <code>progress_bar</code> <code>bool</code> <p>Whether the value should be logged to the progress bar.</p> <code>False</code> Source code in <code>src/anypinn/core/types.py</code> <pre><code>def __call__(self, name: str, value: Tensor, progress_bar: bool = False) -&gt; None:\n    \"\"\"\n    Log a value.\n\n    Args:\n        name: The name to log the value under.\n        value: The value to log.\n        progress_bar: Whether the value should be logged to the progress bar.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.LogUniform1DSampler","title":"<code>LogUniform1DSampler</code>","text":"<p>Log-uniform sampler for 1-D domains (reproduces SIR collocation behavior).</p> <p>Samples uniformly in <code>log1p</code> space and maps back via <code>expm1</code>, producing a distribution that is denser near the lower bound \u2014 useful for epidemic models where early dynamics are most informative.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Optional seed for reproducible sampling.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the domain is not 1-D or <code>x0 &lt;= -1</code>.</p> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>class LogUniform1DSampler:\n    \"\"\"Log-uniform sampler for 1-D domains (reproduces SIR collocation behavior).\n\n    Samples uniformly in ``log1p`` space and maps back via ``expm1``, producing\n    a distribution that is denser near the lower bound \u2014 useful for epidemic\n    models where early dynamics are most informative.\n\n    Args:\n        seed: Optional seed for reproducible sampling.\n\n    Raises:\n        ValueError: If the domain is not 1-D or ``x0 &lt;= -1``.\n    \"\"\"\n\n    def __init__(self, seed: int | None = None) -&gt; None:\n        self._gen = torch.Generator()\n        if seed is not None:\n            self._gen.manual_seed(seed)\n\n    def sample(self, n: int, domain: Domain) -&gt; Tensor:\n        if domain.ndim != 1:\n            raise ValueError(\n                f\"log_uniform_1d sampler supports only 1-D domains, got ndim={domain.ndim}.\"\n            )\n        x0, x1 = domain.x0, domain.x1\n        if x0 &lt;= -1.0:\n            raise ValueError(f\"log_uniform_1d requires x0 &gt; -1 for log1p, got x0={x0}.\")\n        log_lo = torch.tensor(x0, dtype=torch.float32).log1p()\n        log_hi = torch.tensor(x1, dtype=torch.float32).log1p()\n        u = torch.rand((n, 1), generator=self._gen)\n        return torch.expm1(u * (log_hi - log_lo) + log_lo)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.LogUniform1DSampler.__init__","title":"<code>__init__(seed: int | None = None) -&gt; None</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def __init__(self, seed: int | None = None) -&gt; None:\n    self._gen = torch.Generator()\n    if seed is not None:\n        self._gen.manual_seed(seed)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.LogUniform1DSampler.sample","title":"<code>sample(n: int, domain: Domain) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def sample(self, n: int, domain: Domain) -&gt; Tensor:\n    if domain.ndim != 1:\n        raise ValueError(\n            f\"log_uniform_1d sampler supports only 1-D domains, got ndim={domain.ndim}.\"\n        )\n    x0, x1 = domain.x0, domain.x1\n    if x0 &lt;= -1.0:\n        raise ValueError(f\"log_uniform_1d requires x0 &gt; -1 for log1p, got x0={x0}.\")\n    log_lo = torch.tensor(x0, dtype=torch.float32).log1p()\n    log_hi = torch.tensor(x1, dtype=torch.float32).log1p()\n    u = torch.rand((n, 1), generator=self._gen)\n    return torch.expm1(u * (log_hi - log_lo) + log_lo)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.MLPConfig","title":"<code>MLPConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a Multi-Layer Perceptron (MLP).</p> <p>Attributes:</p> Name Type Description <code>in_dim</code> <code>int</code> <p>Dimension of input layer.</p> <code>out_dim</code> <code>int</code> <p>Dimension of output layer.</p> <code>hidden_layers</code> <code>list[int]</code> <p>List of dimensions for hidden layers.</p> <code>activation</code> <code>Activations</code> <p>Activation function to use between layers.</p> <code>output_activation</code> <code>Activations | None</code> <p>Optional activation function for the output layer.</p> <code>encode</code> <code>Callable[[Tensor], Tensor] | None</code> <p>Optional function to encode inputs before passing to MLP.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass MLPConfig:\n    \"\"\"\n    Configuration for a Multi-Layer Perceptron (MLP).\n\n    Attributes:\n        in_dim: Dimension of input layer.\n        out_dim: Dimension of output layer.\n        hidden_layers: List of dimensions for hidden layers.\n        activation: Activation function to use between layers.\n        output_activation: Optional activation function for the output layer.\n        encode: Optional function to encode inputs before passing to MLP.\n    \"\"\"\n\n    in_dim: int\n    out_dim: int\n    hidden_layers: list[int]\n    activation: Activations\n    output_activation: Activations | None = None\n    encode: Callable[[Tensor], Tensor] | None = None\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.MLPConfig.activation","title":"<code>activation: Activations</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.MLPConfig.encode","title":"<code>encode: Callable[[Tensor], Tensor] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.MLPConfig.hidden_layers","title":"<code>hidden_layers: list[int]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.MLPConfig.in_dim","title":"<code>in_dim: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.MLPConfig.out_dim","title":"<code>out_dim: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.MLPConfig.output_activation","title":"<code>output_activation: Activations | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.MLPConfig.__init__","title":"<code>__init__(*, in_dim: int, out_dim: int, hidden_layers: list[int], activation: Activations, output_activation: Activations | None = None, encode: Callable[[Tensor], Tensor] | None = None) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNDataModule","title":"<code>PINNDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code>, <code>ABC</code></p> <p>LightningDataModule for PINNs. Manages data and collocation datasets and creates the combined PINNDataset.</p> <p>Collocation points are generated via a <code>CollocationSampler</code> selected by the <code>collocation_sampler</code> field in <code>TrainingDataConfig</code> (string literal). Subclasses only need to implement <code>gen_data()</code>; collocation generation is handled by the sampler resolved from the hyperparameters.</p> <p>Attributes:</p> Name Type Description <code>pinn_ds</code> <p>Combined PINNDataset for training.</p> <code>callbacks</code> <code>list[DataCallback]</code> <p>Sequence of DataCallback callbacks applied after data loading.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>class PINNDataModule(pl.LightningDataModule, ABC):\n    \"\"\"\n    LightningDataModule for PINNs.\n    Manages data and collocation datasets and creates the combined PINNDataset.\n\n    Collocation points are generated via a ``CollocationSampler`` selected by the\n    ``collocation_sampler`` field in ``TrainingDataConfig`` (string literal).\n    Subclasses only need to implement ``gen_data()``; collocation generation is\n    handled by the sampler resolved from the hyperparameters.\n\n    Attributes:\n        pinn_ds: Combined PINNDataset for training.\n        callbacks: Sequence of DataCallback callbacks applied after data loading.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n        residual_scorer: ResidualScorer | None = None,\n    ) -&gt; None:\n        super().__init__()\n        self.hp = hp\n        self.callbacks: list[DataCallback] = list(callbacks) if callbacks else []\n        self._residual_scorer = residual_scorer\n\n        self._unresolved_validation = validation or {}\n        self._context: InferredContext | None = None\n\n    def _build_sampler(self, strategy: CollocationStrategies) -&gt; CollocationSampler:\n        \"\"\"Resolve a collocation sampler from a strategy name.\"\"\"\n        return build_sampler(\n            strategy=strategy,\n            seed=self.hp.training_data.collocation_seed,\n            scorer=self._residual_scorer,\n        )\n\n    def load_data(self, config: IngestionConfig) -&gt; DataBatch:\n        \"\"\"Load raw data from IngestionConfig.\"\"\"\n        df = pd.read_csv(config.df_path)\n\n        if config.x_column is not None:\n            x_values = df[config.x_column].values\n\n            if config.x_transform is not None:\n                x_values = config.x_transform(x_values)\n\n            x = torch.tensor(x_values, dtype=torch.float32)\n        else:\n            x = torch.arange(len(df), dtype=torch.float32)\n\n        y = torch.tensor(df[config.y_columns].values, dtype=torch.float32)\n\n        if y.ndim == 1:\n            y = y.unsqueeze(-1)  # (N,) \u2192 (N, 1)\n        y = y.unsqueeze(-1)  # (N, k) \u2192 (N, k, 1) always\n\n        return x.unsqueeze(-1), y\n\n    @abstractmethod\n    def gen_data(self, config: GenerationConfig) -&gt; DataBatch:\n        \"\"\"Generate synthetic data from GenerationConfig.\"\"\"\n\n    @override\n    def setup(self, stage: str | None = None) -&gt; None:\n        \"\"\"\n        Load raw data from IngestionConfig, or generate synthetic data from GenerationConfig.\n        Apply registered callbacks, create InferredContext and datasets.\n        \"\"\"\n        config = self.hp.training_data\n\n        self.validation = resolve_validation(\n            self._unresolved_validation,\n            config.df_path if isinstance(config, IngestionConfig) else None,\n        )\n\n        self.data = (\n            self.load_data(config)\n            if isinstance(config, IngestionConfig)\n            else self.gen_data(config)\n        )\n\n        domain = Domain.from_x(self.data[0])\n        self._domain = domain\n        self._sampler = self._build_sampler(config.collocation_sampler)\n        self.coll = self._sampler.sample(config.collocations, domain)\n\n        for callback in self.callbacks:\n            self.data, self.coll = callback.transform_data(self.data, self.coll)\n\n        x_data, y_data = self.data\n\n        if x_data.shape[0] != y_data.shape[0]:\n            raise ValueError(\n                f\"Size mismatch: x has {x_data.shape[0]} rows, y has {y_data.shape[0]} rows.\"\n            )\n        if x_data.ndim != 2 or x_data.shape[1] &lt; 1:\n            raise ValueError(f\"Expected x shape (n, d) with d &gt;= 1, got {tuple(x_data.shape)}.\")\n        if y_data.ndim &lt; 2 or y_data.shape[-1] != 1:\n            raise ValueError(f\"Expected y shape (n, ..., 1), got {tuple(y_data.shape)}.\")\n        if self.coll.ndim != 2 or self.coll.shape[1] &lt; 1:\n            raise ValueError(\n                f\"Expected coll shape (m, d) with d &gt;= 1, got {tuple(self.coll.shape)}.\"\n            )\n        if x_data.shape[1] != self.coll.shape[1]:\n            raise ValueError(\n                f\"Spatial dimension mismatch: x_data has d={x_data.shape[1]}, \"\n                f\"coll has d={self.coll.shape[1]}. Both must share the same number of dimensions.\"\n            )\n\n        self._data_size = x_data.shape[0]\n\n        self._context = InferredContext(\n            x_data,\n            y_data,\n            self.validation,\n        )\n\n        self.pinn_ds = PINNDataset(\n            x_data,\n            y_data,\n            self.coll,\n            config.batch_size,\n            config.data_ratio,\n        )\n\n        self.predict_ds = TensorDataset(\n            x_data,\n            y_data,\n        )\n\n        for callback in self.callbacks:\n            callback.on_after_setup(self)\n\n    @override\n    def train_dataloader(self) -&gt; DataLoader[TrainingBatch]:\n        \"\"\"\n        Returns the training dataloader using PINNDataset.\n        \"\"\"\n        return DataLoader[TrainingBatch](\n            self.pinn_ds,\n            batch_size=None,  # handled internally\n            num_workers=cpu_count() or 1,\n            persistent_workers=True,\n            pin_memory=True,\n        )\n\n    @override\n    def predict_dataloader(self) -&gt; DataLoader[PredictionBatch]:\n        \"\"\"\n        Returns the prediction dataloader using only the data dataset.\n        \"\"\"\n        return DataLoader[PredictionBatch](\n            cast(Dataset[PredictionBatch], self.predict_ds),\n            batch_size=self._data_size,\n            num_workers=cpu_count() or 1,\n            persistent_workers=True,\n            pin_memory=True,\n        )\n\n    @property\n    def context(self) -&gt; InferredContext:\n        if self._context is None:\n            raise RuntimeError(\"Context does not exist. Call setup() before accessing context.\")\n        return self._context\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.PINNDataModule.callbacks","title":"<code>callbacks: list[DataCallback] = list(callbacks) if callbacks else []</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNDataModule.context","title":"<code>context: InferredContext</code>  <code>property</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNDataModule.hp","title":"<code>hp = hp</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None, residual_scorer: ResidualScorer | None = None) -&gt; None</code>","text":"Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n    residual_scorer: ResidualScorer | None = None,\n) -&gt; None:\n    super().__init__()\n    self.hp = hp\n    self.callbacks: list[DataCallback] = list(callbacks) if callbacks else []\n    self._residual_scorer = residual_scorer\n\n    self._unresolved_validation = validation or {}\n    self._context: InferredContext | None = None\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.PINNDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; DataBatch</code>  <code>abstractmethod</code>","text":"<p>Generate synthetic data from GenerationConfig.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>@abstractmethod\ndef gen_data(self, config: GenerationConfig) -&gt; DataBatch:\n    \"\"\"Generate synthetic data from GenerationConfig.\"\"\"\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.PINNDataModule.load_data","title":"<code>load_data(config: IngestionConfig) -&gt; DataBatch</code>","text":"<p>Load raw data from IngestionConfig.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>def load_data(self, config: IngestionConfig) -&gt; DataBatch:\n    \"\"\"Load raw data from IngestionConfig.\"\"\"\n    df = pd.read_csv(config.df_path)\n\n    if config.x_column is not None:\n        x_values = df[config.x_column].values\n\n        if config.x_transform is not None:\n            x_values = config.x_transform(x_values)\n\n        x = torch.tensor(x_values, dtype=torch.float32)\n    else:\n        x = torch.arange(len(df), dtype=torch.float32)\n\n    y = torch.tensor(df[config.y_columns].values, dtype=torch.float32)\n\n    if y.ndim == 1:\n        y = y.unsqueeze(-1)  # (N,) \u2192 (N, 1)\n    y = y.unsqueeze(-1)  # (N, k) \u2192 (N, k, 1) always\n\n    return x.unsqueeze(-1), y\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.PINNDataModule.predict_dataloader","title":"<code>predict_dataloader() -&gt; DataLoader[PredictionBatch]</code>","text":"<p>Returns the prediction dataloader using only the data dataset.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>@override\ndef predict_dataloader(self) -&gt; DataLoader[PredictionBatch]:\n    \"\"\"\n    Returns the prediction dataloader using only the data dataset.\n    \"\"\"\n    return DataLoader[PredictionBatch](\n        cast(Dataset[PredictionBatch], self.predict_ds),\n        batch_size=self._data_size,\n        num_workers=cpu_count() or 1,\n        persistent_workers=True,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.PINNDataModule.setup","title":"<code>setup(stage: str | None = None) -&gt; None</code>","text":"<p>Load raw data from IngestionConfig, or generate synthetic data from GenerationConfig. Apply registered callbacks, create InferredContext and datasets.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>@override\ndef setup(self, stage: str | None = None) -&gt; None:\n    \"\"\"\n    Load raw data from IngestionConfig, or generate synthetic data from GenerationConfig.\n    Apply registered callbacks, create InferredContext and datasets.\n    \"\"\"\n    config = self.hp.training_data\n\n    self.validation = resolve_validation(\n        self._unresolved_validation,\n        config.df_path if isinstance(config, IngestionConfig) else None,\n    )\n\n    self.data = (\n        self.load_data(config)\n        if isinstance(config, IngestionConfig)\n        else self.gen_data(config)\n    )\n\n    domain = Domain.from_x(self.data[0])\n    self._domain = domain\n    self._sampler = self._build_sampler(config.collocation_sampler)\n    self.coll = self._sampler.sample(config.collocations, domain)\n\n    for callback in self.callbacks:\n        self.data, self.coll = callback.transform_data(self.data, self.coll)\n\n    x_data, y_data = self.data\n\n    if x_data.shape[0] != y_data.shape[0]:\n        raise ValueError(\n            f\"Size mismatch: x has {x_data.shape[0]} rows, y has {y_data.shape[0]} rows.\"\n        )\n    if x_data.ndim != 2 or x_data.shape[1] &lt; 1:\n        raise ValueError(f\"Expected x shape (n, d) with d &gt;= 1, got {tuple(x_data.shape)}.\")\n    if y_data.ndim &lt; 2 or y_data.shape[-1] != 1:\n        raise ValueError(f\"Expected y shape (n, ..., 1), got {tuple(y_data.shape)}.\")\n    if self.coll.ndim != 2 or self.coll.shape[1] &lt; 1:\n        raise ValueError(\n            f\"Expected coll shape (m, d) with d &gt;= 1, got {tuple(self.coll.shape)}.\"\n        )\n    if x_data.shape[1] != self.coll.shape[1]:\n        raise ValueError(\n            f\"Spatial dimension mismatch: x_data has d={x_data.shape[1]}, \"\n            f\"coll has d={self.coll.shape[1]}. Both must share the same number of dimensions.\"\n        )\n\n    self._data_size = x_data.shape[0]\n\n    self._context = InferredContext(\n        x_data,\n        y_data,\n        self.validation,\n    )\n\n    self.pinn_ds = PINNDataset(\n        x_data,\n        y_data,\n        self.coll,\n        config.batch_size,\n        config.data_ratio,\n    )\n\n    self.predict_ds = TensorDataset(\n        x_data,\n        y_data,\n    )\n\n    for callback in self.callbacks:\n        callback.on_after_setup(self)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.PINNDataModule.train_dataloader","title":"<code>train_dataloader() -&gt; DataLoader[TrainingBatch]</code>","text":"<p>Returns the training dataloader using PINNDataset.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>@override\ndef train_dataloader(self) -&gt; DataLoader[TrainingBatch]:\n    \"\"\"\n    Returns the training dataloader using PINNDataset.\n    \"\"\"\n    return DataLoader[TrainingBatch](\n        self.pinn_ds,\n        batch_size=None,  # handled internally\n        num_workers=cpu_count() or 1,\n        persistent_workers=True,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.PINNDataset","title":"<code>PINNDataset</code>","text":"<p>               Bases: <code>Dataset[TrainingBatch]</code></p> <p>Dataset used for PINN training. Combines labeled data and collocation points per sample.  Given a data_ratio, the amount of data points <code>K</code> is determined either by applying <code>data_ratio * batch_size</code> if ratio is a float between 0 and 1 or by an absolute count if ratio is an integer. The remaining <code>C</code> points are used for collocation.  The data points are sampled without replacement per epoch i.e. cycles through all data points and at the last batch, wraps around to the first indices to ensure batch size. The collocation points are sampled with replacement from the pool. The dataset produces a batch of shape ((x_data[K,d], y_data[K,...]), x_coll[C,d]).</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>Tensor</code> <p>Data point x coordinates (time values).</p> required <code>y_data</code> <code>Tensor</code> <p>Data point y values (observations).</p> required <code>x_coll</code> <code>Tensor</code> <p>Collocation point x coordinates.</p> required <code>batch_size</code> <code>int</code> <p>Size of the batch.</p> required <code>data_ratio</code> <code>float | int</code> <p>Ratio of data points to collocation points, either as a ratio [0,1] or absolute count [0,batch_size].</p> required Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>class PINNDataset(Dataset[TrainingBatch]):\n    \"\"\"\n    Dataset used for PINN training. Combines labeled data and collocation points\n    per sample.  Given a data_ratio, the amount of data points `K` is determined\n    either by applying `data_ratio * batch_size` if ratio is a float between 0\n    and 1 or by an absolute count if ratio is an integer. The remaining `C`\n    points are used for collocation.  The data points are sampled without\n    replacement per epoch i.e. cycles through all data points and at the last\n    batch, wraps around to the first indices to ensure batch size. The collocation\n    points are sampled with replacement from the pool.\n    The dataset produces a batch of shape ((x_data[K,d], y_data[K,...]), x_coll[C,d]).\n\n    Args:\n        x_data: Data point x coordinates (time values).\n        y_data: Data point y values (observations).\n        x_coll: Collocation point x coordinates.\n        batch_size: Size of the batch.\n        data_ratio: Ratio of data points to collocation points, either as a ratio [0,1] or absolute\n            count [0,batch_size].\n    \"\"\"\n\n    def __init__(\n        self,\n        x_data: Tensor,\n        y_data: Tensor,\n        x_coll: Tensor,\n        batch_size: int,\n        data_ratio: float | int,\n    ):\n        super().__init__()\n        if batch_size &lt;= 0:\n            raise ValueError(f\"batch_size must be positive, got {batch_size}.\")\n\n        if isinstance(data_ratio, float):\n            if not (0.0 &lt;= data_ratio &lt;= 1.0):\n                raise ValueError(f\"Float data_ratio must be in [0.0, 1.0], got {data_ratio}.\")\n            self.K = round(data_ratio * batch_size)\n        else:\n            if not (0 &lt;= data_ratio &lt;= batch_size):\n                raise ValueError(\n                    f\"Integer data_ratio must be in [0, {batch_size}], got {data_ratio}.\"\n                )\n            self.K = data_ratio\n\n        self.x_data = x_data\n        self.y_data = y_data\n        self.x_coll = x_coll\n\n        self.batch_size = batch_size\n        self.C = batch_size - self.K\n\n        self.total_data = x_data.shape[0]\n        self.total_coll = x_coll.shape[0]\n\n        self._coll_gen = torch.Generator()\n\n    def __len__(self) -&gt; int:\n        \"\"\"Number of steps per epoch to see all data points once. Ceiling division.\"\"\"\n        return (self.total_data + self.K - 1) // self.K\n\n    @override\n    def __getitem__(self, index: int) -&gt; TrainingBatch:\n        \"\"\"Return one sample containing K data points and C collocation points.\"\"\"\n        data_idx = self._get_data_indices(index)\n        coll_idx = self._get_coll_indices(index)\n\n        x_data = self.x_data[data_idx]\n        y_data = self.y_data[data_idx]\n        x_coll = self.x_coll[coll_idx]\n\n        return ((x_data, y_data), x_coll)\n\n    def _get_data_indices(self, idx: int) -&gt; Tensor:\n        \"\"\"Get data indices for this step without replacement.\n        When getting the last batch, wrap around to the first indices to ensure batch size.\n        \"\"\"\n        if self.total_data == 0:\n            return torch.empty(0, 1)\n\n        start = idx * self.K\n        indices = [(start + i) % self.total_data for i in range(self.K)]\n        return torch.tensor(indices)\n\n    def _get_coll_indices(self, idx: int) -&gt; Tensor:\n        \"\"\"Get collocation indices for this step with replacement.\"\"\"\n        if self.total_coll == 0:\n            return torch.empty(0, 1)\n\n        self._coll_gen.manual_seed(idx)\n        return torch.randint(0, self.total_coll, (self.C,), generator=self._coll_gen)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.PINNDataset.C","title":"<code>C = batch_size - self.K</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNDataset.K","title":"<code>K = round(data_ratio * batch_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNDataset.batch_size","title":"<code>batch_size = batch_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNDataset.total_coll","title":"<code>total_coll = x_coll.shape[0]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNDataset.total_data","title":"<code>total_data = x_data.shape[0]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNDataset.x_coll","title":"<code>x_coll = x_coll</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNDataset.x_data","title":"<code>x_data = x_data</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNDataset.y_data","title":"<code>y_data = y_data</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNDataset.__getitem__","title":"<code>__getitem__(index: int) -&gt; TrainingBatch</code>","text":"<p>Return one sample containing K data points and C collocation points.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>@override\ndef __getitem__(self, index: int) -&gt; TrainingBatch:\n    \"\"\"Return one sample containing K data points and C collocation points.\"\"\"\n    data_idx = self._get_data_indices(index)\n    coll_idx = self._get_coll_indices(index)\n\n    x_data = self.x_data[data_idx]\n    y_data = self.y_data[data_idx]\n    x_coll = self.x_coll[coll_idx]\n\n    return ((x_data, y_data), x_coll)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.PINNDataset.__init__","title":"<code>__init__(x_data: Tensor, y_data: Tensor, x_coll: Tensor, batch_size: int, data_ratio: float | int)</code>","text":"Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>def __init__(\n    self,\n    x_data: Tensor,\n    y_data: Tensor,\n    x_coll: Tensor,\n    batch_size: int,\n    data_ratio: float | int,\n):\n    super().__init__()\n    if batch_size &lt;= 0:\n        raise ValueError(f\"batch_size must be positive, got {batch_size}.\")\n\n    if isinstance(data_ratio, float):\n        if not (0.0 &lt;= data_ratio &lt;= 1.0):\n            raise ValueError(f\"Float data_ratio must be in [0.0, 1.0], got {data_ratio}.\")\n        self.K = round(data_ratio * batch_size)\n    else:\n        if not (0 &lt;= data_ratio &lt;= batch_size):\n            raise ValueError(\n                f\"Integer data_ratio must be in [0, {batch_size}], got {data_ratio}.\"\n            )\n        self.K = data_ratio\n\n    self.x_data = x_data\n    self.y_data = y_data\n    self.x_coll = x_coll\n\n    self.batch_size = batch_size\n    self.C = batch_size - self.K\n\n    self.total_data = x_data.shape[0]\n    self.total_coll = x_coll.shape[0]\n\n    self._coll_gen = torch.Generator()\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.PINNDataset.__len__","title":"<code>__len__() -&gt; int</code>","text":"<p>Number of steps per epoch to see all data points once. Ceiling division.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Number of steps per epoch to see all data points once. Ceiling division.\"\"\"\n    return (self.total_data + self.K - 1) // self.K\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.PINNHyperparameters","title":"<code>PINNHyperparameters</code>  <code>dataclass</code>","text":"<p>Aggregated hyperparameters for the PINN model.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass PINNHyperparameters:\n    \"\"\"\n    Aggregated hyperparameters for the PINN model.\n    \"\"\"\n\n    lr: float\n    training_data: IngestionConfig | GenerationConfig\n    fields_config: MLPConfig\n    params_config: MLPConfig | ScalarConfig\n    max_epochs: int | None = None\n    gradient_clip_val: float | None = None\n    criterion: Criteria = \"mse\"\n    optimizer: AdamConfig | LBFGSConfig | None = None\n    scheduler: ReduceLROnPlateauConfig | CosineAnnealingConfig | None = None\n    early_stopping: EarlyStoppingConfig | None = None\n    smma_stopping: SMMAStoppingConfig | None = None\n\n    def __post_init__(self) -&gt; None:\n        if self.lr &lt;= 0:\n            raise ValueError(f\"lr must be positive, got {self.lr}.\")\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.PINNHyperparameters.criterion","title":"<code>criterion: Criteria = 'mse'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNHyperparameters.early_stopping","title":"<code>early_stopping: EarlyStoppingConfig | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNHyperparameters.fields_config","title":"<code>fields_config: MLPConfig</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNHyperparameters.gradient_clip_val","title":"<code>gradient_clip_val: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNHyperparameters.lr","title":"<code>lr: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNHyperparameters.max_epochs","title":"<code>max_epochs: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNHyperparameters.optimizer","title":"<code>optimizer: AdamConfig | LBFGSConfig | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNHyperparameters.params_config","title":"<code>params_config: MLPConfig | ScalarConfig</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNHyperparameters.scheduler","title":"<code>scheduler: ReduceLROnPlateauConfig | CosineAnnealingConfig | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNHyperparameters.smma_stopping","title":"<code>smma_stopping: SMMAStoppingConfig | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNHyperparameters.training_data","title":"<code>training_data: IngestionConfig | GenerationConfig</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNHyperparameters.__init__","title":"<code>__init__(*, lr: float, training_data: IngestionConfig | GenerationConfig, fields_config: MLPConfig, params_config: MLPConfig | ScalarConfig, max_epochs: int | None = None, gradient_clip_val: float | None = None, criterion: Criteria = 'mse', optimizer: AdamConfig | LBFGSConfig | None = None, scheduler: ReduceLROnPlateauConfig | CosineAnnealingConfig | None = None, early_stopping: EarlyStoppingConfig | None = None, smma_stopping: SMMAStoppingConfig | None = None) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.PINNHyperparameters.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/core/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.lr &lt;= 0:\n        raise ValueError(f\"lr must be positive, got {self.lr}.\")\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Parameter","title":"<code>Parameter</code>","text":"<p>               Bases: <code>Module</code>, <code>Argument</code></p> <p>Learnable parameter. Supports scalar or function-valued parameter. For function-valued parameters (e.g. \u03b2(t)), uses a small MLP.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ScalarConfig | MLPConfig</code> <p>Configuration for the parameter (ScalarConfig or MLPConfig).</p> required Source code in <code>src/anypinn/core/nn.py</code> <pre><code>class Parameter(nn.Module, Argument):\n    \"\"\"\n    Learnable parameter. Supports scalar or function-valued parameter.\n    For function-valued parameters (e.g. \u03b2(t)), uses a small MLP.\n\n    Args:\n        config: Configuration for the parameter (ScalarConfig or MLPConfig).\n    \"\"\"\n\n    def __init__(\n        self,\n        config: ScalarConfig | MLPConfig,\n    ):\n        super().__init__()\n        self.config = config\n        self._mode: Literal[\"scalar\", \"mlp\"]\n\n        if isinstance(config, ScalarConfig):\n            self._mode = \"scalar\"\n            self.value = nn.Parameter(torch.tensor(float(config.init_value), dtype=torch.float32))\n\n        else:  # isinstance(config, MLPConfig)\n            self._mode = \"mlp\"\n            dims = [config.in_dim] + config.hidden_layers + [config.out_dim]\n            act = get_activation(config.activation)\n\n            layers: list[nn.Module] = []\n            for i in range(len(dims) - 1):\n                layers.append(nn.Linear(dims[i], dims[i + 1]))\n                if i &lt; len(dims) - 2:\n                    layers.append(act)\n\n            if config.output_activation is not None:\n                out_act = get_activation(config.output_activation)\n                layers.append(out_act)\n\n            self.net = nn.Sequential(*layers)\n            self.apply(self._init)\n\n    @property\n    def mode(self) -&gt; Literal[\"scalar\", \"mlp\"]:\n        \"\"\"Mode of the parameter: 'scalar' or 'mlp'.\"\"\"\n        return self._mode\n\n    @staticmethod\n    def _init(m: nn.Module) -&gt; None:\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            nn.init.zeros_(m.bias)\n\n    @override\n    def forward(self, x: Tensor | None = None) -&gt; Tensor:\n        \"\"\"\n        Get the value of the parameter.\n\n        Args:\n            x: Input tensor (required for 'mlp' mode).\n\n        Returns:\n            The parameter value.\n        \"\"\"\n        if self.mode == \"scalar\":\n            return self.value if x is None else self.value.expand_as(x)\n        else:\n            if x is None:\n                raise TypeError(\"Function-valued parameter requires input.\")\n            return cast(Tensor, self.net(x))\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Parameter.config","title":"<code>config = config</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.Parameter.mode","title":"<code>mode: Literal['scalar', 'mlp']</code>  <code>property</code>","text":"<p>Mode of the parameter: 'scalar' or 'mlp'.</p>"},{"location":"reference/anypinn/core/#anypinn.core.Parameter.net","title":"<code>net = nn.Sequential(*layers)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.Parameter.value","title":"<code>value = nn.Parameter(torch.tensor(float(config.init_value), dtype=(torch.float32)))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.Parameter.__init__","title":"<code>__init__(config: ScalarConfig | MLPConfig)</code>","text":"Source code in <code>src/anypinn/core/nn.py</code> <pre><code>def __init__(\n    self,\n    config: ScalarConfig | MLPConfig,\n):\n    super().__init__()\n    self.config = config\n    self._mode: Literal[\"scalar\", \"mlp\"]\n\n    if isinstance(config, ScalarConfig):\n        self._mode = \"scalar\"\n        self.value = nn.Parameter(torch.tensor(float(config.init_value), dtype=torch.float32))\n\n    else:  # isinstance(config, MLPConfig)\n        self._mode = \"mlp\"\n        dims = [config.in_dim] + config.hidden_layers + [config.out_dim]\n        act = get_activation(config.activation)\n\n        layers: list[nn.Module] = []\n        for i in range(len(dims) - 1):\n            layers.append(nn.Linear(dims[i], dims[i + 1]))\n            if i &lt; len(dims) - 2:\n                layers.append(act)\n\n        if config.output_activation is not None:\n            out_act = get_activation(config.output_activation)\n            layers.append(out_act)\n\n        self.net = nn.Sequential(*layers)\n        self.apply(self._init)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Parameter.forward","title":"<code>forward(x: Tensor | None = None) -&gt; Tensor</code>","text":"<p>Get the value of the parameter.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor | None</code> <p>Input tensor (required for 'mlp' mode).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The parameter value.</p> Source code in <code>src/anypinn/core/nn.py</code> <pre><code>@override\ndef forward(self, x: Tensor | None = None) -&gt; Tensor:\n    \"\"\"\n    Get the value of the parameter.\n\n    Args:\n        x: Input tensor (required for 'mlp' mode).\n\n    Returns:\n        The parameter value.\n    \"\"\"\n    if self.mode == \"scalar\":\n        return self.value if x is None else self.value.expand_as(x)\n    else:\n        if x is None:\n            raise TypeError(\"Function-valued parameter requires input.\")\n        return cast(Tensor, self.net(x))\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Problem","title":"<code>Problem</code>","text":"<p>               Bases: <code>Module</code></p> <p>Aggregates operator residuals and constraints into total loss. Manages fields, parameters, constraints, and validation.</p> <p>Parameters:</p> Name Type Description Default <code>constraints</code> <code>list[Constraint]</code> <p>List of constraints to enforce.</p> required <code>criterion</code> <code>Module</code> <p>Loss function module.</p> required <code>fields</code> <code>FieldsRegistry</code> <p>List of fields (neural networks) to solve for.</p> required <code>params</code> <code>ParamsRegistry</code> <p>List of learnable parameters.</p> required Source code in <code>src/anypinn/core/problem.py</code> <pre><code>class Problem(nn.Module):\n    \"\"\"\n    Aggregates operator residuals and constraints into total loss.\n    Manages fields, parameters, constraints, and validation.\n\n    Args:\n        constraints: List of constraints to enforce.\n        criterion: Loss function module.\n        fields: List of fields (neural networks) to solve for.\n        params: List of learnable parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        constraints: list[Constraint],\n        criterion: nn.Module,\n        fields: FieldsRegistry,\n        params: ParamsRegistry,\n    ):\n        super().__init__()\n        self.constraints = constraints\n        self.criterion = criterion\n        self.fields = fields\n        self.params = params\n\n        self._fields = nn.ModuleList(fields.values())\n        self._params = nn.ModuleList(params.values())\n\n    def inject_context(self, context: InferredContext) -&gt; None:\n        \"\"\"\n        Inject the context into the problem.\n\n        This should be called after data is loaded but before training starts.\n        Pure function entries are passed through unchanged.\n\n        Args:\n            context: The context to inject.\n        \"\"\"\n        self.context = context\n        for c in self.constraints:\n            c.inject_context(context)\n\n    def training_loss(self, batch: TrainingBatch, log: LogFn | None = None) -&gt; Tensor:\n        \"\"\"\n        Calculate the total loss from all constraints.\n\n        Args:\n            batch: Current batch.\n            log: Optional logging function.\n\n        Returns:\n            Sum of losses from all constraints.\n        \"\"\"\n        _, x_coll = batch\n\n        if not self.constraints:\n            total = torch.tensor(0.0, device=x_coll.device)\n        else:\n            losses = iter(self.constraints)\n            total = next(losses).loss(batch, self.criterion, log)\n            for c in losses:\n                total = total + c.loss(batch, self.criterion, log)\n\n        if log is not None:\n            for name, param in self.params.items():\n                param_loss = self._param_validation_loss(name, param, x_coll)\n                if param_loss is not None:\n                    log(f\"loss/{name}\", param_loss, progress_bar=True)\n\n            log(LOSS_KEY, total, progress_bar=True)\n\n        return total\n\n    def predict(self, batch: DataBatch) -&gt; tuple[DataBatch, dict[str, Tensor]]:\n        \"\"\"\n        Generate predictions for a given batch of data.\n        Returns unscaled predictions in original domain.\n\n        Args:\n            batch: Batch of input coordinates.\n\n        Returns:\n            Tuple of (original_batch, predictions_dict).\n        \"\"\"\n\n        x, y = batch\n\n        n = x.shape[0]\n        preds = {name: f(x).reshape(n, -1).squeeze(-1) for name, f in self.fields.items()}\n        preds |= {name: p(x).reshape(n, -1).squeeze(-1) for name, p in self.params.items()}\n\n        return (x.squeeze(-1), y.squeeze(-1)), preds\n\n    def true_values(self, x: Tensor) -&gt; dict[str, Tensor] | None:\n        \"\"\"\n        Get the true values for a given x coordinates.\n        Returns None if no validation source is configured.\n        \"\"\"\n\n        return {\n            name: p_true.reshape(x.shape[0], -1).squeeze(-1)\n            for name, p in self.params.items()\n            if (p_true := self._get_true_param(name, x)) is not None\n        } or None\n\n    def _get_true_param(self, param_name: str, x: Tensor) -&gt; Tensor | None:\n        \"\"\"\n        Get the ground truth values for a parameter at given coordinates.\n\n        Args:\n            param_name: Name of the parameter.\n            x: Input coordinates.\n\n        Returns:\n            Ground truth values, or None if no validation source is configured.\n        \"\"\"\n        if param_name not in self.context.validation:\n            return None\n\n        fn = self.context.validation[param_name]\n\n        if isinstance(fn, _ColumnLookup):\n            domain = self.context.domain\n            if domain.dx is None:\n                raise ValueError(\n                    f\"Cannot perform ColumnRef lookup for '{param_name}': \"\n                    \"domain step size (dx) is unknown. Ensure the domain was inferred from \"\n                    \"a uniformly-spaced coordinate tensor, or use a callable validation source.\"\n                )\n            x_idx = ((x.squeeze(-1) - domain.x0) / domain.dx[0]).round().unsqueeze(-1)\n            return fn(x_idx)\n\n        return fn(x)\n\n    @torch.no_grad()\n    def _param_validation_loss(\n        self, param_name: str, param: Parameter, x_coll: Tensor\n    ) -&gt; Tensor | None:\n        \"\"\"\n        Compute validation loss for a parameter against ground truth.\n\n        Args:\n            param: The parameter to compute validation loss for.\n            x_coll: The input coordinates.\n\n        Returns:\n            Loss value, or None if no validation source is configured.\n        \"\"\"\n        true = self._get_true_param(param_name, x_coll)\n        if true is None:\n            return None\n\n        pred = param(x_coll)\n\n        return torch.mean((true - pred) ** 2)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Problem.constraints","title":"<code>constraints = constraints</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.Problem.criterion","title":"<code>criterion = criterion</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.Problem.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.Problem.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.Problem.__init__","title":"<code>__init__(constraints: list[Constraint], criterion: nn.Module, fields: FieldsRegistry, params: ParamsRegistry)</code>","text":"Source code in <code>src/anypinn/core/problem.py</code> <pre><code>def __init__(\n    self,\n    constraints: list[Constraint],\n    criterion: nn.Module,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n):\n    super().__init__()\n    self.constraints = constraints\n    self.criterion = criterion\n    self.fields = fields\n    self.params = params\n\n    self._fields = nn.ModuleList(fields.values())\n    self._params = nn.ModuleList(params.values())\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Problem.inject_context","title":"<code>inject_context(context: InferredContext) -&gt; None</code>","text":"<p>Inject the context into the problem.</p> <p>This should be called after data is loaded but before training starts. Pure function entries are passed through unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>InferredContext</code> <p>The context to inject.</p> required Source code in <code>src/anypinn/core/problem.py</code> <pre><code>def inject_context(self, context: InferredContext) -&gt; None:\n    \"\"\"\n    Inject the context into the problem.\n\n    This should be called after data is loaded but before training starts.\n    Pure function entries are passed through unchanged.\n\n    Args:\n        context: The context to inject.\n    \"\"\"\n    self.context = context\n    for c in self.constraints:\n        c.inject_context(context)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Problem.predict","title":"<code>predict(batch: DataBatch) -&gt; tuple[DataBatch, dict[str, Tensor]]</code>","text":"<p>Generate predictions for a given batch of data. Returns unscaled predictions in original domain.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>DataBatch</code> <p>Batch of input coordinates.</p> required <p>Returns:</p> Type Description <code>tuple[DataBatch, dict[str, Tensor]]</code> <p>Tuple of (original_batch, predictions_dict).</p> Source code in <code>src/anypinn/core/problem.py</code> <pre><code>def predict(self, batch: DataBatch) -&gt; tuple[DataBatch, dict[str, Tensor]]:\n    \"\"\"\n    Generate predictions for a given batch of data.\n    Returns unscaled predictions in original domain.\n\n    Args:\n        batch: Batch of input coordinates.\n\n    Returns:\n        Tuple of (original_batch, predictions_dict).\n    \"\"\"\n\n    x, y = batch\n\n    n = x.shape[0]\n    preds = {name: f(x).reshape(n, -1).squeeze(-1) for name, f in self.fields.items()}\n    preds |= {name: p(x).reshape(n, -1).squeeze(-1) for name, p in self.params.items()}\n\n    return (x.squeeze(-1), y.squeeze(-1)), preds\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Problem.training_loss","title":"<code>training_loss(batch: TrainingBatch, log: LogFn | None = None) -&gt; Tensor</code>","text":"<p>Calculate the total loss from all constraints.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>TrainingBatch</code> <p>Current batch.</p> required <code>log</code> <code>LogFn | None</code> <p>Optional logging function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Sum of losses from all constraints.</p> Source code in <code>src/anypinn/core/problem.py</code> <pre><code>def training_loss(self, batch: TrainingBatch, log: LogFn | None = None) -&gt; Tensor:\n    \"\"\"\n    Calculate the total loss from all constraints.\n\n    Args:\n        batch: Current batch.\n        log: Optional logging function.\n\n    Returns:\n        Sum of losses from all constraints.\n    \"\"\"\n    _, x_coll = batch\n\n    if not self.constraints:\n        total = torch.tensor(0.0, device=x_coll.device)\n    else:\n        losses = iter(self.constraints)\n        total = next(losses).loss(batch, self.criterion, log)\n        for c in losses:\n            total = total + c.loss(batch, self.criterion, log)\n\n    if log is not None:\n        for name, param in self.params.items():\n            param_loss = self._param_validation_loss(name, param, x_coll)\n            if param_loss is not None:\n                log(f\"loss/{name}\", param_loss, progress_bar=True)\n\n        log(LOSS_KEY, total, progress_bar=True)\n\n    return total\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.Problem.true_values","title":"<code>true_values(x: Tensor) -&gt; dict[str, Tensor] | None</code>","text":"<p>Get the true values for a given x coordinates. Returns None if no validation source is configured.</p> Source code in <code>src/anypinn/core/problem.py</code> <pre><code>def true_values(self, x: Tensor) -&gt; dict[str, Tensor] | None:\n    \"\"\"\n    Get the true values for a given x coordinates.\n    Returns None if no validation source is configured.\n    \"\"\"\n\n    return {\n        name: p_true.reshape(x.shape[0], -1).squeeze(-1)\n        for name, p in self.params.items()\n        if (p_true := self._get_true_param(name, x)) is not None\n    } or None\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.RandomFourierFeatures","title":"<code>RandomFourierFeatures</code>","text":"<p>               Bases: <code>Module</code></p> <p>Random Fourier Features (Rahimi &amp; Recht, 2007) for RBF kernel approximation.</p> <p>Draws a fixed random matrix \\(\\mathbf{B} \\sim \\mathcal{N}(0, \\sigma^2)\\) of shape \\((d_{\\text{in}},\\, m)\\) and maps \\(\\mathbf{x} \\in \\mathbb{R}^{n \\times d_{\\text{in}}}\\) to:</p> \\[ \\phi(\\mathbf{x}) = \\frac{1}{\\sqrt{m}}     [\\cos(\\mathbf{x}\\mathbf{B}),\\; \\sin(\\mathbf{x}\\mathbf{B})]     \\in \\mathbb{R}^{n \\times 2m} \\] <p>\\(\\mathbf{B}\\) is registered as a buffer and moves with the module across devices.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Spatial dimension \\(d_{\\text{in}}\\) of the input.</p> required <code>num_features</code> <code>int</code> <p>Number of random features \\(m\\)           (output dimension \\(= 2m\\)).</p> <code>256</code> <code>scale</code> <code>float</code> <p>Standard deviation \\(\\sigma\\) of the frequency distribution.           Higher values capture higher-frequency variation. Default: 1.0.</p> <code>1.0</code> <code>seed</code> <code>int | None</code> <p>Optional seed for reproducible frequency sampling.</p> <code>None</code> Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>class RandomFourierFeatures(nn.Module):\n    \"\"\"Random Fourier Features (Rahimi &amp; Recht, 2007) for RBF kernel approximation.\n\n    Draws a fixed random matrix $\\\\mathbf{B} \\\\sim \\\\mathcal{N}(0, \\\\sigma^2)$\n    of shape $(d_{\\\\text{in}},\\\\, m)$ and maps\n    $\\\\mathbf{x} \\\\in \\\\mathbb{R}^{n \\\\times d_{\\\\text{in}}}$ to:\n\n    $$\n    \\\\phi(\\\\mathbf{x}) = \\\\frac{1}{\\\\sqrt{m}}\n        [\\\\cos(\\\\mathbf{x}\\\\mathbf{B}),\\\\; \\\\sin(\\\\mathbf{x}\\\\mathbf{B})]\n        \\\\in \\\\mathbb{R}^{n \\\\times 2m}\n    $$\n\n    $\\\\mathbf{B}$ is registered as a buffer and moves with the module across devices.\n\n    Args:\n        in_dim:       Spatial dimension $d_{\\\\text{in}}$ of the input.\n        num_features: Number of random features $m$\n                      (output dimension $= 2m$).\n        scale:        Standard deviation $\\\\sigma$ of the frequency distribution.\n                      Higher values capture higher-frequency variation. Default: 1.0.\n        seed:         Optional seed for reproducible frequency sampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_dim: int,\n        num_features: int = 256,\n        scale: float = 1.0,\n        seed: int | None = None,\n    ) -&gt; None:\n        if in_dim &lt; 1:\n            raise ValueError(f\"in_dim must be &gt;= 1, got {in_dim}.\")\n        if num_features &lt; 1:\n            raise ValueError(f\"num_features must be &gt;= 1, got {num_features}.\")\n        if scale &lt;= 0.0:\n            raise ValueError(f\"scale must be &gt; 0, got {scale}.\")\n        super().__init__()\n        gen = torch.Generator()\n        if seed is not None:\n            gen.manual_seed(seed)\n        B = torch.randn(in_dim, num_features, generator=gen) * scale\n        self.register_buffer(\"B\", B)\n        self.num_features = num_features\n\n    @property\n    def out_dim(self) -&gt; int:\n        \"\"\"Output dimension (always 2 * num_features).\"\"\"\n        return 2 * self.num_features\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        proj = x @ self.B  # type: ignore[operator]  # (n, num_features)\n        return torch.cat([torch.cos(proj), torch.sin(proj)], dim=-1) / (self.num_features**0.5)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.RandomFourierFeatures.num_features","title":"<code>num_features = num_features</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.RandomFourierFeatures.out_dim","title":"<code>out_dim: int</code>  <code>property</code>","text":"<p>Output dimension (always 2 * num_features).</p>"},{"location":"reference/anypinn/core/#anypinn.core.RandomFourierFeatures.__init__","title":"<code>__init__(in_dim: int, num_features: int = 256, scale: float = 1.0, seed: int | None = None) -&gt; None</code>","text":"Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>def __init__(\n    self,\n    in_dim: int,\n    num_features: int = 256,\n    scale: float = 1.0,\n    seed: int | None = None,\n) -&gt; None:\n    if in_dim &lt; 1:\n        raise ValueError(f\"in_dim must be &gt;= 1, got {in_dim}.\")\n    if num_features &lt; 1:\n        raise ValueError(f\"num_features must be &gt;= 1, got {num_features}.\")\n    if scale &lt;= 0.0:\n        raise ValueError(f\"scale must be &gt; 0, got {scale}.\")\n    super().__init__()\n    gen = torch.Generator()\n    if seed is not None:\n        gen.manual_seed(seed)\n    B = torch.randn(in_dim, num_features, generator=gen) * scale\n    self.register_buffer(\"B\", B)\n    self.num_features = num_features\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.RandomFourierFeatures.forward","title":"<code>forward(x: Tensor) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    proj = x @ self.B  # type: ignore[operator]  # (n, num_features)\n    return torch.cat([torch.cos(proj), torch.sin(proj)], dim=-1) / (self.num_features**0.5)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.RandomSampler","title":"<code>RandomSampler</code>","text":"<p>Uniform random sampler inside domain bounds.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Optional seed for reproducible sampling.</p> <code>None</code> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>class RandomSampler:\n    \"\"\"Uniform random sampler inside domain bounds.\n\n    Args:\n        seed: Optional seed for reproducible sampling.\n    \"\"\"\n\n    def __init__(self, seed: int | None = None) -&gt; None:\n        self._gen = torch.Generator()\n        if seed is not None:\n            self._gen.manual_seed(seed)\n\n    def sample(self, n: int, domain: Domain) -&gt; Tensor:\n        d = domain.ndim\n        u = torch.rand((n, d), generator=self._gen)\n        for i, (lo, hi) in enumerate(domain.bounds):\n            u[:, i] = u[:, i] * (hi - lo) + lo\n        return u\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.RandomSampler.__init__","title":"<code>__init__(seed: int | None = None) -&gt; None</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def __init__(self, seed: int | None = None) -&gt; None:\n    self._gen = torch.Generator()\n    if seed is not None:\n        self._gen.manual_seed(seed)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.RandomSampler.sample","title":"<code>sample(n: int, domain: Domain) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def sample(self, n: int, domain: Domain) -&gt; Tensor:\n    d = domain.ndim\n    u = torch.rand((n, d), generator=self._gen)\n    for i, (lo, hi) in enumerate(domain.bounds):\n        u[:, i] = u[:, i] * (hi - lo) + lo\n    return u\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.ReduceLROnPlateauConfig","title":"<code>ReduceLROnPlateauConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Learning Rate Scheduler (ReduceLROnPlateau).</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass ReduceLROnPlateauConfig:\n    \"\"\"\n    Configuration for Learning Rate Scheduler (ReduceLROnPlateau).\n    \"\"\"\n\n    mode: Literal[\"min\", \"max\"]\n    factor: float\n    patience: int\n    threshold: float\n    min_lr: float\n\n    def __post_init__(self) -&gt; None:\n        if not (0 &lt; self.factor &lt; 1):\n            raise ValueError(f\"factor must be in (0, 1), got {self.factor}.\")\n        if self.patience &lt;= 0:\n            raise ValueError(f\"patience must be positive, got {self.patience}.\")\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.ReduceLROnPlateauConfig.factor","title":"<code>factor: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.ReduceLROnPlateauConfig.min_lr","title":"<code>min_lr: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.ReduceLROnPlateauConfig.mode","title":"<code>mode: Literal['min', 'max']</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.ReduceLROnPlateauConfig.patience","title":"<code>patience: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.ReduceLROnPlateauConfig.threshold","title":"<code>threshold: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.ReduceLROnPlateauConfig.__init__","title":"<code>__init__(*, mode: Literal['min', 'max'], factor: float, patience: int, threshold: float, min_lr: float) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.ReduceLROnPlateauConfig.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/core/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if not (0 &lt; self.factor &lt; 1):\n        raise ValueError(f\"factor must be in (0, 1), got {self.factor}.\")\n    if self.patience &lt;= 0:\n        raise ValueError(f\"patience must be positive, got {self.patience}.\")\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.ResidualScorer","title":"<code>ResidualScorer</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for scoring candidate collocation points by PDE residual magnitude.</p> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>class ResidualScorer(Protocol):\n    \"\"\"Protocol for scoring candidate collocation points by PDE residual magnitude.\"\"\"\n\n    def residual_score(self, x: Tensor) -&gt; Tensor:\n        \"\"\"Return per-point non-negative residual score of shape ``(n,)``.\n\n        Args:\n            x: Candidate collocation points ``(n, d)``.\n\n        Returns:\n            Scores ``(n,)`` \u2014 higher means larger residual.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.ResidualScorer.residual_score","title":"<code>residual_score(x: Tensor) -&gt; Tensor</code>","text":"<p>Return per-point non-negative residual score of shape <code>(n,)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Candidate collocation points <code>(n, d)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Scores <code>(n,)</code> \u2014 higher means larger residual.</p> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def residual_score(self, x: Tensor) -&gt; Tensor:\n    \"\"\"Return per-point non-negative residual score of shape ``(n,)``.\n\n    Args:\n        x: Candidate collocation points ``(n, d)``.\n\n    Returns:\n        Scores ``(n,)`` \u2014 higher means larger residual.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.SMMAStoppingConfig","title":"<code>SMMAStoppingConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Simple Moving Average Stopping callback.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass SMMAStoppingConfig:\n    \"\"\"\n    Configuration for Simple Moving Average Stopping callback.\n    \"\"\"\n\n    window: int\n    threshold: float\n    lookback: int\n\n    def __post_init__(self) -&gt; None:\n        if self.window &lt;= 0:\n            raise ValueError(f\"window must be positive, got {self.window}.\")\n        if self.lookback &lt;= 0:\n            raise ValueError(f\"lookback must be positive, got {self.lookback}.\")\n        if self.threshold &lt;= 0:\n            raise ValueError(f\"threshold must be positive, got {self.threshold}.\")\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.SMMAStoppingConfig.lookback","title":"<code>lookback: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.SMMAStoppingConfig.threshold","title":"<code>threshold: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.SMMAStoppingConfig.window","title":"<code>window: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.SMMAStoppingConfig.__init__","title":"<code>__init__(*, window: int, threshold: float, lookback: int) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.SMMAStoppingConfig.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/core/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.window &lt;= 0:\n        raise ValueError(f\"window must be positive, got {self.window}.\")\n    if self.lookback &lt;= 0:\n        raise ValueError(f\"lookback must be positive, got {self.lookback}.\")\n    if self.threshold &lt;= 0:\n        raise ValueError(f\"threshold must be positive, got {self.threshold}.\")\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.ScalarConfig","title":"<code>ScalarConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a scalar parameter.</p> <p>Attributes:</p> Name Type Description <code>init_value</code> <code>float</code> <p>Initial value for the parameter.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass ScalarConfig:\n    \"\"\"\n    Configuration for a scalar parameter.\n\n    Attributes:\n        init_value: Initial value for the parameter.\n    \"\"\"\n\n    init_value: float\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.ScalarConfig.init_value","title":"<code>init_value: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.ScalarConfig.__init__","title":"<code>__init__(*, init_value: float) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.TrainingDataConfig","title":"<code>TrainingDataConfig</code>  <code>dataclass</code>","text":"<p>Configuration for data loading and batching.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>Number of points per training batch.</p> <code>data_ratio</code> <code>int | float</code> <p>Ratio of data to collocation points per batch.</p> <code>collocations</code> <code>int</code> <p>Total number of collocation points to generate.</p> <code>collocation_sampler</code> <code>CollocationStrategies</code> <p>Sampling strategy for collocation points.</p> <code>collocation_seed</code> <code>int | None</code> <p>Optional seed for reproducible collocation sampling.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass TrainingDataConfig:\n    \"\"\"\n    Configuration for data loading and batching.\n\n    Attributes:\n        batch_size: Number of points per training batch.\n        data_ratio: Ratio of data to collocation points per batch.\n        collocations: Total number of collocation points to generate.\n        collocation_sampler: Sampling strategy for collocation points.\n        collocation_seed: Optional seed for reproducible collocation sampling.\n    \"\"\"\n\n    batch_size: int\n    data_ratio: int | float\n    collocations: int\n    collocation_sampler: CollocationStrategies = \"random\"\n    collocation_seed: int | None = None\n\n    def __post_init__(self) -&gt; None:\n        if self.batch_size &lt;= 0:\n            raise ValueError(f\"batch_size must be positive, got {self.batch_size}.\")\n        if self.collocations &lt; 0:\n            raise ValueError(f\"collocations must be non-negative, got {self.collocations}.\")\n        if isinstance(self.data_ratio, float):\n            if not (0.0 &lt;= self.data_ratio &lt;= 1.0):\n                raise ValueError(f\"Float data_ratio must be in [0.0, 1.0], got {self.data_ratio}.\")\n        else:\n            if not (0 &lt;= self.data_ratio &lt;= self.batch_size):\n                raise ValueError(\n                    f\"Integer data_ratio must be in [0, {self.batch_size}], got {self.data_ratio}.\"\n                )\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.TrainingDataConfig.batch_size","title":"<code>batch_size: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.TrainingDataConfig.collocation_sampler","title":"<code>collocation_sampler: CollocationStrategies = 'random'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.TrainingDataConfig.collocation_seed","title":"<code>collocation_seed: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.TrainingDataConfig.collocations","title":"<code>collocations: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.TrainingDataConfig.data_ratio","title":"<code>data_ratio: int | float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.TrainingDataConfig.__init__","title":"<code>__init__(*, batch_size: int, data_ratio: int | float, collocations: int, collocation_sampler: CollocationStrategies = 'random', collocation_seed: int | None = None) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/#anypinn.core.TrainingDataConfig.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/core/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.batch_size &lt;= 0:\n        raise ValueError(f\"batch_size must be positive, got {self.batch_size}.\")\n    if self.collocations &lt; 0:\n        raise ValueError(f\"collocations must be non-negative, got {self.collocations}.\")\n    if isinstance(self.data_ratio, float):\n        if not (0.0 &lt;= self.data_ratio &lt;= 1.0):\n            raise ValueError(f\"Float data_ratio must be in [0.0, 1.0], got {self.data_ratio}.\")\n    else:\n        if not (0 &lt;= self.data_ratio &lt;= self.batch_size):\n            raise ValueError(\n                f\"Integer data_ratio must be in [0, {self.batch_size}], got {self.data_ratio}.\"\n            )\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.UniformSampler","title":"<code>UniformSampler</code>","text":"<p>Cartesian grid sampler that distributes points evenly across the domain.</p> <p>For d-dimensional domains, places <code>ceil(n^(1/d))</code> points per axis then takes the first <code>n</code> points of the resulting grid.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Optional seed (unused \u2014 grid is deterministic).</p> <code>None</code> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>class UniformSampler:\n    \"\"\"Cartesian grid sampler that distributes points evenly across the domain.\n\n    For d-dimensional domains, places ``ceil(n^(1/d))`` points per axis then\n    takes the first ``n`` points of the resulting grid.\n\n    Args:\n        seed: Optional seed (unused \u2014 grid is deterministic).\n    \"\"\"\n\n    def __init__(self, seed: int | None = None) -&gt; None:\n        pass\n\n    def sample(self, n: int, domain: Domain) -&gt; Tensor:\n        d = domain.ndim\n        pts_per_dim = math.ceil(n ** (1.0 / d))\n\n        linspaces = [torch.linspace(lo, hi, pts_per_dim) for lo, hi in domain.bounds]\n        grids = torch.meshgrid(*linspaces, indexing=\"ij\")\n        flat = torch.stack([g.reshape(-1) for g in grids], dim=-1)\n        return flat[:n]\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.UniformSampler.__init__","title":"<code>__init__(seed: int | None = None) -&gt; None</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def __init__(self, seed: int | None = None) -&gt; None:\n    pass\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.UniformSampler.sample","title":"<code>sample(n: int, domain: Domain) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def sample(self, n: int, domain: Domain) -&gt; Tensor:\n    d = domain.ndim\n    pts_per_dim = math.ceil(n ** (1.0 / d))\n\n    linspaces = [torch.linspace(lo, hi, pts_per_dim) for lo, hi in domain.bounds]\n    grids = torch.meshgrid(*linspaces, indexing=\"ij\")\n    flat = torch.stack([g.reshape(-1) for g in grids], dim=-1)\n    return flat[:n]\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.build_criterion","title":"<code>build_criterion(name: Criteria) -&gt; nn.Module</code>","text":"<p>Return the loss-criterion module for the given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Criteria</code> <p>One of <code>\"mse\"</code>, <code>\"huber\"</code>, <code>\"l1\"</code>.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The corresponding PyTorch loss module.</p> Source code in <code>src/anypinn/core/nn.py</code> <pre><code>def build_criterion(name: Criteria) -&gt; nn.Module:\n    \"\"\"\n    Return the loss-criterion module for the given name.\n\n    Args:\n        name: One of ``\"mse\"``, ``\"huber\"``, ``\"l1\"``.\n\n    Returns:\n        The corresponding PyTorch loss module.\n    \"\"\"\n    return {\n        \"mse\": nn.MSELoss(),\n        \"huber\": nn.HuberLoss(),\n        \"l1\": nn.L1Loss(),\n    }[name]\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.build_sampler","title":"<code>build_sampler(strategy: CollocationStrategies, seed: int | None = None, scorer: ResidualScorer | None = None) -&gt; CollocationSampler</code>","text":"<p>Construct a collocation sampler from a strategy name.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>CollocationStrategies</code> <p>One of the <code>CollocationStrategies</code> literals.</p> required <code>seed</code> <code>int | None</code> <p>Optional seed for reproducible sampling.</p> <code>None</code> <code>scorer</code> <code>ResidualScorer | None</code> <p>Required when <code>strategy=\"adaptive\"</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>CollocationSampler</code> <p>A sampler instance satisfying the <code>CollocationSampler</code> protocol.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>strategy=\"adaptive\"</code> but no scorer is provided.</p> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def build_sampler(\n    strategy: CollocationStrategies,\n    seed: int | None = None,\n    scorer: ResidualScorer | None = None,\n) -&gt; CollocationSampler:\n    \"\"\"Construct a collocation sampler from a strategy name.\n\n    Args:\n        strategy: One of the ``CollocationStrategies`` literals.\n        seed: Optional seed for reproducible sampling.\n        scorer: Required when ``strategy=\"adaptive\"``.\n\n    Returns:\n        A sampler instance satisfying the ``CollocationSampler`` protocol.\n\n    Raises:\n        ValueError: If ``strategy=\"adaptive\"`` but no scorer is provided.\n    \"\"\"\n    if strategy == \"adaptive\":\n        if scorer is None:\n            raise ValueError(\n                \"AdaptiveSampler requires a ResidualScorer. \"\n                \"Pass a scorer via PINNDataModule or use a different strategy.\"\n            )\n        return AdaptiveSampler(scorer=scorer, seed=seed)\n\n    cls = _SAMPLER_REGISTRY.get(strategy)\n    if cls is None:\n        raise ValueError(\n            f\"Unknown collocation strategy '{strategy}'. \"\n            f\"Choose from: {', '.join(_SAMPLER_REGISTRY)} or 'adaptive'.\"\n        )\n    return cls(seed=seed)\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.get_activation","title":"<code>get_activation(name: Activations) -&gt; nn.Module</code>","text":"<p>Get the activation function module by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Activations</code> <p>The name of the activation function.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The PyTorch activation module.</p> Source code in <code>src/anypinn/core/nn.py</code> <pre><code>def get_activation(name: Activations) -&gt; nn.Module:\n    \"\"\"\n    Get the activation function module by name.\n\n    Args:\n        name: The name of the activation function.\n\n    Returns:\n        The PyTorch activation module.\n    \"\"\"\n    return {\n        \"tanh\": nn.Tanh(),\n        \"relu\": nn.ReLU(),\n        \"leaky_relu\": nn.LeakyReLU(),\n        \"sigmoid\": nn.Sigmoid(),\n        \"selu\": nn.SELU(),\n        \"softplus\": nn.Softplus(),\n        \"identity\": nn.Identity(),\n    }[name]\n</code></pre>"},{"location":"reference/anypinn/core/#anypinn.core.resolve_validation","title":"<code>resolve_validation(registry: ValidationRegistry, df_path: Path | None = None) -&gt; ResolvedValidation</code>","text":"<p>Resolve a ValidationRegistry by converting ColumnRef entries to callables.</p> <p>Pure function entries are passed through unchanged. ColumnRef entries are resolved using the provided data file path.</p> <p>Parameters:</p> Name Type Description Default <code>registry</code> <code>ValidationRegistry</code> <p>The validation registry to resolve.</p> required <code>df_path</code> <code>Path | None</code> <p>Path to the CSV file for ColumnRef resolution.</p> <code>None</code> <p>Returns:</p> Type Description <code>ResolvedValidation</code> <p>A dictionary mapping parameter names to callable validation functions.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a ColumnRef cannot be resolved (missing column or no df_path).</p> Source code in <code>src/anypinn/core/validation.py</code> <pre><code>def resolve_validation(\n    registry: ValidationRegistry,\n    df_path: Path | None = None,\n) -&gt; ResolvedValidation:\n    \"\"\"\n    Resolve a ValidationRegistry by converting ColumnRef entries to callables.\n\n    Pure function entries are passed through unchanged. ColumnRef entries\n    are resolved using the provided data file path.\n\n    Args:\n        registry: The validation registry to resolve.\n        df_path: Path to the CSV file for ColumnRef resolution.\n\n    Returns:\n        A dictionary mapping parameter names to callable validation functions.\n\n    Raises:\n        ValueError: If a ColumnRef cannot be resolved (missing column or no df_path).\n    \"\"\"\n\n    resolved: ResolvedValidation = {}\n    df: pd.DataFrame | None = None\n\n    for name, source in registry.items():\n        if source is None:\n            continue\n\n        if callable(source) and not isinstance(source, ColumnRef):\n            resolved[name] = source\n\n        elif isinstance(source, ColumnRef):\n            if df_path is None:\n                raise ValueError(\n                    f\"Cannot resolve ColumnRef for '{name}': no df_path provided. \"\n                    \"Either pass a df_path or use a callable instead of ColumnRef.\"\n                )\n\n            if df is None:\n                df = pd.read_csv(df_path)\n\n            if source.column not in df.columns:\n                raise ValueError(\n                    f\"Cannot resolve ColumnRef for '{name}': \"\n                    f\"column '{source.column}' not found in data. \"\n                    f\"Available columns: {list(df.columns)}\"\n                )\n\n            column_values = torch.tensor(df[source.column].values, dtype=torch.float32)\n\n            if source.transform is not None:\n                column_values = source.transform(column_values)\n\n            def make_lookup_fn(values: Tensor) -&gt; Callable[[Tensor], Tensor]:\n                cache: dict[torch.device, Tensor] = {}\n\n                def lookup(x: Tensor) -&gt; Tensor:\n                    device = x.device\n                    if device not in cache:\n                        cache[device] = values.to(device)\n                    idx = x.squeeze(-1).round().to(torch.int32)\n                    return cache[device][idx]\n\n                return lookup\n\n            resolved[name] = _ColumnLookup(make_lookup_fn(column_values))\n\n    return resolved\n</code></pre>"},{"location":"reference/anypinn/core/config/","title":"config","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config","title":"<code>anypinn.core.config</code>","text":"<p>Configuration dataclasses for PINN models.</p>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.AdamConfig","title":"<code>AdamConfig</code>  <code>dataclass</code>","text":"<p>Configuration for the Adam optimizer.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass AdamConfig:\n    \"\"\"\n    Configuration for the Adam optimizer.\n    \"\"\"\n\n    lr: float = 1e-3\n    betas: tuple[float, float] = (0.9, 0.999)\n    weight_decay: float = 0.0\n\n    def __post_init__(self) -&gt; None:\n        if self.lr &lt;= 0:\n            raise ValueError(f\"lr must be positive, got {self.lr}.\")\n        if self.weight_decay &lt; 0:\n            raise ValueError(f\"weight_decay must be non-negative, got {self.weight_decay}.\")\n        if not (0 &lt; self.betas[0] &lt; 1):\n            raise ValueError(f\"betas[0] must be in (0, 1), got {self.betas[0]}.\")\n        if not (0 &lt; self.betas[1] &lt; 1):\n            raise ValueError(f\"betas[1] must be in (0, 1), got {self.betas[1]}.\")\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.AdamConfig.betas","title":"<code>betas: tuple[float, float] = (0.9, 0.999)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.AdamConfig.lr","title":"<code>lr: float = 0.001</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.AdamConfig.weight_decay","title":"<code>weight_decay: float = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.AdamConfig.__init__","title":"<code>__init__(*, lr: float = 0.001, betas: tuple[float, float] = (0.9, 0.999), weight_decay: float = 0.0) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.AdamConfig.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/core/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.lr &lt;= 0:\n        raise ValueError(f\"lr must be positive, got {self.lr}.\")\n    if self.weight_decay &lt; 0:\n        raise ValueError(f\"weight_decay must be non-negative, got {self.weight_decay}.\")\n    if not (0 &lt; self.betas[0] &lt; 1):\n        raise ValueError(f\"betas[0] must be in (0, 1), got {self.betas[0]}.\")\n    if not (0 &lt; self.betas[1] &lt; 1):\n        raise ValueError(f\"betas[1] must be in (0, 1), got {self.betas[1]}.\")\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.CosineAnnealingConfig","title":"<code>CosineAnnealingConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Cosine Annealing LR Scheduler.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass CosineAnnealingConfig:\n    \"\"\"\n    Configuration for Cosine Annealing LR Scheduler.\n    \"\"\"\n\n    T_max: int\n    eta_min: float = 0.0\n\n    def __post_init__(self) -&gt; None:\n        if self.T_max &lt;= 0:\n            raise ValueError(f\"T_max must be positive, got {self.T_max}.\")\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.CosineAnnealingConfig.T_max","title":"<code>T_max: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.CosineAnnealingConfig.eta_min","title":"<code>eta_min: float = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.CosineAnnealingConfig.__init__","title":"<code>__init__(*, T_max: int, eta_min: float = 0.0) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.CosineAnnealingConfig.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/core/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.T_max &lt;= 0:\n        raise ValueError(f\"T_max must be positive, got {self.T_max}.\")\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.EarlyStoppingConfig","title":"<code>EarlyStoppingConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Early Stopping callback.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass EarlyStoppingConfig:\n    \"\"\"\n    Configuration for Early Stopping callback.\n    \"\"\"\n\n    patience: int\n    mode: Literal[\"min\", \"max\"]\n\n    def __post_init__(self) -&gt; None:\n        if self.patience &lt;= 0:\n            raise ValueError(f\"patience must be positive, got {self.patience}.\")\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.EarlyStoppingConfig.mode","title":"<code>mode: Literal['min', 'max']</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.EarlyStoppingConfig.patience","title":"<code>patience: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.EarlyStoppingConfig.__init__","title":"<code>__init__(*, patience: int, mode: Literal['min', 'max']) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.EarlyStoppingConfig.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/core/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.patience &lt;= 0:\n        raise ValueError(f\"patience must be positive, got {self.patience}.\")\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.GenerationConfig","title":"<code>GenerationConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrainingDataConfig</code></p> <p>Configuration for data generation.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass GenerationConfig(TrainingDataConfig):\n    \"\"\"\n    Configuration for data generation.\n    \"\"\"\n\n    x: Tensor\n    noise_level: float\n    args_to_train: ArgsRegistry\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.GenerationConfig.args_to_train","title":"<code>args_to_train: ArgsRegistry</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.GenerationConfig.noise_level","title":"<code>noise_level: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.GenerationConfig.x","title":"<code>x: Tensor</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.GenerationConfig.__init__","title":"<code>__init__(*, batch_size: int, data_ratio: int | float, collocations: int, collocation_sampler: CollocationStrategies = 'random', collocation_seed: int | None = None, x: Tensor, noise_level: float, args_to_train: ArgsRegistry) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.IngestionConfig","title":"<code>IngestionConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrainingDataConfig</code></p> <p>Configuration for data ingestion from files. If x_column is None, the data is assumed to be evenly spaced.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass IngestionConfig(TrainingDataConfig):\n    \"\"\"\n    Configuration for data ingestion from files.\n    If x_column is None, the data is assumed to be evenly spaced.\n    \"\"\"\n\n    df_path: Path\n    x_transform: Callable[[Any], Any] | None = None\n    x_column: str | None = None\n    y_columns: list[str]\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.IngestionConfig.df_path","title":"<code>df_path: Path</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.IngestionConfig.x_column","title":"<code>x_column: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.IngestionConfig.x_transform","title":"<code>x_transform: Callable[[Any], Any] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.IngestionConfig.y_columns","title":"<code>y_columns: list[str]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.IngestionConfig.__init__","title":"<code>__init__(*, batch_size: int, data_ratio: int | float, collocations: int, collocation_sampler: CollocationStrategies = 'random', collocation_seed: int | None = None, df_path: Path, x_transform: Callable[[Any], Any] | None = None, x_column: str | None = None, y_columns: list[str]) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.LBFGSConfig","title":"<code>LBFGSConfig</code>  <code>dataclass</code>","text":"<p>Configuration for the L-BFGS optimizer.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass LBFGSConfig:\n    \"\"\"\n    Configuration for the L-BFGS optimizer.\n    \"\"\"\n\n    lr: float = 1.0\n    max_iter: int = 20\n    max_eval: int | None = None\n    history_size: int = 100\n    line_search_fn: str | None = \"strong_wolfe\"\n\n    def __post_init__(self) -&gt; None:\n        if self.lr &lt;= 0:\n            raise ValueError(f\"lr must be positive, got {self.lr}.\")\n        if self.max_iter &lt;= 0:\n            raise ValueError(f\"max_iter must be positive, got {self.max_iter}.\")\n        if self.history_size &lt;= 0:\n            raise ValueError(f\"history_size must be positive, got {self.history_size}.\")\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.LBFGSConfig.history_size","title":"<code>history_size: int = 100</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.LBFGSConfig.line_search_fn","title":"<code>line_search_fn: str | None = 'strong_wolfe'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.LBFGSConfig.lr","title":"<code>lr: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.LBFGSConfig.max_eval","title":"<code>max_eval: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.LBFGSConfig.max_iter","title":"<code>max_iter: int = 20</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.LBFGSConfig.__init__","title":"<code>__init__(*, lr: float = 1.0, max_iter: int = 20, max_eval: int | None = None, history_size: int = 100, line_search_fn: str | None = 'strong_wolfe') -&gt; None</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.LBFGSConfig.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/core/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.lr &lt;= 0:\n        raise ValueError(f\"lr must be positive, got {self.lr}.\")\n    if self.max_iter &lt;= 0:\n        raise ValueError(f\"max_iter must be positive, got {self.max_iter}.\")\n    if self.history_size &lt;= 0:\n        raise ValueError(f\"history_size must be positive, got {self.history_size}.\")\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.MLPConfig","title":"<code>MLPConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a Multi-Layer Perceptron (MLP).</p> <p>Attributes:</p> Name Type Description <code>in_dim</code> <code>int</code> <p>Dimension of input layer.</p> <code>out_dim</code> <code>int</code> <p>Dimension of output layer.</p> <code>hidden_layers</code> <code>list[int]</code> <p>List of dimensions for hidden layers.</p> <code>activation</code> <code>Activations</code> <p>Activation function to use between layers.</p> <code>output_activation</code> <code>Activations | None</code> <p>Optional activation function for the output layer.</p> <code>encode</code> <code>Callable[[Tensor], Tensor] | None</code> <p>Optional function to encode inputs before passing to MLP.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass MLPConfig:\n    \"\"\"\n    Configuration for a Multi-Layer Perceptron (MLP).\n\n    Attributes:\n        in_dim: Dimension of input layer.\n        out_dim: Dimension of output layer.\n        hidden_layers: List of dimensions for hidden layers.\n        activation: Activation function to use between layers.\n        output_activation: Optional activation function for the output layer.\n        encode: Optional function to encode inputs before passing to MLP.\n    \"\"\"\n\n    in_dim: int\n    out_dim: int\n    hidden_layers: list[int]\n    activation: Activations\n    output_activation: Activations | None = None\n    encode: Callable[[Tensor], Tensor] | None = None\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.MLPConfig.activation","title":"<code>activation: Activations</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.MLPConfig.encode","title":"<code>encode: Callable[[Tensor], Tensor] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.MLPConfig.hidden_layers","title":"<code>hidden_layers: list[int]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.MLPConfig.in_dim","title":"<code>in_dim: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.MLPConfig.out_dim","title":"<code>out_dim: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.MLPConfig.output_activation","title":"<code>output_activation: Activations | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.MLPConfig.__init__","title":"<code>__init__(*, in_dim: int, out_dim: int, hidden_layers: list[int], activation: Activations, output_activation: Activations | None = None, encode: Callable[[Tensor], Tensor] | None = None) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.PINNHyperparameters","title":"<code>PINNHyperparameters</code>  <code>dataclass</code>","text":"<p>Aggregated hyperparameters for the PINN model.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass PINNHyperparameters:\n    \"\"\"\n    Aggregated hyperparameters for the PINN model.\n    \"\"\"\n\n    lr: float\n    training_data: IngestionConfig | GenerationConfig\n    fields_config: MLPConfig\n    params_config: MLPConfig | ScalarConfig\n    max_epochs: int | None = None\n    gradient_clip_val: float | None = None\n    criterion: Criteria = \"mse\"\n    optimizer: AdamConfig | LBFGSConfig | None = None\n    scheduler: ReduceLROnPlateauConfig | CosineAnnealingConfig | None = None\n    early_stopping: EarlyStoppingConfig | None = None\n    smma_stopping: SMMAStoppingConfig | None = None\n\n    def __post_init__(self) -&gt; None:\n        if self.lr &lt;= 0:\n            raise ValueError(f\"lr must be positive, got {self.lr}.\")\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.PINNHyperparameters.criterion","title":"<code>criterion: Criteria = 'mse'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.PINNHyperparameters.early_stopping","title":"<code>early_stopping: EarlyStoppingConfig | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.PINNHyperparameters.fields_config","title":"<code>fields_config: MLPConfig</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.PINNHyperparameters.gradient_clip_val","title":"<code>gradient_clip_val: float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.PINNHyperparameters.lr","title":"<code>lr: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.PINNHyperparameters.max_epochs","title":"<code>max_epochs: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.PINNHyperparameters.optimizer","title":"<code>optimizer: AdamConfig | LBFGSConfig | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.PINNHyperparameters.params_config","title":"<code>params_config: MLPConfig | ScalarConfig</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.PINNHyperparameters.scheduler","title":"<code>scheduler: ReduceLROnPlateauConfig | CosineAnnealingConfig | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.PINNHyperparameters.smma_stopping","title":"<code>smma_stopping: SMMAStoppingConfig | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.PINNHyperparameters.training_data","title":"<code>training_data: IngestionConfig | GenerationConfig</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.PINNHyperparameters.__init__","title":"<code>__init__(*, lr: float, training_data: IngestionConfig | GenerationConfig, fields_config: MLPConfig, params_config: MLPConfig | ScalarConfig, max_epochs: int | None = None, gradient_clip_val: float | None = None, criterion: Criteria = 'mse', optimizer: AdamConfig | LBFGSConfig | None = None, scheduler: ReduceLROnPlateauConfig | CosineAnnealingConfig | None = None, early_stopping: EarlyStoppingConfig | None = None, smma_stopping: SMMAStoppingConfig | None = None) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.PINNHyperparameters.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/core/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.lr &lt;= 0:\n        raise ValueError(f\"lr must be positive, got {self.lr}.\")\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.ReduceLROnPlateauConfig","title":"<code>ReduceLROnPlateauConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Learning Rate Scheduler (ReduceLROnPlateau).</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass ReduceLROnPlateauConfig:\n    \"\"\"\n    Configuration for Learning Rate Scheduler (ReduceLROnPlateau).\n    \"\"\"\n\n    mode: Literal[\"min\", \"max\"]\n    factor: float\n    patience: int\n    threshold: float\n    min_lr: float\n\n    def __post_init__(self) -&gt; None:\n        if not (0 &lt; self.factor &lt; 1):\n            raise ValueError(f\"factor must be in (0, 1), got {self.factor}.\")\n        if self.patience &lt;= 0:\n            raise ValueError(f\"patience must be positive, got {self.patience}.\")\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.ReduceLROnPlateauConfig.factor","title":"<code>factor: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.ReduceLROnPlateauConfig.min_lr","title":"<code>min_lr: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.ReduceLROnPlateauConfig.mode","title":"<code>mode: Literal['min', 'max']</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.ReduceLROnPlateauConfig.patience","title":"<code>patience: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.ReduceLROnPlateauConfig.threshold","title":"<code>threshold: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.ReduceLROnPlateauConfig.__init__","title":"<code>__init__(*, mode: Literal['min', 'max'], factor: float, patience: int, threshold: float, min_lr: float) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.ReduceLROnPlateauConfig.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/core/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if not (0 &lt; self.factor &lt; 1):\n        raise ValueError(f\"factor must be in (0, 1), got {self.factor}.\")\n    if self.patience &lt;= 0:\n        raise ValueError(f\"patience must be positive, got {self.patience}.\")\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.SMMAStoppingConfig","title":"<code>SMMAStoppingConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Simple Moving Average Stopping callback.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass SMMAStoppingConfig:\n    \"\"\"\n    Configuration for Simple Moving Average Stopping callback.\n    \"\"\"\n\n    window: int\n    threshold: float\n    lookback: int\n\n    def __post_init__(self) -&gt; None:\n        if self.window &lt;= 0:\n            raise ValueError(f\"window must be positive, got {self.window}.\")\n        if self.lookback &lt;= 0:\n            raise ValueError(f\"lookback must be positive, got {self.lookback}.\")\n        if self.threshold &lt;= 0:\n            raise ValueError(f\"threshold must be positive, got {self.threshold}.\")\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.SMMAStoppingConfig.lookback","title":"<code>lookback: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.SMMAStoppingConfig.threshold","title":"<code>threshold: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.SMMAStoppingConfig.window","title":"<code>window: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.SMMAStoppingConfig.__init__","title":"<code>__init__(*, window: int, threshold: float, lookback: int) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.SMMAStoppingConfig.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/core/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.window &lt;= 0:\n        raise ValueError(f\"window must be positive, got {self.window}.\")\n    if self.lookback &lt;= 0:\n        raise ValueError(f\"lookback must be positive, got {self.lookback}.\")\n    if self.threshold &lt;= 0:\n        raise ValueError(f\"threshold must be positive, got {self.threshold}.\")\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.ScalarConfig","title":"<code>ScalarConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a scalar parameter.</p> <p>Attributes:</p> Name Type Description <code>init_value</code> <code>float</code> <p>Initial value for the parameter.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass ScalarConfig:\n    \"\"\"\n    Configuration for a scalar parameter.\n\n    Attributes:\n        init_value: Initial value for the parameter.\n    \"\"\"\n\n    init_value: float\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.ScalarConfig.init_value","title":"<code>init_value: float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.ScalarConfig.__init__","title":"<code>__init__(*, init_value: float) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.TrainingDataConfig","title":"<code>TrainingDataConfig</code>  <code>dataclass</code>","text":"<p>Configuration for data loading and batching.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>Number of points per training batch.</p> <code>data_ratio</code> <code>int | float</code> <p>Ratio of data to collocation points per batch.</p> <code>collocations</code> <code>int</code> <p>Total number of collocation points to generate.</p> <code>collocation_sampler</code> <code>CollocationStrategies</code> <p>Sampling strategy for collocation points.</p> <code>collocation_seed</code> <code>int | None</code> <p>Optional seed for reproducible collocation sampling.</p> Source code in <code>src/anypinn/core/config.py</code> <pre><code>@dataclass(kw_only=True)\nclass TrainingDataConfig:\n    \"\"\"\n    Configuration for data loading and batching.\n\n    Attributes:\n        batch_size: Number of points per training batch.\n        data_ratio: Ratio of data to collocation points per batch.\n        collocations: Total number of collocation points to generate.\n        collocation_sampler: Sampling strategy for collocation points.\n        collocation_seed: Optional seed for reproducible collocation sampling.\n    \"\"\"\n\n    batch_size: int\n    data_ratio: int | float\n    collocations: int\n    collocation_sampler: CollocationStrategies = \"random\"\n    collocation_seed: int | None = None\n\n    def __post_init__(self) -&gt; None:\n        if self.batch_size &lt;= 0:\n            raise ValueError(f\"batch_size must be positive, got {self.batch_size}.\")\n        if self.collocations &lt; 0:\n            raise ValueError(f\"collocations must be non-negative, got {self.collocations}.\")\n        if isinstance(self.data_ratio, float):\n            if not (0.0 &lt;= self.data_ratio &lt;= 1.0):\n                raise ValueError(f\"Float data_ratio must be in [0.0, 1.0], got {self.data_ratio}.\")\n        else:\n            if not (0 &lt;= self.data_ratio &lt;= self.batch_size):\n                raise ValueError(\n                    f\"Integer data_ratio must be in [0, {self.batch_size}], got {self.data_ratio}.\"\n                )\n</code></pre>"},{"location":"reference/anypinn/core/config/#anypinn.core.config.TrainingDataConfig.batch_size","title":"<code>batch_size: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.TrainingDataConfig.collocation_sampler","title":"<code>collocation_sampler: CollocationStrategies = 'random'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.TrainingDataConfig.collocation_seed","title":"<code>collocation_seed: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.TrainingDataConfig.collocations","title":"<code>collocations: int</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.TrainingDataConfig.data_ratio","title":"<code>data_ratio: int | float</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.TrainingDataConfig.__init__","title":"<code>__init__(*, batch_size: int, data_ratio: int | float, collocations: int, collocation_sampler: CollocationStrategies = 'random', collocation_seed: int | None = None) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/config/#anypinn.core.config.TrainingDataConfig.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/core/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.batch_size &lt;= 0:\n        raise ValueError(f\"batch_size must be positive, got {self.batch_size}.\")\n    if self.collocations &lt; 0:\n        raise ValueError(f\"collocations must be non-negative, got {self.collocations}.\")\n    if isinstance(self.data_ratio, float):\n        if not (0.0 &lt;= self.data_ratio &lt;= 1.0):\n            raise ValueError(f\"Float data_ratio must be in [0.0, 1.0], got {self.data_ratio}.\")\n    else:\n        if not (0 &lt;= self.data_ratio &lt;= self.batch_size):\n            raise ValueError(\n                f\"Integer data_ratio must be in [0, {self.batch_size}], got {self.data_ratio}.\"\n            )\n</code></pre>"},{"location":"reference/anypinn/core/context/","title":"context","text":""},{"location":"reference/anypinn/core/context/#anypinn.core.context","title":"<code>anypinn.core.context</code>","text":"<p>Runtime context inferred from training data.</p>"},{"location":"reference/anypinn/core/context/#anypinn.core.context.InferredContext","title":"<code>InferredContext</code>  <code>dataclass</code>","text":"<p>Runtime context inferred from training data.</p> <p>This holds the data that is either explicitly provided in props or inferred from training data.</p> Source code in <code>src/anypinn/core/context.py</code> <pre><code>@dataclass\nclass InferredContext:\n    \"\"\"\n    Runtime context inferred from training data.\n\n    This holds the data that is either explicitly provided in props or inferred from training data.\n    \"\"\"\n\n    def __init__(\n        self,\n        x: Tensor,\n        y: Tensor,\n        validation: ResolvedValidation,\n    ):\n        \"\"\"\n        Infer context from either generated or loaded data.\n\n        Args:\n            x: x coordinates.\n            y: observations.\n            validation: Resolved validation dictionary.\n        \"\"\"\n\n        self.domain = Domain.from_x(x)\n        self.validation = validation\n</code></pre>"},{"location":"reference/anypinn/core/context/#anypinn.core.context.InferredContext.domain","title":"<code>domain = Domain.from_x(x)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/context/#anypinn.core.context.InferredContext.validation","title":"<code>validation = validation</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/context/#anypinn.core.context.InferredContext.__init__","title":"<code>__init__(x: Tensor, y: Tensor, validation: ResolvedValidation)</code>","text":"<p>Infer context from either generated or loaded data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>x coordinates.</p> required <code>y</code> <code>Tensor</code> <p>observations.</p> required <code>validation</code> <code>ResolvedValidation</code> <p>Resolved validation dictionary.</p> required Source code in <code>src/anypinn/core/context.py</code> <pre><code>def __init__(\n    self,\n    x: Tensor,\n    y: Tensor,\n    validation: ResolvedValidation,\n):\n    \"\"\"\n    Infer context from either generated or loaded data.\n\n    Args:\n        x: x coordinates.\n        y: observations.\n        validation: Resolved validation dictionary.\n    \"\"\"\n\n    self.domain = Domain.from_x(x)\n    self.validation = validation\n</code></pre>"},{"location":"reference/anypinn/core/dataset/","title":"dataset","text":""},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset","title":"<code>anypinn.core.dataset</code>","text":"<p>Data handling for PINN training.</p>"},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.DataCallback","title":"<code>DataCallback</code>","text":"<p>Abstract base class for building new data callbacks.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>class DataCallback:\n    \"\"\"Abstract base class for building new data callbacks.\"\"\"\n\n    def transform_data(self, data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]:\n        \"\"\"Transform the data and collocation points.\"\"\"\n        return data, coll\n\n    def on_after_setup(self, dm: \"PINNDataModule\") -&gt; None:\n        \"\"\"Called after setup is complete.\"\"\"\n        return None\n</code></pre>"},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.DataCallback.on_after_setup","title":"<code>on_after_setup(dm: PINNDataModule) -&gt; None</code>","text":"<p>Called after setup is complete.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>def on_after_setup(self, dm: \"PINNDataModule\") -&gt; None:\n    \"\"\"Called after setup is complete.\"\"\"\n    return None\n</code></pre>"},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.DataCallback.transform_data","title":"<code>transform_data(data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]</code>","text":"<p>Transform the data and collocation points.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>def transform_data(self, data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]:\n    \"\"\"Transform the data and collocation points.\"\"\"\n    return data, coll\n</code></pre>"},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataModule","title":"<code>PINNDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code>, <code>ABC</code></p> <p>LightningDataModule for PINNs. Manages data and collocation datasets and creates the combined PINNDataset.</p> <p>Collocation points are generated via a <code>CollocationSampler</code> selected by the <code>collocation_sampler</code> field in <code>TrainingDataConfig</code> (string literal). Subclasses only need to implement <code>gen_data()</code>; collocation generation is handled by the sampler resolved from the hyperparameters.</p> <p>Attributes:</p> Name Type Description <code>pinn_ds</code> <p>Combined PINNDataset for training.</p> <code>callbacks</code> <code>list[DataCallback]</code> <p>Sequence of DataCallback callbacks applied after data loading.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>class PINNDataModule(pl.LightningDataModule, ABC):\n    \"\"\"\n    LightningDataModule for PINNs.\n    Manages data and collocation datasets and creates the combined PINNDataset.\n\n    Collocation points are generated via a ``CollocationSampler`` selected by the\n    ``collocation_sampler`` field in ``TrainingDataConfig`` (string literal).\n    Subclasses only need to implement ``gen_data()``; collocation generation is\n    handled by the sampler resolved from the hyperparameters.\n\n    Attributes:\n        pinn_ds: Combined PINNDataset for training.\n        callbacks: Sequence of DataCallback callbacks applied after data loading.\n    \"\"\"\n\n    def __init__(\n        self,\n        hp: PINNHyperparameters,\n        validation: ValidationRegistry | None = None,\n        callbacks: Sequence[DataCallback] | None = None,\n        residual_scorer: ResidualScorer | None = None,\n    ) -&gt; None:\n        super().__init__()\n        self.hp = hp\n        self.callbacks: list[DataCallback] = list(callbacks) if callbacks else []\n        self._residual_scorer = residual_scorer\n\n        self._unresolved_validation = validation or {}\n        self._context: InferredContext | None = None\n\n    def _build_sampler(self, strategy: CollocationStrategies) -&gt; CollocationSampler:\n        \"\"\"Resolve a collocation sampler from a strategy name.\"\"\"\n        return build_sampler(\n            strategy=strategy,\n            seed=self.hp.training_data.collocation_seed,\n            scorer=self._residual_scorer,\n        )\n\n    def load_data(self, config: IngestionConfig) -&gt; DataBatch:\n        \"\"\"Load raw data from IngestionConfig.\"\"\"\n        df = pd.read_csv(config.df_path)\n\n        if config.x_column is not None:\n            x_values = df[config.x_column].values\n\n            if config.x_transform is not None:\n                x_values = config.x_transform(x_values)\n\n            x = torch.tensor(x_values, dtype=torch.float32)\n        else:\n            x = torch.arange(len(df), dtype=torch.float32)\n\n        y = torch.tensor(df[config.y_columns].values, dtype=torch.float32)\n\n        if y.ndim == 1:\n            y = y.unsqueeze(-1)  # (N,) \u2192 (N, 1)\n        y = y.unsqueeze(-1)  # (N, k) \u2192 (N, k, 1) always\n\n        return x.unsqueeze(-1), y\n\n    @abstractmethod\n    def gen_data(self, config: GenerationConfig) -&gt; DataBatch:\n        \"\"\"Generate synthetic data from GenerationConfig.\"\"\"\n\n    @override\n    def setup(self, stage: str | None = None) -&gt; None:\n        \"\"\"\n        Load raw data from IngestionConfig, or generate synthetic data from GenerationConfig.\n        Apply registered callbacks, create InferredContext and datasets.\n        \"\"\"\n        config = self.hp.training_data\n\n        self.validation = resolve_validation(\n            self._unresolved_validation,\n            config.df_path if isinstance(config, IngestionConfig) else None,\n        )\n\n        self.data = (\n            self.load_data(config)\n            if isinstance(config, IngestionConfig)\n            else self.gen_data(config)\n        )\n\n        domain = Domain.from_x(self.data[0])\n        self._domain = domain\n        self._sampler = self._build_sampler(config.collocation_sampler)\n        self.coll = self._sampler.sample(config.collocations, domain)\n\n        for callback in self.callbacks:\n            self.data, self.coll = callback.transform_data(self.data, self.coll)\n\n        x_data, y_data = self.data\n\n        if x_data.shape[0] != y_data.shape[0]:\n            raise ValueError(\n                f\"Size mismatch: x has {x_data.shape[0]} rows, y has {y_data.shape[0]} rows.\"\n            )\n        if x_data.ndim != 2 or x_data.shape[1] &lt; 1:\n            raise ValueError(f\"Expected x shape (n, d) with d &gt;= 1, got {tuple(x_data.shape)}.\")\n        if y_data.ndim &lt; 2 or y_data.shape[-1] != 1:\n            raise ValueError(f\"Expected y shape (n, ..., 1), got {tuple(y_data.shape)}.\")\n        if self.coll.ndim != 2 or self.coll.shape[1] &lt; 1:\n            raise ValueError(\n                f\"Expected coll shape (m, d) with d &gt;= 1, got {tuple(self.coll.shape)}.\"\n            )\n        if x_data.shape[1] != self.coll.shape[1]:\n            raise ValueError(\n                f\"Spatial dimension mismatch: x_data has d={x_data.shape[1]}, \"\n                f\"coll has d={self.coll.shape[1]}. Both must share the same number of dimensions.\"\n            )\n\n        self._data_size = x_data.shape[0]\n\n        self._context = InferredContext(\n            x_data,\n            y_data,\n            self.validation,\n        )\n\n        self.pinn_ds = PINNDataset(\n            x_data,\n            y_data,\n            self.coll,\n            config.batch_size,\n            config.data_ratio,\n        )\n\n        self.predict_ds = TensorDataset(\n            x_data,\n            y_data,\n        )\n\n        for callback in self.callbacks:\n            callback.on_after_setup(self)\n\n    @override\n    def train_dataloader(self) -&gt; DataLoader[TrainingBatch]:\n        \"\"\"\n        Returns the training dataloader using PINNDataset.\n        \"\"\"\n        return DataLoader[TrainingBatch](\n            self.pinn_ds,\n            batch_size=None,  # handled internally\n            num_workers=cpu_count() or 1,\n            persistent_workers=True,\n            pin_memory=True,\n        )\n\n    @override\n    def predict_dataloader(self) -&gt; DataLoader[PredictionBatch]:\n        \"\"\"\n        Returns the prediction dataloader using only the data dataset.\n        \"\"\"\n        return DataLoader[PredictionBatch](\n            cast(Dataset[PredictionBatch], self.predict_ds),\n            batch_size=self._data_size,\n            num_workers=cpu_count() or 1,\n            persistent_workers=True,\n            pin_memory=True,\n        )\n\n    @property\n    def context(self) -&gt; InferredContext:\n        if self._context is None:\n            raise RuntimeError(\"Context does not exist. Call setup() before accessing context.\")\n        return self._context\n</code></pre>"},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataModule.callbacks","title":"<code>callbacks: list[DataCallback] = list(callbacks) if callbacks else []</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataModule.context","title":"<code>context: InferredContext</code>  <code>property</code>","text":""},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataModule.hp","title":"<code>hp = hp</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataModule.__init__","title":"<code>__init__(hp: PINNHyperparameters, validation: ValidationRegistry | None = None, callbacks: Sequence[DataCallback] | None = None, residual_scorer: ResidualScorer | None = None) -&gt; None</code>","text":"Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>def __init__(\n    self,\n    hp: PINNHyperparameters,\n    validation: ValidationRegistry | None = None,\n    callbacks: Sequence[DataCallback] | None = None,\n    residual_scorer: ResidualScorer | None = None,\n) -&gt; None:\n    super().__init__()\n    self.hp = hp\n    self.callbacks: list[DataCallback] = list(callbacks) if callbacks else []\n    self._residual_scorer = residual_scorer\n\n    self._unresolved_validation = validation or {}\n    self._context: InferredContext | None = None\n</code></pre>"},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataModule.gen_data","title":"<code>gen_data(config: GenerationConfig) -&gt; DataBatch</code>  <code>abstractmethod</code>","text":"<p>Generate synthetic data from GenerationConfig.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>@abstractmethod\ndef gen_data(self, config: GenerationConfig) -&gt; DataBatch:\n    \"\"\"Generate synthetic data from GenerationConfig.\"\"\"\n</code></pre>"},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataModule.load_data","title":"<code>load_data(config: IngestionConfig) -&gt; DataBatch</code>","text":"<p>Load raw data from IngestionConfig.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>def load_data(self, config: IngestionConfig) -&gt; DataBatch:\n    \"\"\"Load raw data from IngestionConfig.\"\"\"\n    df = pd.read_csv(config.df_path)\n\n    if config.x_column is not None:\n        x_values = df[config.x_column].values\n\n        if config.x_transform is not None:\n            x_values = config.x_transform(x_values)\n\n        x = torch.tensor(x_values, dtype=torch.float32)\n    else:\n        x = torch.arange(len(df), dtype=torch.float32)\n\n    y = torch.tensor(df[config.y_columns].values, dtype=torch.float32)\n\n    if y.ndim == 1:\n        y = y.unsqueeze(-1)  # (N,) \u2192 (N, 1)\n    y = y.unsqueeze(-1)  # (N, k) \u2192 (N, k, 1) always\n\n    return x.unsqueeze(-1), y\n</code></pre>"},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataModule.predict_dataloader","title":"<code>predict_dataloader() -&gt; DataLoader[PredictionBatch]</code>","text":"<p>Returns the prediction dataloader using only the data dataset.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>@override\ndef predict_dataloader(self) -&gt; DataLoader[PredictionBatch]:\n    \"\"\"\n    Returns the prediction dataloader using only the data dataset.\n    \"\"\"\n    return DataLoader[PredictionBatch](\n        cast(Dataset[PredictionBatch], self.predict_ds),\n        batch_size=self._data_size,\n        num_workers=cpu_count() or 1,\n        persistent_workers=True,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataModule.setup","title":"<code>setup(stage: str | None = None) -&gt; None</code>","text":"<p>Load raw data from IngestionConfig, or generate synthetic data from GenerationConfig. Apply registered callbacks, create InferredContext and datasets.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>@override\ndef setup(self, stage: str | None = None) -&gt; None:\n    \"\"\"\n    Load raw data from IngestionConfig, or generate synthetic data from GenerationConfig.\n    Apply registered callbacks, create InferredContext and datasets.\n    \"\"\"\n    config = self.hp.training_data\n\n    self.validation = resolve_validation(\n        self._unresolved_validation,\n        config.df_path if isinstance(config, IngestionConfig) else None,\n    )\n\n    self.data = (\n        self.load_data(config)\n        if isinstance(config, IngestionConfig)\n        else self.gen_data(config)\n    )\n\n    domain = Domain.from_x(self.data[0])\n    self._domain = domain\n    self._sampler = self._build_sampler(config.collocation_sampler)\n    self.coll = self._sampler.sample(config.collocations, domain)\n\n    for callback in self.callbacks:\n        self.data, self.coll = callback.transform_data(self.data, self.coll)\n\n    x_data, y_data = self.data\n\n    if x_data.shape[0] != y_data.shape[0]:\n        raise ValueError(\n            f\"Size mismatch: x has {x_data.shape[0]} rows, y has {y_data.shape[0]} rows.\"\n        )\n    if x_data.ndim != 2 or x_data.shape[1] &lt; 1:\n        raise ValueError(f\"Expected x shape (n, d) with d &gt;= 1, got {tuple(x_data.shape)}.\")\n    if y_data.ndim &lt; 2 or y_data.shape[-1] != 1:\n        raise ValueError(f\"Expected y shape (n, ..., 1), got {tuple(y_data.shape)}.\")\n    if self.coll.ndim != 2 or self.coll.shape[1] &lt; 1:\n        raise ValueError(\n            f\"Expected coll shape (m, d) with d &gt;= 1, got {tuple(self.coll.shape)}.\"\n        )\n    if x_data.shape[1] != self.coll.shape[1]:\n        raise ValueError(\n            f\"Spatial dimension mismatch: x_data has d={x_data.shape[1]}, \"\n            f\"coll has d={self.coll.shape[1]}. Both must share the same number of dimensions.\"\n        )\n\n    self._data_size = x_data.shape[0]\n\n    self._context = InferredContext(\n        x_data,\n        y_data,\n        self.validation,\n    )\n\n    self.pinn_ds = PINNDataset(\n        x_data,\n        y_data,\n        self.coll,\n        config.batch_size,\n        config.data_ratio,\n    )\n\n    self.predict_ds = TensorDataset(\n        x_data,\n        y_data,\n    )\n\n    for callback in self.callbacks:\n        callback.on_after_setup(self)\n</code></pre>"},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataModule.train_dataloader","title":"<code>train_dataloader() -&gt; DataLoader[TrainingBatch]</code>","text":"<p>Returns the training dataloader using PINNDataset.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>@override\ndef train_dataloader(self) -&gt; DataLoader[TrainingBatch]:\n    \"\"\"\n    Returns the training dataloader using PINNDataset.\n    \"\"\"\n    return DataLoader[TrainingBatch](\n        self.pinn_ds,\n        batch_size=None,  # handled internally\n        num_workers=cpu_count() or 1,\n        persistent_workers=True,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataset","title":"<code>PINNDataset</code>","text":"<p>               Bases: <code>Dataset[TrainingBatch]</code></p> <p>Dataset used for PINN training. Combines labeled data and collocation points per sample.  Given a data_ratio, the amount of data points <code>K</code> is determined either by applying <code>data_ratio * batch_size</code> if ratio is a float between 0 and 1 or by an absolute count if ratio is an integer. The remaining <code>C</code> points are used for collocation.  The data points are sampled without replacement per epoch i.e. cycles through all data points and at the last batch, wraps around to the first indices to ensure batch size. The collocation points are sampled with replacement from the pool. The dataset produces a batch of shape ((x_data[K,d], y_data[K,...]), x_coll[C,d]).</p> <p>Parameters:</p> Name Type Description Default <code>x_data</code> <code>Tensor</code> <p>Data point x coordinates (time values).</p> required <code>y_data</code> <code>Tensor</code> <p>Data point y values (observations).</p> required <code>x_coll</code> <code>Tensor</code> <p>Collocation point x coordinates.</p> required <code>batch_size</code> <code>int</code> <p>Size of the batch.</p> required <code>data_ratio</code> <code>float | int</code> <p>Ratio of data points to collocation points, either as a ratio [0,1] or absolute count [0,batch_size].</p> required Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>class PINNDataset(Dataset[TrainingBatch]):\n    \"\"\"\n    Dataset used for PINN training. Combines labeled data and collocation points\n    per sample.  Given a data_ratio, the amount of data points `K` is determined\n    either by applying `data_ratio * batch_size` if ratio is a float between 0\n    and 1 or by an absolute count if ratio is an integer. The remaining `C`\n    points are used for collocation.  The data points are sampled without\n    replacement per epoch i.e. cycles through all data points and at the last\n    batch, wraps around to the first indices to ensure batch size. The collocation\n    points are sampled with replacement from the pool.\n    The dataset produces a batch of shape ((x_data[K,d], y_data[K,...]), x_coll[C,d]).\n\n    Args:\n        x_data: Data point x coordinates (time values).\n        y_data: Data point y values (observations).\n        x_coll: Collocation point x coordinates.\n        batch_size: Size of the batch.\n        data_ratio: Ratio of data points to collocation points, either as a ratio [0,1] or absolute\n            count [0,batch_size].\n    \"\"\"\n\n    def __init__(\n        self,\n        x_data: Tensor,\n        y_data: Tensor,\n        x_coll: Tensor,\n        batch_size: int,\n        data_ratio: float | int,\n    ):\n        super().__init__()\n        if batch_size &lt;= 0:\n            raise ValueError(f\"batch_size must be positive, got {batch_size}.\")\n\n        if isinstance(data_ratio, float):\n            if not (0.0 &lt;= data_ratio &lt;= 1.0):\n                raise ValueError(f\"Float data_ratio must be in [0.0, 1.0], got {data_ratio}.\")\n            self.K = round(data_ratio * batch_size)\n        else:\n            if not (0 &lt;= data_ratio &lt;= batch_size):\n                raise ValueError(\n                    f\"Integer data_ratio must be in [0, {batch_size}], got {data_ratio}.\"\n                )\n            self.K = data_ratio\n\n        self.x_data = x_data\n        self.y_data = y_data\n        self.x_coll = x_coll\n\n        self.batch_size = batch_size\n        self.C = batch_size - self.K\n\n        self.total_data = x_data.shape[0]\n        self.total_coll = x_coll.shape[0]\n\n        self._coll_gen = torch.Generator()\n\n    def __len__(self) -&gt; int:\n        \"\"\"Number of steps per epoch to see all data points once. Ceiling division.\"\"\"\n        return (self.total_data + self.K - 1) // self.K\n\n    @override\n    def __getitem__(self, index: int) -&gt; TrainingBatch:\n        \"\"\"Return one sample containing K data points and C collocation points.\"\"\"\n        data_idx = self._get_data_indices(index)\n        coll_idx = self._get_coll_indices(index)\n\n        x_data = self.x_data[data_idx]\n        y_data = self.y_data[data_idx]\n        x_coll = self.x_coll[coll_idx]\n\n        return ((x_data, y_data), x_coll)\n\n    def _get_data_indices(self, idx: int) -&gt; Tensor:\n        \"\"\"Get data indices for this step without replacement.\n        When getting the last batch, wrap around to the first indices to ensure batch size.\n        \"\"\"\n        if self.total_data == 0:\n            return torch.empty(0, 1)\n\n        start = idx * self.K\n        indices = [(start + i) % self.total_data for i in range(self.K)]\n        return torch.tensor(indices)\n\n    def _get_coll_indices(self, idx: int) -&gt; Tensor:\n        \"\"\"Get collocation indices for this step with replacement.\"\"\"\n        if self.total_coll == 0:\n            return torch.empty(0, 1)\n\n        self._coll_gen.manual_seed(idx)\n        return torch.randint(0, self.total_coll, (self.C,), generator=self._coll_gen)\n</code></pre>"},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataset.C","title":"<code>C = batch_size - self.K</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataset.K","title":"<code>K = round(data_ratio * batch_size)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataset.batch_size","title":"<code>batch_size = batch_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataset.total_coll","title":"<code>total_coll = x_coll.shape[0]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataset.total_data","title":"<code>total_data = x_data.shape[0]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataset.x_coll","title":"<code>x_coll = x_coll</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataset.x_data","title":"<code>x_data = x_data</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataset.y_data","title":"<code>y_data = y_data</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataset.__getitem__","title":"<code>__getitem__(index: int) -&gt; TrainingBatch</code>","text":"<p>Return one sample containing K data points and C collocation points.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>@override\ndef __getitem__(self, index: int) -&gt; TrainingBatch:\n    \"\"\"Return one sample containing K data points and C collocation points.\"\"\"\n    data_idx = self._get_data_indices(index)\n    coll_idx = self._get_coll_indices(index)\n\n    x_data = self.x_data[data_idx]\n    y_data = self.y_data[data_idx]\n    x_coll = self.x_coll[coll_idx]\n\n    return ((x_data, y_data), x_coll)\n</code></pre>"},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataset.__init__","title":"<code>__init__(x_data: Tensor, y_data: Tensor, x_coll: Tensor, batch_size: int, data_ratio: float | int)</code>","text":"Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>def __init__(\n    self,\n    x_data: Tensor,\n    y_data: Tensor,\n    x_coll: Tensor,\n    batch_size: int,\n    data_ratio: float | int,\n):\n    super().__init__()\n    if batch_size &lt;= 0:\n        raise ValueError(f\"batch_size must be positive, got {batch_size}.\")\n\n    if isinstance(data_ratio, float):\n        if not (0.0 &lt;= data_ratio &lt;= 1.0):\n            raise ValueError(f\"Float data_ratio must be in [0.0, 1.0], got {data_ratio}.\")\n        self.K = round(data_ratio * batch_size)\n    else:\n        if not (0 &lt;= data_ratio &lt;= batch_size):\n            raise ValueError(\n                f\"Integer data_ratio must be in [0, {batch_size}], got {data_ratio}.\"\n            )\n        self.K = data_ratio\n\n    self.x_data = x_data\n    self.y_data = y_data\n    self.x_coll = x_coll\n\n    self.batch_size = batch_size\n    self.C = batch_size - self.K\n\n    self.total_data = x_data.shape[0]\n    self.total_coll = x_coll.shape[0]\n\n    self._coll_gen = torch.Generator()\n</code></pre>"},{"location":"reference/anypinn/core/dataset/#anypinn.core.dataset.PINNDataset.__len__","title":"<code>__len__() -&gt; int</code>","text":"<p>Number of steps per epoch to see all data points once. Ceiling division.</p> Source code in <code>src/anypinn/core/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Number of steps per epoch to see all data points once. Ceiling division.\"\"\"\n    return (self.total_data + self.K - 1) // self.K\n</code></pre>"},{"location":"reference/anypinn/core/nn/","title":"nn","text":""},{"location":"reference/anypinn/core/nn/#anypinn.core.nn","title":"<code>anypinn.core.nn</code>","text":"<p>Neural network primitives and building blocks for PINN.</p>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.ArgsRegistry","title":"<code>ArgsRegistry: TypeAlias = dict[str, Argument]</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.FieldsRegistry","title":"<code>FieldsRegistry: TypeAlias = dict[str, Field]</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.ParamsRegistry","title":"<code>ParamsRegistry: TypeAlias = dict[str, Parameter]</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Argument","title":"<code>Argument</code>","text":"<p>Represents an argument that can be passed to an ODE/PDE function. Can be a fixed float value or a callable function.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float | Callable[[Tensor], Tensor]</code> <p>The value (float) or function (callable).</p> required Source code in <code>src/anypinn/core/nn.py</code> <pre><code>class Argument:\n    \"\"\"\n    Represents an argument that can be passed to an ODE/PDE function.\n    Can be a fixed float value or a callable function.\n\n    Args:\n        value: The value (float) or function (callable).\n    \"\"\"\n\n    def __init__(self, value: float | Callable[[Tensor], Tensor]):\n        self._value = value\n        self._tensor_cache: dict[torch.device, Tensor] = {}\n\n    def __call__(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Evaluate the argument.\n\n        Args:\n            x: Input tensor (context).\n\n        Returns:\n            The value of the argument, broadcasted if necessary.\n        \"\"\"\n        if callable(self._value):\n            return self._value(x)\n        device = x.device\n        if device not in self._tensor_cache:\n            self._tensor_cache[device] = torch.tensor(self._value, device=device)\n        return self._tensor_cache[device]\n\n    @override\n    def __repr__(self) -&gt; str:\n        return f\"Argument(value={self._value})\"\n</code></pre>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Argument.__call__","title":"<code>__call__(x: Tensor) -&gt; Tensor</code>","text":"<p>Evaluate the argument.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor (context).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The value of the argument, broadcasted if necessary.</p> Source code in <code>src/anypinn/core/nn.py</code> <pre><code>def __call__(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Evaluate the argument.\n\n    Args:\n        x: Input tensor (context).\n\n    Returns:\n        The value of the argument, broadcasted if necessary.\n    \"\"\"\n    if callable(self._value):\n        return self._value(x)\n    device = x.device\n    if device not in self._tensor_cache:\n        self._tensor_cache[device] = torch.tensor(self._value, device=device)\n    return self._tensor_cache[device]\n</code></pre>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Argument.__init__","title":"<code>__init__(value: float | Callable[[Tensor], Tensor])</code>","text":"Source code in <code>src/anypinn/core/nn.py</code> <pre><code>def __init__(self, value: float | Callable[[Tensor], Tensor]):\n    self._value = value\n    self._tensor_cache: dict[torch.device, Tensor] = {}\n</code></pre>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Argument.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"Source code in <code>src/anypinn/core/nn.py</code> <pre><code>@override\ndef __repr__(self) -&gt; str:\n    return f\"Argument(value={self._value})\"\n</code></pre>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Domain","title":"<code>Domain</code>  <code>dataclass</code>","text":"<p>N-dimensional rectangular domain.</p> <p>Attributes:</p> Name Type Description <code>bounds</code> <code>list[tuple[float, float]]</code> <p>Per-dimension (min, max) pairs. <code>bounds[i]</code> covers dimension i.</p> <code>dx</code> <code>list[float] | None</code> <p>Per-dimension step size (<code>None</code> when not applicable).</p> Source code in <code>src/anypinn/core/nn.py</code> <pre><code>@dataclass\nclass Domain:\n    \"\"\"\n    N-dimensional rectangular domain.\n\n    Attributes:\n        bounds: Per-dimension (min, max) pairs. ``bounds[i]`` covers dimension i.\n        dx: Per-dimension step size (``None`` when not applicable).\n    \"\"\"\n\n    bounds: list[tuple[float, float]]\n    dx: list[float] | None = None\n\n    @property\n    def ndim(self) -&gt; int:\n        \"\"\"Number of spatial dimensions.\"\"\"\n        return len(self.bounds)\n\n    @property\n    def x0(self) -&gt; float:\n        \"\"\"Lower bound of the first dimension (convenience for 1-D / time-axis access).\"\"\"\n        return self.bounds[0][0]\n\n    @property\n    def x1(self) -&gt; float:\n        \"\"\"Upper bound of the first dimension.\"\"\"\n        return self.bounds[0][1]\n\n    @classmethod\n    def from_x(cls, x: Tensor) -&gt; Domain:\n        \"\"\"\n        Infer domain bounds and step sizes from a coordinate tensor of shape (N, d).\n\n        Args:\n            x: Coordinate tensor of shape ``(N, d)``.\n\n        Returns:\n            Domain with bounds and dx inferred from the data.\n        \"\"\"\n        if x.ndim != 2:\n            raise ValueError(f\"Expected 2-D coordinate tensor (N, d), got shape {tuple(x.shape)}.\")\n        if x.shape[0] &lt; 2:\n            raise ValueError(\n                f\"At least two points are required to infer the domain, got {x.shape[0]}.\"\n            )\n\n        d = x.shape[1]\n        bounds = [(x[:, i].min().item(), x[:, i].max().item()) for i in range(d)]\n        dx = [(x[1, i] - x[0, i]).item() for i in range(d)]\n        return cls(bounds=bounds, dx=dx)\n\n    @override\n    def __repr__(self) -&gt; str:\n        return f\"Domain(ndim={self.ndim}, bounds={self.bounds}, dx={self.dx})\"\n</code></pre>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Domain.bounds","title":"<code>bounds: list[tuple[float, float]]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Domain.dx","title":"<code>dx: list[float] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Domain.ndim","title":"<code>ndim: int</code>  <code>property</code>","text":"<p>Number of spatial dimensions.</p>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Domain.x0","title":"<code>x0: float</code>  <code>property</code>","text":"<p>Lower bound of the first dimension (convenience for 1-D / time-axis access).</p>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Domain.x1","title":"<code>x1: float</code>  <code>property</code>","text":"<p>Upper bound of the first dimension.</p>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Domain.__init__","title":"<code>__init__(bounds: list[tuple[float, float]], dx: list[float] | None = None) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Domain.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"Source code in <code>src/anypinn/core/nn.py</code> <pre><code>@override\ndef __repr__(self) -&gt; str:\n    return f\"Domain(ndim={self.ndim}, bounds={self.bounds}, dx={self.dx})\"\n</code></pre>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Domain.from_x","title":"<code>from_x(x: Tensor) -&gt; Domain</code>  <code>classmethod</code>","text":"<p>Infer domain bounds and step sizes from a coordinate tensor of shape (N, d).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Coordinate tensor of shape <code>(N, d)</code>.</p> required <p>Returns:</p> Type Description <code>Domain</code> <p>Domain with bounds and dx inferred from the data.</p> Source code in <code>src/anypinn/core/nn.py</code> <pre><code>@classmethod\ndef from_x(cls, x: Tensor) -&gt; Domain:\n    \"\"\"\n    Infer domain bounds and step sizes from a coordinate tensor of shape (N, d).\n\n    Args:\n        x: Coordinate tensor of shape ``(N, d)``.\n\n    Returns:\n        Domain with bounds and dx inferred from the data.\n    \"\"\"\n    if x.ndim != 2:\n        raise ValueError(f\"Expected 2-D coordinate tensor (N, d), got shape {tuple(x.shape)}.\")\n    if x.shape[0] &lt; 2:\n        raise ValueError(\n            f\"At least two points are required to infer the domain, got {x.shape[0]}.\"\n        )\n\n    d = x.shape[1]\n    bounds = [(x[:, i].min().item(), x[:, i].max().item()) for i in range(d)]\n    dx = [(x[1, i] - x[0, i]).item() for i in range(d)]\n    return cls(bounds=bounds, dx=dx)\n</code></pre>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Field","title":"<code>Field</code>","text":"<p>               Bases: <code>Module</code></p> <p>A neural field mapping coordinates -&gt; vector of state variables. Example (ODE): t -&gt; [S, I, R].</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MLPConfig</code> <p>Configuration for the MLP backing this field.</p> required Source code in <code>src/anypinn/core/nn.py</code> <pre><code>class Field(nn.Module):\n    \"\"\"\n    A neural field mapping coordinates -&gt; vector of state variables.\n    Example (ODE): t -&gt; [S, I, R].\n\n    Args:\n        config: Configuration for the MLP backing this field.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: MLPConfig,\n    ):\n        super().__init__()\n        encode = config.encode\n        if isinstance(encode, nn.Module):\n            # registers \u2192 participates in .to(), .state_dict()\n            self.encoder: nn.Module | None = encode\n        else:\n            self.encoder = None\n        self._encode_fn = encode  # callable reference (module or plain fn)\n        dims = [config.in_dim] + config.hidden_layers + [config.out_dim]\n        act = get_activation(config.activation)\n\n        layers: list[nn.Module] = []\n        for i in range(len(dims) - 1):\n            layers.append(nn.Linear(dims[i], dims[i + 1]))\n            if i &lt; len(dims) - 2:\n                layers.append(act)\n\n        if config.output_activation is not None:\n            out_act = get_activation(config.output_activation)\n            layers.append(out_act)\n\n        self.net = nn.Sequential(*layers)\n        self.apply(self._init)\n\n    @staticmethod\n    def _init(m: nn.Module) -&gt; None:\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            nn.init.zeros_(m.bias)\n\n    @override\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Forward pass of the field.\n\n        Args:\n            x: Input coordinates (e.g. time, space).\n\n        Returns:\n            The values of the field at input coordinates.\n        \"\"\"\n        if self._encode_fn is not None:\n            x = self._encode_fn(x)\n        return cast(Tensor, self.net(x))\n</code></pre>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Field.encoder","title":"<code>encoder: nn.Module | None = encode</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Field.net","title":"<code>net = nn.Sequential(*layers)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Field.__init__","title":"<code>__init__(config: MLPConfig)</code>","text":"Source code in <code>src/anypinn/core/nn.py</code> <pre><code>def __init__(\n    self,\n    config: MLPConfig,\n):\n    super().__init__()\n    encode = config.encode\n    if isinstance(encode, nn.Module):\n        # registers \u2192 participates in .to(), .state_dict()\n        self.encoder: nn.Module | None = encode\n    else:\n        self.encoder = None\n    self._encode_fn = encode  # callable reference (module or plain fn)\n    dims = [config.in_dim] + config.hidden_layers + [config.out_dim]\n    act = get_activation(config.activation)\n\n    layers: list[nn.Module] = []\n    for i in range(len(dims) - 1):\n        layers.append(nn.Linear(dims[i], dims[i + 1]))\n        if i &lt; len(dims) - 2:\n            layers.append(act)\n\n    if config.output_activation is not None:\n        out_act = get_activation(config.output_activation)\n        layers.append(out_act)\n\n    self.net = nn.Sequential(*layers)\n    self.apply(self._init)\n</code></pre>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Field.forward","title":"<code>forward(x: Tensor) -&gt; Tensor</code>","text":"<p>Forward pass of the field.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input coordinates (e.g. time, space).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The values of the field at input coordinates.</p> Source code in <code>src/anypinn/core/nn.py</code> <pre><code>@override\ndef forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Forward pass of the field.\n\n    Args:\n        x: Input coordinates (e.g. time, space).\n\n    Returns:\n        The values of the field at input coordinates.\n    \"\"\"\n    if self._encode_fn is not None:\n        x = self._encode_fn(x)\n    return cast(Tensor, self.net(x))\n</code></pre>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Parameter","title":"<code>Parameter</code>","text":"<p>               Bases: <code>Module</code>, <code>Argument</code></p> <p>Learnable parameter. Supports scalar or function-valued parameter. For function-valued parameters (e.g. \u03b2(t)), uses a small MLP.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ScalarConfig | MLPConfig</code> <p>Configuration for the parameter (ScalarConfig or MLPConfig).</p> required Source code in <code>src/anypinn/core/nn.py</code> <pre><code>class Parameter(nn.Module, Argument):\n    \"\"\"\n    Learnable parameter. Supports scalar or function-valued parameter.\n    For function-valued parameters (e.g. \u03b2(t)), uses a small MLP.\n\n    Args:\n        config: Configuration for the parameter (ScalarConfig or MLPConfig).\n    \"\"\"\n\n    def __init__(\n        self,\n        config: ScalarConfig | MLPConfig,\n    ):\n        super().__init__()\n        self.config = config\n        self._mode: Literal[\"scalar\", \"mlp\"]\n\n        if isinstance(config, ScalarConfig):\n            self._mode = \"scalar\"\n            self.value = nn.Parameter(torch.tensor(float(config.init_value), dtype=torch.float32))\n\n        else:  # isinstance(config, MLPConfig)\n            self._mode = \"mlp\"\n            dims = [config.in_dim] + config.hidden_layers + [config.out_dim]\n            act = get_activation(config.activation)\n\n            layers: list[nn.Module] = []\n            for i in range(len(dims) - 1):\n                layers.append(nn.Linear(dims[i], dims[i + 1]))\n                if i &lt; len(dims) - 2:\n                    layers.append(act)\n\n            if config.output_activation is not None:\n                out_act = get_activation(config.output_activation)\n                layers.append(out_act)\n\n            self.net = nn.Sequential(*layers)\n            self.apply(self._init)\n\n    @property\n    def mode(self) -&gt; Literal[\"scalar\", \"mlp\"]:\n        \"\"\"Mode of the parameter: 'scalar' or 'mlp'.\"\"\"\n        return self._mode\n\n    @staticmethod\n    def _init(m: nn.Module) -&gt; None:\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            nn.init.zeros_(m.bias)\n\n    @override\n    def forward(self, x: Tensor | None = None) -&gt; Tensor:\n        \"\"\"\n        Get the value of the parameter.\n\n        Args:\n            x: Input tensor (required for 'mlp' mode).\n\n        Returns:\n            The parameter value.\n        \"\"\"\n        if self.mode == \"scalar\":\n            return self.value if x is None else self.value.expand_as(x)\n        else:\n            if x is None:\n                raise TypeError(\"Function-valued parameter requires input.\")\n            return cast(Tensor, self.net(x))\n</code></pre>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Parameter.config","title":"<code>config = config</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Parameter.mode","title":"<code>mode: Literal['scalar', 'mlp']</code>  <code>property</code>","text":"<p>Mode of the parameter: 'scalar' or 'mlp'.</p>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Parameter.net","title":"<code>net = nn.Sequential(*layers)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Parameter.value","title":"<code>value = nn.Parameter(torch.tensor(float(config.init_value), dtype=(torch.float32)))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Parameter.__init__","title":"<code>__init__(config: ScalarConfig | MLPConfig)</code>","text":"Source code in <code>src/anypinn/core/nn.py</code> <pre><code>def __init__(\n    self,\n    config: ScalarConfig | MLPConfig,\n):\n    super().__init__()\n    self.config = config\n    self._mode: Literal[\"scalar\", \"mlp\"]\n\n    if isinstance(config, ScalarConfig):\n        self._mode = \"scalar\"\n        self.value = nn.Parameter(torch.tensor(float(config.init_value), dtype=torch.float32))\n\n    else:  # isinstance(config, MLPConfig)\n        self._mode = \"mlp\"\n        dims = [config.in_dim] + config.hidden_layers + [config.out_dim]\n        act = get_activation(config.activation)\n\n        layers: list[nn.Module] = []\n        for i in range(len(dims) - 1):\n            layers.append(nn.Linear(dims[i], dims[i + 1]))\n            if i &lt; len(dims) - 2:\n                layers.append(act)\n\n        if config.output_activation is not None:\n            out_act = get_activation(config.output_activation)\n            layers.append(out_act)\n\n        self.net = nn.Sequential(*layers)\n        self.apply(self._init)\n</code></pre>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.Parameter.forward","title":"<code>forward(x: Tensor | None = None) -&gt; Tensor</code>","text":"<p>Get the value of the parameter.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor | None</code> <p>Input tensor (required for 'mlp' mode).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The parameter value.</p> Source code in <code>src/anypinn/core/nn.py</code> <pre><code>@override\ndef forward(self, x: Tensor | None = None) -&gt; Tensor:\n    \"\"\"\n    Get the value of the parameter.\n\n    Args:\n        x: Input tensor (required for 'mlp' mode).\n\n    Returns:\n        The parameter value.\n    \"\"\"\n    if self.mode == \"scalar\":\n        return self.value if x is None else self.value.expand_as(x)\n    else:\n        if x is None:\n            raise TypeError(\"Function-valued parameter requires input.\")\n        return cast(Tensor, self.net(x))\n</code></pre>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.build_criterion","title":"<code>build_criterion(name: Criteria) -&gt; nn.Module</code>","text":"<p>Return the loss-criterion module for the given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Criteria</code> <p>One of <code>\"mse\"</code>, <code>\"huber\"</code>, <code>\"l1\"</code>.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The corresponding PyTorch loss module.</p> Source code in <code>src/anypinn/core/nn.py</code> <pre><code>def build_criterion(name: Criteria) -&gt; nn.Module:\n    \"\"\"\n    Return the loss-criterion module for the given name.\n\n    Args:\n        name: One of ``\"mse\"``, ``\"huber\"``, ``\"l1\"``.\n\n    Returns:\n        The corresponding PyTorch loss module.\n    \"\"\"\n    return {\n        \"mse\": nn.MSELoss(),\n        \"huber\": nn.HuberLoss(),\n        \"l1\": nn.L1Loss(),\n    }[name]\n</code></pre>"},{"location":"reference/anypinn/core/nn/#anypinn.core.nn.get_activation","title":"<code>get_activation(name: Activations) -&gt; nn.Module</code>","text":"<p>Get the activation function module by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Activations</code> <p>The name of the activation function.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The PyTorch activation module.</p> Source code in <code>src/anypinn/core/nn.py</code> <pre><code>def get_activation(name: Activations) -&gt; nn.Module:\n    \"\"\"\n    Get the activation function module by name.\n\n    Args:\n        name: The name of the activation function.\n\n    Returns:\n        The PyTorch activation module.\n    \"\"\"\n    return {\n        \"tanh\": nn.Tanh(),\n        \"relu\": nn.ReLU(),\n        \"leaky_relu\": nn.LeakyReLU(),\n        \"sigmoid\": nn.Sigmoid(),\n        \"selu\": nn.SELU(),\n        \"softplus\": nn.Softplus(),\n        \"identity\": nn.Identity(),\n    }[name]\n</code></pre>"},{"location":"reference/anypinn/core/problem/","title":"problem","text":""},{"location":"reference/anypinn/core/problem/#anypinn.core.problem","title":"<code>anypinn.core.problem</code>","text":"<p>Core problem abstractions for PINN.</p>"},{"location":"reference/anypinn/core/problem/#anypinn.core.problem.Constraint","title":"<code>Constraint</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for a constraint (loss term) in the PINN. Returns a loss value for the given batch.</p> Source code in <code>src/anypinn/core/problem.py</code> <pre><code>class Constraint(ABC):\n    \"\"\"\n    Abstract base class for a constraint (loss term) in the PINN.\n    Returns a loss value for the given batch.\n    \"\"\"\n\n    def inject_context(self, context: InferredContext) -&gt; None:\n        \"\"\"\n        Inject the context into the constraint. This can be used by the constraint to access the\n        data used to compute the loss.\n\n        Args:\n            context: The context to inject.\n        \"\"\"\n        return None\n\n    @abstractmethod\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        \"\"\"\n        Calculate the loss for this constraint.\n\n        Args:\n            batch: The current batch of data/collocation points.\n            criterion: The loss function (e.g. MSE).\n            log: Optional logging function.\n\n        Returns:\n            The calculated loss tensor.\n        \"\"\"\n</code></pre>"},{"location":"reference/anypinn/core/problem/#anypinn.core.problem.Constraint.inject_context","title":"<code>inject_context(context: InferredContext) -&gt; None</code>","text":"<p>Inject the context into the constraint. This can be used by the constraint to access the data used to compute the loss.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>InferredContext</code> <p>The context to inject.</p> required Source code in <code>src/anypinn/core/problem.py</code> <pre><code>def inject_context(self, context: InferredContext) -&gt; None:\n    \"\"\"\n    Inject the context into the constraint. This can be used by the constraint to access the\n    data used to compute the loss.\n\n    Args:\n        context: The context to inject.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/anypinn/core/problem/#anypinn.core.problem.Constraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>  <code>abstractmethod</code>","text":"<p>Calculate the loss for this constraint.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>TrainingBatch</code> <p>The current batch of data/collocation points.</p> required <code>criterion</code> <code>Module</code> <p>The loss function (e.g. MSE).</p> required <code>log</code> <code>LogFn | None</code> <p>Optional logging function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The calculated loss tensor.</p> Source code in <code>src/anypinn/core/problem.py</code> <pre><code>@abstractmethod\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    \"\"\"\n    Calculate the loss for this constraint.\n\n    Args:\n        batch: The current batch of data/collocation points.\n        criterion: The loss function (e.g. MSE).\n        log: Optional logging function.\n\n    Returns:\n        The calculated loss tensor.\n    \"\"\"\n</code></pre>"},{"location":"reference/anypinn/core/problem/#anypinn.core.problem.Problem","title":"<code>Problem</code>","text":"<p>               Bases: <code>Module</code></p> <p>Aggregates operator residuals and constraints into total loss. Manages fields, parameters, constraints, and validation.</p> <p>Parameters:</p> Name Type Description Default <code>constraints</code> <code>list[Constraint]</code> <p>List of constraints to enforce.</p> required <code>criterion</code> <code>Module</code> <p>Loss function module.</p> required <code>fields</code> <code>FieldsRegistry</code> <p>List of fields (neural networks) to solve for.</p> required <code>params</code> <code>ParamsRegistry</code> <p>List of learnable parameters.</p> required Source code in <code>src/anypinn/core/problem.py</code> <pre><code>class Problem(nn.Module):\n    \"\"\"\n    Aggregates operator residuals and constraints into total loss.\n    Manages fields, parameters, constraints, and validation.\n\n    Args:\n        constraints: List of constraints to enforce.\n        criterion: Loss function module.\n        fields: List of fields (neural networks) to solve for.\n        params: List of learnable parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        constraints: list[Constraint],\n        criterion: nn.Module,\n        fields: FieldsRegistry,\n        params: ParamsRegistry,\n    ):\n        super().__init__()\n        self.constraints = constraints\n        self.criterion = criterion\n        self.fields = fields\n        self.params = params\n\n        self._fields = nn.ModuleList(fields.values())\n        self._params = nn.ModuleList(params.values())\n\n    def inject_context(self, context: InferredContext) -&gt; None:\n        \"\"\"\n        Inject the context into the problem.\n\n        This should be called after data is loaded but before training starts.\n        Pure function entries are passed through unchanged.\n\n        Args:\n            context: The context to inject.\n        \"\"\"\n        self.context = context\n        for c in self.constraints:\n            c.inject_context(context)\n\n    def training_loss(self, batch: TrainingBatch, log: LogFn | None = None) -&gt; Tensor:\n        \"\"\"\n        Calculate the total loss from all constraints.\n\n        Args:\n            batch: Current batch.\n            log: Optional logging function.\n\n        Returns:\n            Sum of losses from all constraints.\n        \"\"\"\n        _, x_coll = batch\n\n        if not self.constraints:\n            total = torch.tensor(0.0, device=x_coll.device)\n        else:\n            losses = iter(self.constraints)\n            total = next(losses).loss(batch, self.criterion, log)\n            for c in losses:\n                total = total + c.loss(batch, self.criterion, log)\n\n        if log is not None:\n            for name, param in self.params.items():\n                param_loss = self._param_validation_loss(name, param, x_coll)\n                if param_loss is not None:\n                    log(f\"loss/{name}\", param_loss, progress_bar=True)\n\n            log(LOSS_KEY, total, progress_bar=True)\n\n        return total\n\n    def predict(self, batch: DataBatch) -&gt; tuple[DataBatch, dict[str, Tensor]]:\n        \"\"\"\n        Generate predictions for a given batch of data.\n        Returns unscaled predictions in original domain.\n\n        Args:\n            batch: Batch of input coordinates.\n\n        Returns:\n            Tuple of (original_batch, predictions_dict).\n        \"\"\"\n\n        x, y = batch\n\n        n = x.shape[0]\n        preds = {name: f(x).reshape(n, -1).squeeze(-1) for name, f in self.fields.items()}\n        preds |= {name: p(x).reshape(n, -1).squeeze(-1) for name, p in self.params.items()}\n\n        return (x.squeeze(-1), y.squeeze(-1)), preds\n\n    def true_values(self, x: Tensor) -&gt; dict[str, Tensor] | None:\n        \"\"\"\n        Get the true values for a given x coordinates.\n        Returns None if no validation source is configured.\n        \"\"\"\n\n        return {\n            name: p_true.reshape(x.shape[0], -1).squeeze(-1)\n            for name, p in self.params.items()\n            if (p_true := self._get_true_param(name, x)) is not None\n        } or None\n\n    def _get_true_param(self, param_name: str, x: Tensor) -&gt; Tensor | None:\n        \"\"\"\n        Get the ground truth values for a parameter at given coordinates.\n\n        Args:\n            param_name: Name of the parameter.\n            x: Input coordinates.\n\n        Returns:\n            Ground truth values, or None if no validation source is configured.\n        \"\"\"\n        if param_name not in self.context.validation:\n            return None\n\n        fn = self.context.validation[param_name]\n\n        if isinstance(fn, _ColumnLookup):\n            domain = self.context.domain\n            if domain.dx is None:\n                raise ValueError(\n                    f\"Cannot perform ColumnRef lookup for '{param_name}': \"\n                    \"domain step size (dx) is unknown. Ensure the domain was inferred from \"\n                    \"a uniformly-spaced coordinate tensor, or use a callable validation source.\"\n                )\n            x_idx = ((x.squeeze(-1) - domain.x0) / domain.dx[0]).round().unsqueeze(-1)\n            return fn(x_idx)\n\n        return fn(x)\n\n    @torch.no_grad()\n    def _param_validation_loss(\n        self, param_name: str, param: Parameter, x_coll: Tensor\n    ) -&gt; Tensor | None:\n        \"\"\"\n        Compute validation loss for a parameter against ground truth.\n\n        Args:\n            param: The parameter to compute validation loss for.\n            x_coll: The input coordinates.\n\n        Returns:\n            Loss value, or None if no validation source is configured.\n        \"\"\"\n        true = self._get_true_param(param_name, x_coll)\n        if true is None:\n            return None\n\n        pred = param(x_coll)\n\n        return torch.mean((true - pred) ** 2)\n</code></pre>"},{"location":"reference/anypinn/core/problem/#anypinn.core.problem.Problem.constraints","title":"<code>constraints = constraints</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/problem/#anypinn.core.problem.Problem.criterion","title":"<code>criterion = criterion</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/problem/#anypinn.core.problem.Problem.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/problem/#anypinn.core.problem.Problem.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/problem/#anypinn.core.problem.Problem.__init__","title":"<code>__init__(constraints: list[Constraint], criterion: nn.Module, fields: FieldsRegistry, params: ParamsRegistry)</code>","text":"Source code in <code>src/anypinn/core/problem.py</code> <pre><code>def __init__(\n    self,\n    constraints: list[Constraint],\n    criterion: nn.Module,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n):\n    super().__init__()\n    self.constraints = constraints\n    self.criterion = criterion\n    self.fields = fields\n    self.params = params\n\n    self._fields = nn.ModuleList(fields.values())\n    self._params = nn.ModuleList(params.values())\n</code></pre>"},{"location":"reference/anypinn/core/problem/#anypinn.core.problem.Problem.inject_context","title":"<code>inject_context(context: InferredContext) -&gt; None</code>","text":"<p>Inject the context into the problem.</p> <p>This should be called after data is loaded but before training starts. Pure function entries are passed through unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>InferredContext</code> <p>The context to inject.</p> required Source code in <code>src/anypinn/core/problem.py</code> <pre><code>def inject_context(self, context: InferredContext) -&gt; None:\n    \"\"\"\n    Inject the context into the problem.\n\n    This should be called after data is loaded but before training starts.\n    Pure function entries are passed through unchanged.\n\n    Args:\n        context: The context to inject.\n    \"\"\"\n    self.context = context\n    for c in self.constraints:\n        c.inject_context(context)\n</code></pre>"},{"location":"reference/anypinn/core/problem/#anypinn.core.problem.Problem.predict","title":"<code>predict(batch: DataBatch) -&gt; tuple[DataBatch, dict[str, Tensor]]</code>","text":"<p>Generate predictions for a given batch of data. Returns unscaled predictions in original domain.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>DataBatch</code> <p>Batch of input coordinates.</p> required <p>Returns:</p> Type Description <code>tuple[DataBatch, dict[str, Tensor]]</code> <p>Tuple of (original_batch, predictions_dict).</p> Source code in <code>src/anypinn/core/problem.py</code> <pre><code>def predict(self, batch: DataBatch) -&gt; tuple[DataBatch, dict[str, Tensor]]:\n    \"\"\"\n    Generate predictions for a given batch of data.\n    Returns unscaled predictions in original domain.\n\n    Args:\n        batch: Batch of input coordinates.\n\n    Returns:\n        Tuple of (original_batch, predictions_dict).\n    \"\"\"\n\n    x, y = batch\n\n    n = x.shape[0]\n    preds = {name: f(x).reshape(n, -1).squeeze(-1) for name, f in self.fields.items()}\n    preds |= {name: p(x).reshape(n, -1).squeeze(-1) for name, p in self.params.items()}\n\n    return (x.squeeze(-1), y.squeeze(-1)), preds\n</code></pre>"},{"location":"reference/anypinn/core/problem/#anypinn.core.problem.Problem.training_loss","title":"<code>training_loss(batch: TrainingBatch, log: LogFn | None = None) -&gt; Tensor</code>","text":"<p>Calculate the total loss from all constraints.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>TrainingBatch</code> <p>Current batch.</p> required <code>log</code> <code>LogFn | None</code> <p>Optional logging function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Sum of losses from all constraints.</p> Source code in <code>src/anypinn/core/problem.py</code> <pre><code>def training_loss(self, batch: TrainingBatch, log: LogFn | None = None) -&gt; Tensor:\n    \"\"\"\n    Calculate the total loss from all constraints.\n\n    Args:\n        batch: Current batch.\n        log: Optional logging function.\n\n    Returns:\n        Sum of losses from all constraints.\n    \"\"\"\n    _, x_coll = batch\n\n    if not self.constraints:\n        total = torch.tensor(0.0, device=x_coll.device)\n    else:\n        losses = iter(self.constraints)\n        total = next(losses).loss(batch, self.criterion, log)\n        for c in losses:\n            total = total + c.loss(batch, self.criterion, log)\n\n    if log is not None:\n        for name, param in self.params.items():\n            param_loss = self._param_validation_loss(name, param, x_coll)\n            if param_loss is not None:\n                log(f\"loss/{name}\", param_loss, progress_bar=True)\n\n        log(LOSS_KEY, total, progress_bar=True)\n\n    return total\n</code></pre>"},{"location":"reference/anypinn/core/problem/#anypinn.core.problem.Problem.true_values","title":"<code>true_values(x: Tensor) -&gt; dict[str, Tensor] | None</code>","text":"<p>Get the true values for a given x coordinates. Returns None if no validation source is configured.</p> Source code in <code>src/anypinn/core/problem.py</code> <pre><code>def true_values(self, x: Tensor) -&gt; dict[str, Tensor] | None:\n    \"\"\"\n    Get the true values for a given x coordinates.\n    Returns None if no validation source is configured.\n    \"\"\"\n\n    return {\n        name: p_true.reshape(x.shape[0], -1).squeeze(-1)\n        for name, p in self.params.items()\n        if (p_true := self._get_true_param(name, x)) is not None\n    } or None\n</code></pre>"},{"location":"reference/anypinn/core/samplers/","title":"samplers","text":""},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers","title":"<code>anypinn.core.samplers</code>","text":"<p>Collocation point sampling strategies for PINN training.</p>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.AdaptiveSampler","title":"<code>AdaptiveSampler</code>","text":"<p>Residual-weighted adaptive collocation sampler.</p> <p>Draws an oversample of candidate points, scores them using a <code>ResidualScorer</code>, and retains the top-scoring subset. A configurable <code>exploration_ratio</code> ensures a fraction of purely random points to prevent mode collapse.</p> <p>Parameters:</p> Name Type Description Default <code>scorer</code> <code>ResidualScorer</code> <p>Callable returning per-point residual scores <code>(n,)</code>.</p> required <code>oversample_factor</code> <code>int</code> <p>Multiplier on <code>n</code> for candidate generation.</p> <code>4</code> <code>exploration_ratio</code> <code>float</code> <p>Fraction of the budget reserved for random points.</p> <code>0.2</code> <code>seed</code> <code>int | None</code> <p>Optional seed for reproducible sampling.</p> <code>None</code> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>class AdaptiveSampler:\n    \"\"\"Residual-weighted adaptive collocation sampler.\n\n    Draws an oversample of candidate points, scores them using a\n    ``ResidualScorer``, and retains the top-scoring subset. A configurable\n    ``exploration_ratio`` ensures a fraction of purely random points to prevent\n    mode collapse.\n\n    Args:\n        scorer: Callable returning per-point residual scores ``(n,)``.\n        oversample_factor: Multiplier on ``n`` for candidate generation.\n        exploration_ratio: Fraction of the budget reserved for random points.\n        seed: Optional seed for reproducible sampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        scorer: ResidualScorer,\n        oversample_factor: int = 4,\n        exploration_ratio: float = 0.2,\n        seed: int | None = None,\n    ) -&gt; None:\n        if oversample_factor &lt; 1:\n            raise ValueError(f\"oversample_factor must be &gt;= 1, got {oversample_factor}.\")\n        if not (0.0 &lt;= exploration_ratio &lt;= 1.0):\n            raise ValueError(f\"exploration_ratio must be in [0, 1], got {exploration_ratio}.\")\n        self._scorer = scorer\n        self._oversample = oversample_factor\n        self._explore = exploration_ratio\n        self._random = RandomSampler(seed=seed)\n\n    def sample(self, n: int, domain: Domain) -&gt; Tensor:\n        n_explore = max(1, int(n * self._explore))\n        n_exploit = n - n_explore\n\n        explore_pts = self._random.sample(n_explore, domain)\n\n        if n_exploit &lt;= 0:\n            return explore_pts\n\n        n_candidates = n_exploit * self._oversample\n        candidates = self._random.sample(n_candidates, domain)\n\n        with torch.no_grad():\n            scores = self._scorer.residual_score(candidates)\n\n        _, top_idx = scores.topk(min(n_exploit, len(scores)))\n        exploit_pts = candidates[top_idx]\n\n        return torch.cat([explore_pts, exploit_pts], dim=0)\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.AdaptiveSampler.__init__","title":"<code>__init__(scorer: ResidualScorer, oversample_factor: int = 4, exploration_ratio: float = 0.2, seed: int | None = None) -&gt; None</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def __init__(\n    self,\n    scorer: ResidualScorer,\n    oversample_factor: int = 4,\n    exploration_ratio: float = 0.2,\n    seed: int | None = None,\n) -&gt; None:\n    if oversample_factor &lt; 1:\n        raise ValueError(f\"oversample_factor must be &gt;= 1, got {oversample_factor}.\")\n    if not (0.0 &lt;= exploration_ratio &lt;= 1.0):\n        raise ValueError(f\"exploration_ratio must be in [0, 1], got {exploration_ratio}.\")\n    self._scorer = scorer\n    self._oversample = oversample_factor\n    self._explore = exploration_ratio\n    self._random = RandomSampler(seed=seed)\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.AdaptiveSampler.sample","title":"<code>sample(n: int, domain: Domain) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def sample(self, n: int, domain: Domain) -&gt; Tensor:\n    n_explore = max(1, int(n * self._explore))\n    n_exploit = n - n_explore\n\n    explore_pts = self._random.sample(n_explore, domain)\n\n    if n_exploit &lt;= 0:\n        return explore_pts\n\n    n_candidates = n_exploit * self._oversample\n    candidates = self._random.sample(n_candidates, domain)\n\n    with torch.no_grad():\n        scores = self._scorer.residual_score(candidates)\n\n    _, top_idx = scores.topk(min(n_exploit, len(scores)))\n    exploit_pts = candidates[top_idx]\n\n    return torch.cat([explore_pts, exploit_pts], dim=0)\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.CollocationSampler","title":"<code>CollocationSampler</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for collocation point samplers.</p> <p>Implementations must return a tensor of shape <code>(n, domain.ndim)</code> with all points inside the domain bounds.</p> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>class CollocationSampler(Protocol):\n    \"\"\"Protocol for collocation point samplers.\n\n    Implementations must return a tensor of shape ``(n, domain.ndim)`` with all\n    points inside the domain bounds.\n    \"\"\"\n\n    def sample(self, n: int, domain: Domain) -&gt; Tensor: ...\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.CollocationSampler.sample","title":"<code>sample(n: int, domain: Domain) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def sample(self, n: int, domain: Domain) -&gt; Tensor: ...\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.LatinHypercubeSampler","title":"<code>LatinHypercubeSampler</code>","text":"<p>Latin Hypercube sampler (pure-PyTorch, no SciPy dependency).</p> <p>Stratifies each dimension into <code>n</code> equal intervals and places one sample per interval, then shuffles columns independently.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Optional seed for reproducible sampling.</p> <code>None</code> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>class LatinHypercubeSampler:\n    \"\"\"Latin Hypercube sampler (pure-PyTorch, no SciPy dependency).\n\n    Stratifies each dimension into ``n`` equal intervals and places one sample\n    per interval, then shuffles columns independently.\n\n    Args:\n        seed: Optional seed for reproducible sampling.\n    \"\"\"\n\n    def __init__(self, seed: int | None = None) -&gt; None:\n        self._gen = torch.Generator()\n        if seed is not None:\n            self._gen.manual_seed(seed)\n\n    def sample(self, n: int, domain: Domain) -&gt; Tensor:\n        d = domain.ndim\n        result = torch.empty(n, d)\n\n        for i, (lo, hi) in enumerate(domain.bounds):\n            perm = torch.randperm(n, generator=self._gen)\n            base = (perm.float() + torch.rand(n, generator=self._gen)) / n\n            result[:, i] = base * (hi - lo) + lo\n\n        return result\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.LatinHypercubeSampler.__init__","title":"<code>__init__(seed: int | None = None) -&gt; None</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def __init__(self, seed: int | None = None) -&gt; None:\n    self._gen = torch.Generator()\n    if seed is not None:\n        self._gen.manual_seed(seed)\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.LatinHypercubeSampler.sample","title":"<code>sample(n: int, domain: Domain) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def sample(self, n: int, domain: Domain) -&gt; Tensor:\n    d = domain.ndim\n    result = torch.empty(n, d)\n\n    for i, (lo, hi) in enumerate(domain.bounds):\n        perm = torch.randperm(n, generator=self._gen)\n        base = (perm.float() + torch.rand(n, generator=self._gen)) / n\n        result[:, i] = base * (hi - lo) + lo\n\n    return result\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.LogUniform1DSampler","title":"<code>LogUniform1DSampler</code>","text":"<p>Log-uniform sampler for 1-D domains (reproduces SIR collocation behavior).</p> <p>Samples uniformly in <code>log1p</code> space and maps back via <code>expm1</code>, producing a distribution that is denser near the lower bound \u2014 useful for epidemic models where early dynamics are most informative.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Optional seed for reproducible sampling.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the domain is not 1-D or <code>x0 &lt;= -1</code>.</p> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>class LogUniform1DSampler:\n    \"\"\"Log-uniform sampler for 1-D domains (reproduces SIR collocation behavior).\n\n    Samples uniformly in ``log1p`` space and maps back via ``expm1``, producing\n    a distribution that is denser near the lower bound \u2014 useful for epidemic\n    models where early dynamics are most informative.\n\n    Args:\n        seed: Optional seed for reproducible sampling.\n\n    Raises:\n        ValueError: If the domain is not 1-D or ``x0 &lt;= -1``.\n    \"\"\"\n\n    def __init__(self, seed: int | None = None) -&gt; None:\n        self._gen = torch.Generator()\n        if seed is not None:\n            self._gen.manual_seed(seed)\n\n    def sample(self, n: int, domain: Domain) -&gt; Tensor:\n        if domain.ndim != 1:\n            raise ValueError(\n                f\"log_uniform_1d sampler supports only 1-D domains, got ndim={domain.ndim}.\"\n            )\n        x0, x1 = domain.x0, domain.x1\n        if x0 &lt;= -1.0:\n            raise ValueError(f\"log_uniform_1d requires x0 &gt; -1 for log1p, got x0={x0}.\")\n        log_lo = torch.tensor(x0, dtype=torch.float32).log1p()\n        log_hi = torch.tensor(x1, dtype=torch.float32).log1p()\n        u = torch.rand((n, 1), generator=self._gen)\n        return torch.expm1(u * (log_hi - log_lo) + log_lo)\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.LogUniform1DSampler.__init__","title":"<code>__init__(seed: int | None = None) -&gt; None</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def __init__(self, seed: int | None = None) -&gt; None:\n    self._gen = torch.Generator()\n    if seed is not None:\n        self._gen.manual_seed(seed)\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.LogUniform1DSampler.sample","title":"<code>sample(n: int, domain: Domain) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def sample(self, n: int, domain: Domain) -&gt; Tensor:\n    if domain.ndim != 1:\n        raise ValueError(\n            f\"log_uniform_1d sampler supports only 1-D domains, got ndim={domain.ndim}.\"\n        )\n    x0, x1 = domain.x0, domain.x1\n    if x0 &lt;= -1.0:\n        raise ValueError(f\"log_uniform_1d requires x0 &gt; -1 for log1p, got x0={x0}.\")\n    log_lo = torch.tensor(x0, dtype=torch.float32).log1p()\n    log_hi = torch.tensor(x1, dtype=torch.float32).log1p()\n    u = torch.rand((n, 1), generator=self._gen)\n    return torch.expm1(u * (log_hi - log_lo) + log_lo)\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.RandomSampler","title":"<code>RandomSampler</code>","text":"<p>Uniform random sampler inside domain bounds.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Optional seed for reproducible sampling.</p> <code>None</code> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>class RandomSampler:\n    \"\"\"Uniform random sampler inside domain bounds.\n\n    Args:\n        seed: Optional seed for reproducible sampling.\n    \"\"\"\n\n    def __init__(self, seed: int | None = None) -&gt; None:\n        self._gen = torch.Generator()\n        if seed is not None:\n            self._gen.manual_seed(seed)\n\n    def sample(self, n: int, domain: Domain) -&gt; Tensor:\n        d = domain.ndim\n        u = torch.rand((n, d), generator=self._gen)\n        for i, (lo, hi) in enumerate(domain.bounds):\n            u[:, i] = u[:, i] * (hi - lo) + lo\n        return u\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.RandomSampler.__init__","title":"<code>__init__(seed: int | None = None) -&gt; None</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def __init__(self, seed: int | None = None) -&gt; None:\n    self._gen = torch.Generator()\n    if seed is not None:\n        self._gen.manual_seed(seed)\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.RandomSampler.sample","title":"<code>sample(n: int, domain: Domain) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def sample(self, n: int, domain: Domain) -&gt; Tensor:\n    d = domain.ndim\n    u = torch.rand((n, d), generator=self._gen)\n    for i, (lo, hi) in enumerate(domain.bounds):\n        u[:, i] = u[:, i] * (hi - lo) + lo\n    return u\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.ResidualScorer","title":"<code>ResidualScorer</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for scoring candidate collocation points by PDE residual magnitude.</p> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>class ResidualScorer(Protocol):\n    \"\"\"Protocol for scoring candidate collocation points by PDE residual magnitude.\"\"\"\n\n    def residual_score(self, x: Tensor) -&gt; Tensor:\n        \"\"\"Return per-point non-negative residual score of shape ``(n,)``.\n\n        Args:\n            x: Candidate collocation points ``(n, d)``.\n\n        Returns:\n            Scores ``(n,)`` \u2014 higher means larger residual.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.ResidualScorer.residual_score","title":"<code>residual_score(x: Tensor) -&gt; Tensor</code>","text":"<p>Return per-point non-negative residual score of shape <code>(n,)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Candidate collocation points <code>(n, d)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Scores <code>(n,)</code> \u2014 higher means larger residual.</p> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def residual_score(self, x: Tensor) -&gt; Tensor:\n    \"\"\"Return per-point non-negative residual score of shape ``(n,)``.\n\n    Args:\n        x: Candidate collocation points ``(n, d)``.\n\n    Returns:\n        Scores ``(n,)`` \u2014 higher means larger residual.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.UniformSampler","title":"<code>UniformSampler</code>","text":"<p>Cartesian grid sampler that distributes points evenly across the domain.</p> <p>For d-dimensional domains, places <code>ceil(n^(1/d))</code> points per axis then takes the first <code>n</code> points of the resulting grid.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Optional seed (unused \u2014 grid is deterministic).</p> <code>None</code> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>class UniformSampler:\n    \"\"\"Cartesian grid sampler that distributes points evenly across the domain.\n\n    For d-dimensional domains, places ``ceil(n^(1/d))`` points per axis then\n    takes the first ``n`` points of the resulting grid.\n\n    Args:\n        seed: Optional seed (unused \u2014 grid is deterministic).\n    \"\"\"\n\n    def __init__(self, seed: int | None = None) -&gt; None:\n        pass\n\n    def sample(self, n: int, domain: Domain) -&gt; Tensor:\n        d = domain.ndim\n        pts_per_dim = math.ceil(n ** (1.0 / d))\n\n        linspaces = [torch.linspace(lo, hi, pts_per_dim) for lo, hi in domain.bounds]\n        grids = torch.meshgrid(*linspaces, indexing=\"ij\")\n        flat = torch.stack([g.reshape(-1) for g in grids], dim=-1)\n        return flat[:n]\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.UniformSampler.__init__","title":"<code>__init__(seed: int | None = None) -&gt; None</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def __init__(self, seed: int | None = None) -&gt; None:\n    pass\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.UniformSampler.sample","title":"<code>sample(n: int, domain: Domain) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def sample(self, n: int, domain: Domain) -&gt; Tensor:\n    d = domain.ndim\n    pts_per_dim = math.ceil(n ** (1.0 / d))\n\n    linspaces = [torch.linspace(lo, hi, pts_per_dim) for lo, hi in domain.bounds]\n    grids = torch.meshgrid(*linspaces, indexing=\"ij\")\n    flat = torch.stack([g.reshape(-1) for g in grids], dim=-1)\n    return flat[:n]\n</code></pre>"},{"location":"reference/anypinn/core/samplers/#anypinn.core.samplers.build_sampler","title":"<code>build_sampler(strategy: CollocationStrategies, seed: int | None = None, scorer: ResidualScorer | None = None) -&gt; CollocationSampler</code>","text":"<p>Construct a collocation sampler from a strategy name.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>CollocationStrategies</code> <p>One of the <code>CollocationStrategies</code> literals.</p> required <code>seed</code> <code>int | None</code> <p>Optional seed for reproducible sampling.</p> <code>None</code> <code>scorer</code> <code>ResidualScorer | None</code> <p>Required when <code>strategy=\"adaptive\"</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>CollocationSampler</code> <p>A sampler instance satisfying the <code>CollocationSampler</code> protocol.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>strategy=\"adaptive\"</code> but no scorer is provided.</p> Source code in <code>src/anypinn/core/samplers.py</code> <pre><code>def build_sampler(\n    strategy: CollocationStrategies,\n    seed: int | None = None,\n    scorer: ResidualScorer | None = None,\n) -&gt; CollocationSampler:\n    \"\"\"Construct a collocation sampler from a strategy name.\n\n    Args:\n        strategy: One of the ``CollocationStrategies`` literals.\n        seed: Optional seed for reproducible sampling.\n        scorer: Required when ``strategy=\"adaptive\"``.\n\n    Returns:\n        A sampler instance satisfying the ``CollocationSampler`` protocol.\n\n    Raises:\n        ValueError: If ``strategy=\"adaptive\"`` but no scorer is provided.\n    \"\"\"\n    if strategy == \"adaptive\":\n        if scorer is None:\n            raise ValueError(\n                \"AdaptiveSampler requires a ResidualScorer. \"\n                \"Pass a scorer via PINNDataModule or use a different strategy.\"\n            )\n        return AdaptiveSampler(scorer=scorer, seed=seed)\n\n    cls = _SAMPLER_REGISTRY.get(strategy)\n    if cls is None:\n        raise ValueError(\n            f\"Unknown collocation strategy '{strategy}'. \"\n            f\"Choose from: {', '.join(_SAMPLER_REGISTRY)} or 'adaptive'.\"\n        )\n    return cls(seed=seed)\n</code></pre>"},{"location":"reference/anypinn/core/types/","title":"types","text":""},{"location":"reference/anypinn/core/types/#anypinn.core.types","title":"<code>anypinn.core.types</code>","text":"<p>Core type aliases, constants, and protocols for PINN.</p>"},{"location":"reference/anypinn/core/types/#anypinn.core.types.Activations","title":"<code>Activations: TypeAlias = Literal['tanh', 'relu', 'leaky_relu', 'sigmoid', 'selu', 'softplus', 'identity']</code>  <code>module-attribute</code>","text":"<p>Supported activation functions.</p>"},{"location":"reference/anypinn/core/types/#anypinn.core.types.CollocationStrategies","title":"<code>CollocationStrategies: TypeAlias = Literal['uniform', 'random', 'latin_hypercube', 'log_uniform_1d', 'adaptive']</code>  <code>module-attribute</code>","text":"<p>Supported collocation sampling strategies.</p>"},{"location":"reference/anypinn/core/types/#anypinn.core.types.Criteria","title":"<code>Criteria: TypeAlias = Literal['mse', 'huber', 'l1']</code>  <code>module-attribute</code>","text":"<p>Supported loss criteria.</p>"},{"location":"reference/anypinn/core/types/#anypinn.core.types.DataBatch","title":"<code>DataBatch: TypeAlias = tuple[Tensor, Tensor]</code>  <code>module-attribute</code>","text":"<p>Type alias for data batch: (x, y).</p>"},{"location":"reference/anypinn/core/types/#anypinn.core.types.LOSS_KEY","title":"<code>LOSS_KEY = 'loss'</code>  <code>module-attribute</code>","text":"<p>Key used for logging the total loss.</p>"},{"location":"reference/anypinn/core/types/#anypinn.core.types.PredictionBatch","title":"<code>PredictionBatch: TypeAlias = tuple[Tensor, Tensor]</code>  <code>module-attribute</code>","text":"<p>Prediction batch tuple: (x_data, y_data).</p>"},{"location":"reference/anypinn/core/types/#anypinn.core.types.Predictions","title":"<code>Predictions: TypeAlias = tuple[DataBatch, dict[str, Tensor], dict[str, Tensor] | None]</code>  <code>module-attribute</code>","text":"<p>Type alias for model predictions: (input_batch, predictions_dictionary, true_values_dictionary)  where predictions_dictionary is a dictionary of {[field_name | param_name]: prediction} and where true_values_dictionary is a dictionary of {[field_name | param_name]: true_value}. If no validation source is configured, true_values_dictionary is None.</p>"},{"location":"reference/anypinn/core/types/#anypinn.core.types.TrainingBatch","title":"<code>TrainingBatch: TypeAlias = tuple[DataBatch, Tensor]</code>  <code>module-attribute</code>","text":"<p>Training batch tuple: ((x_data, y_data), x_coll).</p>"},{"location":"reference/anypinn/core/types/#anypinn.core.types.LogFn","title":"<code>LogFn</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>A function that logs a value to a dictionary.</p> Source code in <code>src/anypinn/core/types.py</code> <pre><code>class LogFn(Protocol):\n    \"\"\"\n    A function that logs a value to a dictionary.\n    \"\"\"\n\n    def __call__(self, name: str, value: Tensor, progress_bar: bool = False) -&gt; None:\n        \"\"\"\n        Log a value.\n\n        Args:\n            name: The name to log the value under.\n            value: The value to log.\n            progress_bar: Whether the value should be logged to the progress bar.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/anypinn/core/types/#anypinn.core.types.LogFn.__call__","title":"<code>__call__(name: str, value: Tensor, progress_bar: bool = False) -&gt; None</code>","text":"<p>Log a value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to log the value under.</p> required <code>value</code> <code>Tensor</code> <p>The value to log.</p> required <code>progress_bar</code> <code>bool</code> <p>Whether the value should be logged to the progress bar.</p> <code>False</code> Source code in <code>src/anypinn/core/types.py</code> <pre><code>def __call__(self, name: str, value: Tensor, progress_bar: bool = False) -&gt; None:\n    \"\"\"\n    Log a value.\n\n    Args:\n        name: The name to log the value under.\n        value: The value to log.\n        progress_bar: Whether the value should be logged to the progress bar.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/anypinn/core/validation/","title":"validation","text":""},{"location":"reference/anypinn/core/validation/#anypinn.core.validation","title":"<code>anypinn.core.validation</code>","text":"<p>Validation registry for ground truth comparison during training and prediction.</p>"},{"location":"reference/anypinn/core/validation/#anypinn.core.validation.ResolvedValidation","title":"<code>ResolvedValidation: TypeAlias = dict[str, Callable[[Tensor], Tensor]]</code>  <code>module-attribute</code>","text":"<p>Validation registry after ColumnRef entries have been resolved to callables.</p>"},{"location":"reference/anypinn/core/validation/#anypinn.core.validation.ValidationRegistry","title":"<code>ValidationRegistry: TypeAlias = dict[str, ValidationSource]</code>  <code>module-attribute</code>","text":"<p>Registry mapping parameter names to their validation sources.</p> Example <p>validation: ValidationRegistry = { ...     \"beta\": lambda x: torch.sin(x),  # Pure function ...     \"gamma\": ColumnRef(column=\"gamma_true\"),  # From data ...     \"delta\": None,  # No validation ... }</p>"},{"location":"reference/anypinn/core/validation/#anypinn.core.validation.ValidationSource","title":"<code>ValidationSource: TypeAlias = Callable[[Tensor], Tensor] | ColumnRef | None</code>  <code>module-attribute</code>","text":"<p>A source for ground truth values. Can be: - A callable that takes x coordinates and returns true values - A ColumnRef that references a column in loaded data - None if no validation is needed for this parameter</p>"},{"location":"reference/anypinn/core/validation/#anypinn.core.validation.ColumnRef","title":"<code>ColumnRef</code>  <code>dataclass</code>","text":"<p>Reference to a column in loaded data for ground truth comparison.</p> <p>This allows practitioners to specify validation data by column name without writing custom functions. The column is resolved lazily when data is loaded.</p> <p>Attributes:</p> Name Type Description <code>column</code> <code>str</code> <p>Name of the column in the loaded DataFrame.</p> <code>transform</code> <code>Callable[[Tensor], Tensor] | None</code> <p>Optional transformation to apply to the column values.</p> Example <p>validation = { ...     \"beta\": ColumnRef(column=\"Rt\", transform=lambda rt: rt * delta), ... }</p> Source code in <code>src/anypinn/core/validation.py</code> <pre><code>@dataclass\nclass ColumnRef:\n    \"\"\"\n    Reference to a column in loaded data for ground truth comparison.\n\n    This allows practitioners to specify validation data by column name\n    without writing custom functions. The column is resolved lazily when\n    data is loaded.\n\n    Attributes:\n        column: Name of the column in the loaded DataFrame.\n        transform: Optional transformation to apply to the column values.\n\n    Example:\n        &gt;&gt;&gt; validation = {\n        ...     \"beta\": ColumnRef(column=\"Rt\", transform=lambda rt: rt * delta),\n        ... }\n    \"\"\"\n\n    column: str\n    transform: Callable[[Tensor], Tensor] | None = None\n</code></pre>"},{"location":"reference/anypinn/core/validation/#anypinn.core.validation.ColumnRef.column","title":"<code>column: str</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/validation/#anypinn.core.validation.ColumnRef.transform","title":"<code>transform: Callable[[Tensor], Tensor] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/core/validation/#anypinn.core.validation.ColumnRef.__init__","title":"<code>__init__(column: str, transform: Callable[[Tensor], Tensor] | None = None) -&gt; None</code>","text":""},{"location":"reference/anypinn/core/validation/#anypinn.core.validation.resolve_validation","title":"<code>resolve_validation(registry: ValidationRegistry, df_path: Path | None = None) -&gt; ResolvedValidation</code>","text":"<p>Resolve a ValidationRegistry by converting ColumnRef entries to callables.</p> <p>Pure function entries are passed through unchanged. ColumnRef entries are resolved using the provided data file path.</p> <p>Parameters:</p> Name Type Description Default <code>registry</code> <code>ValidationRegistry</code> <p>The validation registry to resolve.</p> required <code>df_path</code> <code>Path | None</code> <p>Path to the CSV file for ColumnRef resolution.</p> <code>None</code> <p>Returns:</p> Type Description <code>ResolvedValidation</code> <p>A dictionary mapping parameter names to callable validation functions.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a ColumnRef cannot be resolved (missing column or no df_path).</p> Source code in <code>src/anypinn/core/validation.py</code> <pre><code>def resolve_validation(\n    registry: ValidationRegistry,\n    df_path: Path | None = None,\n) -&gt; ResolvedValidation:\n    \"\"\"\n    Resolve a ValidationRegistry by converting ColumnRef entries to callables.\n\n    Pure function entries are passed through unchanged. ColumnRef entries\n    are resolved using the provided data file path.\n\n    Args:\n        registry: The validation registry to resolve.\n        df_path: Path to the CSV file for ColumnRef resolution.\n\n    Returns:\n        A dictionary mapping parameter names to callable validation functions.\n\n    Raises:\n        ValueError: If a ColumnRef cannot be resolved (missing column or no df_path).\n    \"\"\"\n\n    resolved: ResolvedValidation = {}\n    df: pd.DataFrame | None = None\n\n    for name, source in registry.items():\n        if source is None:\n            continue\n\n        if callable(source) and not isinstance(source, ColumnRef):\n            resolved[name] = source\n\n        elif isinstance(source, ColumnRef):\n            if df_path is None:\n                raise ValueError(\n                    f\"Cannot resolve ColumnRef for '{name}': no df_path provided. \"\n                    \"Either pass a df_path or use a callable instead of ColumnRef.\"\n                )\n\n            if df is None:\n                df = pd.read_csv(df_path)\n\n            if source.column not in df.columns:\n                raise ValueError(\n                    f\"Cannot resolve ColumnRef for '{name}': \"\n                    f\"column '{source.column}' not found in data. \"\n                    f\"Available columns: {list(df.columns)}\"\n                )\n\n            column_values = torch.tensor(df[source.column].values, dtype=torch.float32)\n\n            if source.transform is not None:\n                column_values = source.transform(column_values)\n\n            def make_lookup_fn(values: Tensor) -&gt; Callable[[Tensor], Tensor]:\n                cache: dict[torch.device, Tensor] = {}\n\n                def lookup(x: Tensor) -&gt; Tensor:\n                    device = x.device\n                    if device not in cache:\n                        cache[device] = values.to(device)\n                    idx = x.squeeze(-1).round().to(torch.int32)\n                    return cache[device][idx]\n\n                return lookup\n\n            resolved[name] = _ColumnLookup(make_lookup_fn(column_values))\n\n    return resolved\n</code></pre>"},{"location":"reference/anypinn/lib/","title":"lib","text":""},{"location":"reference/anypinn/lib/#anypinn.lib","title":"<code>anypinn.lib</code>","text":"<p>Utility libraries for anypinn.</p>"},{"location":"reference/anypinn/lib/#anypinn.lib.__all__","title":"<code>__all__ = ['FourierEncoding', 'RandomFourierFeatures']</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/lib/#anypinn.lib.FourierEncoding","title":"<code>FourierEncoding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Sinusoidal positional encoding for periodic or high-frequency signals.</p> <p>For input \\(\\mathbf{x} \\in \\mathbb{R}^{n \\times d}\\) and <code>num_frequencies</code> \\(K\\), the encoding is:</p> \\[ \\gamma(\\mathbf{x}) = [\\mathbf{x},\\,     \\sin(\\mathbf{x}),\\, \\cos(\\mathbf{x}),\\,     \\sin(2\\mathbf{x}),\\, \\cos(2\\mathbf{x}),\\,     \\ldots,\\,     \\sin(K\\mathbf{x}),\\, \\cos(K\\mathbf{x})] \\] <p>producing shape \\((n,\\, d\\,(1 + 2K))\\) when <code>include_input=True</code>, or \\((n,\\, 2dK)\\) when <code>include_input=False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>num_frequencies</code> <code>int</code> <p>Number of frequency bands \\(K \\geq 1\\).</p> <code>6</code> <code>include_input</code> <code>bool</code> <p>Prepend original coordinates to the encoded output.</p> <code>True</code> Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>class FourierEncoding(nn.Module):\n    \"\"\"Sinusoidal positional encoding for periodic or high-frequency signals.\n\n    For input $\\\\mathbf{x} \\\\in \\\\mathbb{R}^{n \\\\times d}$ and\n    `num_frequencies` $K$, the encoding is:\n\n    $$\n    \\\\gamma(\\\\mathbf{x}) = [\\\\mathbf{x},\\\\,\n        \\\\sin(\\\\mathbf{x}),\\\\, \\\\cos(\\\\mathbf{x}),\\\\,\n        \\\\sin(2\\\\mathbf{x}),\\\\, \\\\cos(2\\\\mathbf{x}),\\\\,\n        \\\\ldots,\\\\,\n        \\\\sin(K\\\\mathbf{x}),\\\\, \\\\cos(K\\\\mathbf{x})]\n    $$\n\n    producing shape $(n,\\\\, d\\\\,(1 + 2K))$ when `include_input=True`,\n    or $(n,\\\\, 2dK)$ when `include_input=False`.\n\n    Args:\n        num_frequencies: Number of frequency bands $K \\\\geq 1$.\n        include_input:   Prepend original coordinates to the encoded output.\n    \"\"\"\n\n    def __init__(self, num_frequencies: int = 6, include_input: bool = True) -&gt; None:\n        if num_frequencies &lt; 1:\n            raise ValueError(f\"num_frequencies must be &gt;= 1, got {num_frequencies}.\")\n        super().__init__()\n        self.num_frequencies = num_frequencies\n        self.include_input = include_input\n\n    def out_dim(self, in_dim: int) -&gt; int:\n        \"\"\"Compute output dimension given input dimension.\"\"\"\n        factor = 1 + 2 * self.num_frequencies if self.include_input else 2 * self.num_frequencies\n        return in_dim * factor\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        parts = [x] if self.include_input else []\n        for k in range(1, self.num_frequencies + 1):\n            parts.append(torch.sin(k * x))\n            parts.append(torch.cos(k * x))\n        return torch.cat(parts, dim=-1)\n</code></pre>"},{"location":"reference/anypinn/lib/#anypinn.lib.FourierEncoding.include_input","title":"<code>include_input = include_input</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lib/#anypinn.lib.FourierEncoding.num_frequencies","title":"<code>num_frequencies = num_frequencies</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lib/#anypinn.lib.FourierEncoding.__init__","title":"<code>__init__(num_frequencies: int = 6, include_input: bool = True) -&gt; None</code>","text":"Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>def __init__(self, num_frequencies: int = 6, include_input: bool = True) -&gt; None:\n    if num_frequencies &lt; 1:\n        raise ValueError(f\"num_frequencies must be &gt;= 1, got {num_frequencies}.\")\n    super().__init__()\n    self.num_frequencies = num_frequencies\n    self.include_input = include_input\n</code></pre>"},{"location":"reference/anypinn/lib/#anypinn.lib.FourierEncoding.forward","title":"<code>forward(x: Tensor) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    parts = [x] if self.include_input else []\n    for k in range(1, self.num_frequencies + 1):\n        parts.append(torch.sin(k * x))\n        parts.append(torch.cos(k * x))\n    return torch.cat(parts, dim=-1)\n</code></pre>"},{"location":"reference/anypinn/lib/#anypinn.lib.FourierEncoding.out_dim","title":"<code>out_dim(in_dim: int) -&gt; int</code>","text":"<p>Compute output dimension given input dimension.</p> Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>def out_dim(self, in_dim: int) -&gt; int:\n    \"\"\"Compute output dimension given input dimension.\"\"\"\n    factor = 1 + 2 * self.num_frequencies if self.include_input else 2 * self.num_frequencies\n    return in_dim * factor\n</code></pre>"},{"location":"reference/anypinn/lib/#anypinn.lib.RandomFourierFeatures","title":"<code>RandomFourierFeatures</code>","text":"<p>               Bases: <code>Module</code></p> <p>Random Fourier Features (Rahimi &amp; Recht, 2007) for RBF kernel approximation.</p> <p>Draws a fixed random matrix \\(\\mathbf{B} \\sim \\mathcal{N}(0, \\sigma^2)\\) of shape \\((d_{\\text{in}},\\, m)\\) and maps \\(\\mathbf{x} \\in \\mathbb{R}^{n \\times d_{\\text{in}}}\\) to:</p> \\[ \\phi(\\mathbf{x}) = \\frac{1}{\\sqrt{m}}     [\\cos(\\mathbf{x}\\mathbf{B}),\\; \\sin(\\mathbf{x}\\mathbf{B})]     \\in \\mathbb{R}^{n \\times 2m} \\] <p>\\(\\mathbf{B}\\) is registered as a buffer and moves with the module across devices.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Spatial dimension \\(d_{\\text{in}}\\) of the input.</p> required <code>num_features</code> <code>int</code> <p>Number of random features \\(m\\)           (output dimension \\(= 2m\\)).</p> <code>256</code> <code>scale</code> <code>float</code> <p>Standard deviation \\(\\sigma\\) of the frequency distribution.           Higher values capture higher-frequency variation. Default: 1.0.</p> <code>1.0</code> <code>seed</code> <code>int | None</code> <p>Optional seed for reproducible frequency sampling.</p> <code>None</code> Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>class RandomFourierFeatures(nn.Module):\n    \"\"\"Random Fourier Features (Rahimi &amp; Recht, 2007) for RBF kernel approximation.\n\n    Draws a fixed random matrix $\\\\mathbf{B} \\\\sim \\\\mathcal{N}(0, \\\\sigma^2)$\n    of shape $(d_{\\\\text{in}},\\\\, m)$ and maps\n    $\\\\mathbf{x} \\\\in \\\\mathbb{R}^{n \\\\times d_{\\\\text{in}}}$ to:\n\n    $$\n    \\\\phi(\\\\mathbf{x}) = \\\\frac{1}{\\\\sqrt{m}}\n        [\\\\cos(\\\\mathbf{x}\\\\mathbf{B}),\\\\; \\\\sin(\\\\mathbf{x}\\\\mathbf{B})]\n        \\\\in \\\\mathbb{R}^{n \\\\times 2m}\n    $$\n\n    $\\\\mathbf{B}$ is registered as a buffer and moves with the module across devices.\n\n    Args:\n        in_dim:       Spatial dimension $d_{\\\\text{in}}$ of the input.\n        num_features: Number of random features $m$\n                      (output dimension $= 2m$).\n        scale:        Standard deviation $\\\\sigma$ of the frequency distribution.\n                      Higher values capture higher-frequency variation. Default: 1.0.\n        seed:         Optional seed for reproducible frequency sampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_dim: int,\n        num_features: int = 256,\n        scale: float = 1.0,\n        seed: int | None = None,\n    ) -&gt; None:\n        if in_dim &lt; 1:\n            raise ValueError(f\"in_dim must be &gt;= 1, got {in_dim}.\")\n        if num_features &lt; 1:\n            raise ValueError(f\"num_features must be &gt;= 1, got {num_features}.\")\n        if scale &lt;= 0.0:\n            raise ValueError(f\"scale must be &gt; 0, got {scale}.\")\n        super().__init__()\n        gen = torch.Generator()\n        if seed is not None:\n            gen.manual_seed(seed)\n        B = torch.randn(in_dim, num_features, generator=gen) * scale\n        self.register_buffer(\"B\", B)\n        self.num_features = num_features\n\n    @property\n    def out_dim(self) -&gt; int:\n        \"\"\"Output dimension (always 2 * num_features).\"\"\"\n        return 2 * self.num_features\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        proj = x @ self.B  # type: ignore[operator]  # (n, num_features)\n        return torch.cat([torch.cos(proj), torch.sin(proj)], dim=-1) / (self.num_features**0.5)\n</code></pre>"},{"location":"reference/anypinn/lib/#anypinn.lib.RandomFourierFeatures.num_features","title":"<code>num_features = num_features</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lib/#anypinn.lib.RandomFourierFeatures.out_dim","title":"<code>out_dim: int</code>  <code>property</code>","text":"<p>Output dimension (always 2 * num_features).</p>"},{"location":"reference/anypinn/lib/#anypinn.lib.RandomFourierFeatures.__init__","title":"<code>__init__(in_dim: int, num_features: int = 256, scale: float = 1.0, seed: int | None = None) -&gt; None</code>","text":"Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>def __init__(\n    self,\n    in_dim: int,\n    num_features: int = 256,\n    scale: float = 1.0,\n    seed: int | None = None,\n) -&gt; None:\n    if in_dim &lt; 1:\n        raise ValueError(f\"in_dim must be &gt;= 1, got {in_dim}.\")\n    if num_features &lt; 1:\n        raise ValueError(f\"num_features must be &gt;= 1, got {num_features}.\")\n    if scale &lt;= 0.0:\n        raise ValueError(f\"scale must be &gt; 0, got {scale}.\")\n    super().__init__()\n    gen = torch.Generator()\n    if seed is not None:\n        gen.manual_seed(seed)\n    B = torch.randn(in_dim, num_features, generator=gen) * scale\n    self.register_buffer(\"B\", B)\n    self.num_features = num_features\n</code></pre>"},{"location":"reference/anypinn/lib/#anypinn.lib.RandomFourierFeatures.forward","title":"<code>forward(x: Tensor) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    proj = x @ self.B  # type: ignore[operator]  # (n, num_features)\n    return torch.cat([torch.cos(proj), torch.sin(proj)], dim=-1) / (self.num_features**0.5)\n</code></pre>"},{"location":"reference/anypinn/lib/diff/","title":"diff","text":""},{"location":"reference/anypinn/lib/diff/#anypinn.lib.diff","title":"<code>anypinn.lib.diff</code>","text":"<p>Differential operators for Physics-Informed Neural Networks.</p> <p>Composable utilities built on <code>torch.autograd.grad</code> for computing first-order, higher-order, and mixed partial derivatives without re-implementing autograd boilerplate in every constraint.</p> <p>All operators default to <code>create_graph=True</code> so their outputs are differentiable \u2014 required when used inside loss functions that must back-propagate. Pass <code>create_graph=False</code> for detached results (e.g. visualisation or adaptive sampling).</p> <p>Note</p> <p>For even higher performance on large-batch Hessians or Jacobians, <code>torch.func.jacrev</code> / <code>torch.func.hessian</code> can be composed with <code>torch.vmap</code>. The operators here intentionally use the simpler <code>autograd.grad</code> path for broad compatibility and <code>torch.compile</code> friendliness.</p>"},{"location":"reference/anypinn/lib/diff/#anypinn.lib.diff.divergence","title":"<code>divergence(v: Tensor, x: Tensor, *, create_graph: bool = True) -&gt; Tensor</code>","text":"<p>Compute the divergence \\(\\nabla \\cdot v = \\sum_i \\partial v_i / \\partial x_i\\).</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Tensor</code> <p>Vector field values, shape <code>(n, d)</code> matching <code>x.shape[1]</code>.</p> required <code>x</code> <code>Tensor</code> <p>Input coordinates, shape <code>(n, d)</code> with <code>requires_grad=True</code>.</p> required <code>create_graph</code> <code>bool</code> <p>Keep the result in the computation graph (default <code>True</code>).</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(n, 1)</code>.</p> Source code in <code>src/anypinn/lib/diff.py</code> <pre><code>def divergence(\n    v: Tensor,\n    x: Tensor,\n    *,\n    create_graph: bool = True,\n) -&gt; Tensor:\n    \"\"\"Compute the divergence $\\\\nabla \\\\cdot v = \\\\sum_i \\\\partial v_i / \\\\partial x_i$.\n\n    Args:\n        v: Vector field values, shape ``(n, d)`` matching ``x.shape[1]``.\n        x: Input coordinates, shape ``(n, d)`` with ``requires_grad=True``.\n        create_graph: Keep the result in the computation graph (default ``True``).\n\n    Returns:\n        Tensor of shape ``(n, 1)``.\n    \"\"\"\n    ndim = x.shape[1]\n    if v.shape[1] != ndim:\n        raise ValueError(\n            f\"v has {v.shape[1]} components but x has {ndim} dimensions; they must match\"\n        )\n\n    div = torch.zeros(x.shape[0], 1, device=x.device, dtype=x.dtype)\n    for i in range(ndim):\n        dvi_dxi = grad(v[:, i : i + 1], x, create_graph=True)[:, i : i + 1]\n        div = div + dvi_dxi\n\n    if not create_graph:\n        div = div.detach()\n    return div\n</code></pre>"},{"location":"reference/anypinn/lib/diff/#anypinn.lib.diff.grad","title":"<code>grad(u: Tensor, x: Tensor, *, create_graph: bool = True) -&gt; Tensor</code>","text":"<p>Compute the full gradient \\(\\nabla u\\) with respect to coordinates \\(x\\).</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>Tensor</code> <p>Scalar field values, shape <code>(n,)</code> or <code>(n, 1)</code>.</p> required <code>x</code> <code>Tensor</code> <p>Input coordinates, shape <code>(n, d)</code> with <code>requires_grad=True</code>.</p> required <code>create_graph</code> <code>bool</code> <p>Keep the result in the computation graph (default <code>True</code>).</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(n, d)</code> with rows</p> <code>Tensor</code> <p>\\([\\partial u/\\partial x_0, \\ldots, \\partial u/\\partial x_{d-1}]\\).</p> Source code in <code>src/anypinn/lib/diff.py</code> <pre><code>def grad(\n    u: Tensor,\n    x: Tensor,\n    *,\n    create_graph: bool = True,\n) -&gt; Tensor:\n    \"\"\"Compute the full gradient $\\\\nabla u$ with respect to coordinates $x$.\n\n    Args:\n        u: Scalar field values, shape ``(n,)`` or ``(n, 1)``.\n        x: Input coordinates, shape ``(n, d)`` with ``requires_grad=True``.\n        create_graph: Keep the result in the computation graph (default ``True``).\n\n    Returns:\n        Tensor of shape ``(n, d)`` with rows\n        $[\\\\partial u/\\\\partial x_0, \\\\ldots, \\\\partial u/\\\\partial x_{d-1}]$.\n    \"\"\"\n    (grad_u,) = torch.autograd.grad(\n        u.reshape(-1).sum(),\n        x,\n        create_graph=create_graph,\n    )\n    return grad_u\n</code></pre>"},{"location":"reference/anypinn/lib/diff/#anypinn.lib.diff.hessian","title":"<code>hessian(u: Tensor, x: Tensor, *, create_graph: bool = True) -&gt; Tensor</code>","text":"<p>Compute the Hessian matrix \\(H[u]\\) where \\(H_{ij} = \\partial^2 u / (\\partial x_i\\partial x_j)\\).</p> <p>Computes the first-order gradient once, then differentiates each component to build each row \u2014 <code>d + 1</code> autograd calls total.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>Tensor</code> <p>Scalar field values, shape <code>(n,)</code> or <code>(n, 1)</code>.</p> required <code>x</code> <code>Tensor</code> <p>Input coordinates, shape <code>(n, d)</code> with <code>requires_grad=True</code>.</p> required <code>create_graph</code> <code>bool</code> <p>Keep the result in the computation graph (default <code>True</code>).</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(n, d, d)</code>.</p> Source code in <code>src/anypinn/lib/diff.py</code> <pre><code>def hessian(\n    u: Tensor,\n    x: Tensor,\n    *,\n    create_graph: bool = True,\n) -&gt; Tensor:\n    \"\"\"Compute the Hessian matrix $H[u]$ where\n    $H_{ij} = \\\\partial^2 u / (\\\\partial x_i\\\\partial x_j)$.\n\n    Computes the first-order gradient once, then differentiates each\n    component to build each row \u2014 ``d + 1`` autograd calls total.\n\n    Args:\n        u: Scalar field values, shape ``(n,)`` or ``(n, 1)``.\n        x: Input coordinates, shape ``(n, d)`` with ``requires_grad=True``.\n        create_graph: Keep the result in the computation graph (default ``True``).\n\n    Returns:\n        Tensor of shape ``(n, d, d)``.\n    \"\"\"\n    grad_u = grad(u, x, create_graph=True)\n    ndim = x.shape[1]\n    rows = []\n    for i in range(ndim):\n        row = grad(grad_u[:, i : i + 1], x, create_graph=True)\n        rows.append(row)\n\n    H = torch.stack(rows, dim=1)\n\n    if not create_graph:\n        H = H.detach()\n    return H\n</code></pre>"},{"location":"reference/anypinn/lib/diff/#anypinn.lib.diff.laplacian","title":"<code>laplacian(u: Tensor, x: Tensor, *, create_graph: bool = True) -&gt; Tensor</code>","text":"<p>Compute the Laplacian \\(\\nabla^2 u = \\sum_i \\partial^2 u / \\partial x_i^2\\).</p> <p>Computes the full first-order gradient once and then differentiates each component \u2014 <code>d + 1</code> autograd calls total for <code>d</code> dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>Tensor</code> <p>Scalar field values, shape <code>(n,)</code> or <code>(n, 1)</code>.</p> required <code>x</code> <code>Tensor</code> <p>Input coordinates, shape <code>(n, d)</code> with <code>requires_grad=True</code>.</p> required <code>create_graph</code> <code>bool</code> <p>Keep the result in the computation graph (default <code>True</code>).</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(n, 1)</code>.</p> Source code in <code>src/anypinn/lib/diff.py</code> <pre><code>def laplacian(\n    u: Tensor,\n    x: Tensor,\n    *,\n    create_graph: bool = True,\n) -&gt; Tensor:\n    \"\"\"Compute the Laplacian $\\\\nabla^2 u = \\\\sum_i \\\\partial^2 u / \\\\partial x_i^2$.\n\n    Computes the full first-order gradient once and then differentiates\n    each component \u2014 ``d + 1`` autograd calls total for ``d`` dimensions.\n\n    Args:\n        u: Scalar field values, shape ``(n,)`` or ``(n, 1)``.\n        x: Input coordinates, shape ``(n, d)`` with ``requires_grad=True``.\n        create_graph: Keep the result in the computation graph (default ``True``).\n\n    Returns:\n        Tensor of shape ``(n, 1)``.\n    \"\"\"\n    grad_u = grad(u, x, create_graph=True)\n    ndim = x.shape[1]\n    lap = torch.zeros(x.shape[0], 1, device=x.device, dtype=x.dtype)\n    for i in range(ndim):\n        d2u_dxi2 = grad(grad_u[:, i : i + 1], x, create_graph=True)[:, i : i + 1]\n        lap = lap + d2u_dxi2\n\n    if not create_graph:\n        lap = lap.detach()\n    return lap\n</code></pre>"},{"location":"reference/anypinn/lib/diff/#anypinn.lib.diff.mixed_partial","title":"<code>mixed_partial(u: Tensor, x: Tensor, dims: tuple[int, ...], *, create_graph: bool = True) -&gt; Tensor</code>","text":"<p>Compute a mixed derivative \\(\\partial^k u / (\\partial x_{d_0} \\partial x_{d_1} \\cdots)\\).</p> <p>Derivatives are applied left-to-right: first differentiate w.r.t. <code>dims[0]</code>, then the result w.r.t. <code>dims[1]</code>, and so on.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>Tensor</code> <p>Scalar field values, shape <code>(n,)</code> or <code>(n, 1)</code>.</p> required <code>x</code> <code>Tensor</code> <p>Input coordinates, shape <code>(n, d)</code> with <code>requires_grad=True</code>.</p> required <code>dims</code> <code>tuple[int, ...]</code> <p>Dimension indices to differentiate along, in order.</p> required <code>create_graph</code> <code>bool</code> <p>Keep the result in the computation graph (default <code>True</code>).</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(n, 1)</code>.</p> Source code in <code>src/anypinn/lib/diff.py</code> <pre><code>def mixed_partial(\n    u: Tensor,\n    x: Tensor,\n    dims: tuple[int, ...],\n    *,\n    create_graph: bool = True,\n) -&gt; Tensor:\n    \"\"\"Compute a mixed derivative $\\\\partial^k u / (\\\\partial x_{d_0} \\\\partial x_{d_1} \\\\cdots)$.\n\n    Derivatives are applied left-to-right: first differentiate w.r.t.\n    ``dims[0]``, then the result w.r.t. ``dims[1]``, and so on.\n\n    Args:\n        u: Scalar field values, shape ``(n,)`` or ``(n, 1)``.\n        x: Input coordinates, shape ``(n, d)`` with ``requires_grad=True``.\n        dims: Dimension indices to differentiate along, in order.\n        create_graph: Keep the result in the computation graph (default ``True``).\n\n    Returns:\n        Tensor of shape ``(n, 1)``.\n    \"\"\"\n    if len(dims) == 0:\n        raise ValueError(\"dims must contain at least one dimension index\")\n\n    result = grad(u, x, create_graph=True)[:, dims[0] : dims[0] + 1]\n    for d in dims[1:]:\n        result = grad(result, x, create_graph=True)[:, d : d + 1]\n\n    if not create_graph:\n        result = result.detach()\n    return result\n</code></pre>"},{"location":"reference/anypinn/lib/diff/#anypinn.lib.diff.partial","title":"<code>partial(u: Tensor, x: Tensor, dim: int, *, order: int = 1, create_graph: bool = True) -&gt; Tensor</code>","text":"<p>Compute the order-k derivative \\(\\partial^k u / \\partial x_d^k\\) along one dimension.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>Tensor</code> <p>Scalar field values, shape <code>(n,)</code> or <code>(n, 1)</code>.</p> required <code>x</code> <code>Tensor</code> <p>Input coordinates, shape <code>(n, d)</code> with <code>requires_grad=True</code>.</p> required <code>dim</code> <code>int</code> <p>Spatial dimension index to differentiate along.</p> required <code>order</code> <code>int</code> <p>Derivative order (\u2265 1, default 1).</p> <code>1</code> <code>create_graph</code> <code>bool</code> <p>Keep the result in the computation graph (default <code>True</code>).</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape <code>(n, 1)</code>.</p> Source code in <code>src/anypinn/lib/diff.py</code> <pre><code>def partial(\n    u: Tensor,\n    x: Tensor,\n    dim: int,\n    *,\n    order: int = 1,\n    create_graph: bool = True,\n) -&gt; Tensor:\n    \"\"\"Compute the order-k derivative $\\\\partial^k u / \\\\partial x_d^k$ along one dimension.\n\n    Args:\n        u: Scalar field values, shape ``(n,)`` or ``(n, 1)``.\n        x: Input coordinates, shape ``(n, d)`` with ``requires_grad=True``.\n        dim: Spatial dimension index to differentiate along.\n        order: Derivative order (\u2265 1, default 1).\n        create_graph: Keep the result in the computation graph (default ``True``).\n\n    Returns:\n        Tensor of shape ``(n, 1)``.\n    \"\"\"\n    if order &lt; 1:\n        raise ValueError(f\"order must be &gt;= 1, got {order}\")\n\n    result = grad(u, x, create_graph=True)[:, dim : dim + 1]\n    for _ in range(order - 1):\n        result = grad(result, x, create_graph=True)[:, dim : dim + 1]\n\n    if not create_graph:\n        result = result.detach()\n    return result\n</code></pre>"},{"location":"reference/anypinn/lib/encodings/","title":"encodings","text":""},{"location":"reference/anypinn/lib/encodings/#anypinn.lib.encodings","title":"<code>anypinn.lib.encodings</code>","text":"<p>Built-in input encodings for spatial/periodic signals.</p>"},{"location":"reference/anypinn/lib/encodings/#anypinn.lib.encodings.FourierEncoding","title":"<code>FourierEncoding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Sinusoidal positional encoding for periodic or high-frequency signals.</p> <p>For input \\(\\mathbf{x} \\in \\mathbb{R}^{n \\times d}\\) and <code>num_frequencies</code> \\(K\\), the encoding is:</p> \\[ \\gamma(\\mathbf{x}) = [\\mathbf{x},\\,     \\sin(\\mathbf{x}),\\, \\cos(\\mathbf{x}),\\,     \\sin(2\\mathbf{x}),\\, \\cos(2\\mathbf{x}),\\,     \\ldots,\\,     \\sin(K\\mathbf{x}),\\, \\cos(K\\mathbf{x})] \\] <p>producing shape \\((n,\\, d\\,(1 + 2K))\\) when <code>include_input=True</code>, or \\((n,\\, 2dK)\\) when <code>include_input=False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>num_frequencies</code> <code>int</code> <p>Number of frequency bands \\(K \\geq 1\\).</p> <code>6</code> <code>include_input</code> <code>bool</code> <p>Prepend original coordinates to the encoded output.</p> <code>True</code> Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>class FourierEncoding(nn.Module):\n    \"\"\"Sinusoidal positional encoding for periodic or high-frequency signals.\n\n    For input $\\\\mathbf{x} \\\\in \\\\mathbb{R}^{n \\\\times d}$ and\n    `num_frequencies` $K$, the encoding is:\n\n    $$\n    \\\\gamma(\\\\mathbf{x}) = [\\\\mathbf{x},\\\\,\n        \\\\sin(\\\\mathbf{x}),\\\\, \\\\cos(\\\\mathbf{x}),\\\\,\n        \\\\sin(2\\\\mathbf{x}),\\\\, \\\\cos(2\\\\mathbf{x}),\\\\,\n        \\\\ldots,\\\\,\n        \\\\sin(K\\\\mathbf{x}),\\\\, \\\\cos(K\\\\mathbf{x})]\n    $$\n\n    producing shape $(n,\\\\, d\\\\,(1 + 2K))$ when `include_input=True`,\n    or $(n,\\\\, 2dK)$ when `include_input=False`.\n\n    Args:\n        num_frequencies: Number of frequency bands $K \\\\geq 1$.\n        include_input:   Prepend original coordinates to the encoded output.\n    \"\"\"\n\n    def __init__(self, num_frequencies: int = 6, include_input: bool = True) -&gt; None:\n        if num_frequencies &lt; 1:\n            raise ValueError(f\"num_frequencies must be &gt;= 1, got {num_frequencies}.\")\n        super().__init__()\n        self.num_frequencies = num_frequencies\n        self.include_input = include_input\n\n    def out_dim(self, in_dim: int) -&gt; int:\n        \"\"\"Compute output dimension given input dimension.\"\"\"\n        factor = 1 + 2 * self.num_frequencies if self.include_input else 2 * self.num_frequencies\n        return in_dim * factor\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        parts = [x] if self.include_input else []\n        for k in range(1, self.num_frequencies + 1):\n            parts.append(torch.sin(k * x))\n            parts.append(torch.cos(k * x))\n        return torch.cat(parts, dim=-1)\n</code></pre>"},{"location":"reference/anypinn/lib/encodings/#anypinn.lib.encodings.FourierEncoding.include_input","title":"<code>include_input = include_input</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lib/encodings/#anypinn.lib.encodings.FourierEncoding.num_frequencies","title":"<code>num_frequencies = num_frequencies</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lib/encodings/#anypinn.lib.encodings.FourierEncoding.__init__","title":"<code>__init__(num_frequencies: int = 6, include_input: bool = True) -&gt; None</code>","text":"Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>def __init__(self, num_frequencies: int = 6, include_input: bool = True) -&gt; None:\n    if num_frequencies &lt; 1:\n        raise ValueError(f\"num_frequencies must be &gt;= 1, got {num_frequencies}.\")\n    super().__init__()\n    self.num_frequencies = num_frequencies\n    self.include_input = include_input\n</code></pre>"},{"location":"reference/anypinn/lib/encodings/#anypinn.lib.encodings.FourierEncoding.forward","title":"<code>forward(x: Tensor) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    parts = [x] if self.include_input else []\n    for k in range(1, self.num_frequencies + 1):\n        parts.append(torch.sin(k * x))\n        parts.append(torch.cos(k * x))\n    return torch.cat(parts, dim=-1)\n</code></pre>"},{"location":"reference/anypinn/lib/encodings/#anypinn.lib.encodings.FourierEncoding.out_dim","title":"<code>out_dim(in_dim: int) -&gt; int</code>","text":"<p>Compute output dimension given input dimension.</p> Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>def out_dim(self, in_dim: int) -&gt; int:\n    \"\"\"Compute output dimension given input dimension.\"\"\"\n    factor = 1 + 2 * self.num_frequencies if self.include_input else 2 * self.num_frequencies\n    return in_dim * factor\n</code></pre>"},{"location":"reference/anypinn/lib/encodings/#anypinn.lib.encodings.RandomFourierFeatures","title":"<code>RandomFourierFeatures</code>","text":"<p>               Bases: <code>Module</code></p> <p>Random Fourier Features (Rahimi &amp; Recht, 2007) for RBF kernel approximation.</p> <p>Draws a fixed random matrix \\(\\mathbf{B} \\sim \\mathcal{N}(0, \\sigma^2)\\) of shape \\((d_{\\text{in}},\\, m)\\) and maps \\(\\mathbf{x} \\in \\mathbb{R}^{n \\times d_{\\text{in}}}\\) to:</p> \\[ \\phi(\\mathbf{x}) = \\frac{1}{\\sqrt{m}}     [\\cos(\\mathbf{x}\\mathbf{B}),\\; \\sin(\\mathbf{x}\\mathbf{B})]     \\in \\mathbb{R}^{n \\times 2m} \\] <p>\\(\\mathbf{B}\\) is registered as a buffer and moves with the module across devices.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Spatial dimension \\(d_{\\text{in}}\\) of the input.</p> required <code>num_features</code> <code>int</code> <p>Number of random features \\(m\\)           (output dimension \\(= 2m\\)).</p> <code>256</code> <code>scale</code> <code>float</code> <p>Standard deviation \\(\\sigma\\) of the frequency distribution.           Higher values capture higher-frequency variation. Default: 1.0.</p> <code>1.0</code> <code>seed</code> <code>int | None</code> <p>Optional seed for reproducible frequency sampling.</p> <code>None</code> Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>class RandomFourierFeatures(nn.Module):\n    \"\"\"Random Fourier Features (Rahimi &amp; Recht, 2007) for RBF kernel approximation.\n\n    Draws a fixed random matrix $\\\\mathbf{B} \\\\sim \\\\mathcal{N}(0, \\\\sigma^2)$\n    of shape $(d_{\\\\text{in}},\\\\, m)$ and maps\n    $\\\\mathbf{x} \\\\in \\\\mathbb{R}^{n \\\\times d_{\\\\text{in}}}$ to:\n\n    $$\n    \\\\phi(\\\\mathbf{x}) = \\\\frac{1}{\\\\sqrt{m}}\n        [\\\\cos(\\\\mathbf{x}\\\\mathbf{B}),\\\\; \\\\sin(\\\\mathbf{x}\\\\mathbf{B})]\n        \\\\in \\\\mathbb{R}^{n \\\\times 2m}\n    $$\n\n    $\\\\mathbf{B}$ is registered as a buffer and moves with the module across devices.\n\n    Args:\n        in_dim:       Spatial dimension $d_{\\\\text{in}}$ of the input.\n        num_features: Number of random features $m$\n                      (output dimension $= 2m$).\n        scale:        Standard deviation $\\\\sigma$ of the frequency distribution.\n                      Higher values capture higher-frequency variation. Default: 1.0.\n        seed:         Optional seed for reproducible frequency sampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_dim: int,\n        num_features: int = 256,\n        scale: float = 1.0,\n        seed: int | None = None,\n    ) -&gt; None:\n        if in_dim &lt; 1:\n            raise ValueError(f\"in_dim must be &gt;= 1, got {in_dim}.\")\n        if num_features &lt; 1:\n            raise ValueError(f\"num_features must be &gt;= 1, got {num_features}.\")\n        if scale &lt;= 0.0:\n            raise ValueError(f\"scale must be &gt; 0, got {scale}.\")\n        super().__init__()\n        gen = torch.Generator()\n        if seed is not None:\n            gen.manual_seed(seed)\n        B = torch.randn(in_dim, num_features, generator=gen) * scale\n        self.register_buffer(\"B\", B)\n        self.num_features = num_features\n\n    @property\n    def out_dim(self) -&gt; int:\n        \"\"\"Output dimension (always 2 * num_features).\"\"\"\n        return 2 * self.num_features\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        proj = x @ self.B  # type: ignore[operator]  # (n, num_features)\n        return torch.cat([torch.cos(proj), torch.sin(proj)], dim=-1) / (self.num_features**0.5)\n</code></pre>"},{"location":"reference/anypinn/lib/encodings/#anypinn.lib.encodings.RandomFourierFeatures.num_features","title":"<code>num_features = num_features</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lib/encodings/#anypinn.lib.encodings.RandomFourierFeatures.out_dim","title":"<code>out_dim: int</code>  <code>property</code>","text":"<p>Output dimension (always 2 * num_features).</p>"},{"location":"reference/anypinn/lib/encodings/#anypinn.lib.encodings.RandomFourierFeatures.__init__","title":"<code>__init__(in_dim: int, num_features: int = 256, scale: float = 1.0, seed: int | None = None) -&gt; None</code>","text":"Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>def __init__(\n    self,\n    in_dim: int,\n    num_features: int = 256,\n    scale: float = 1.0,\n    seed: int | None = None,\n) -&gt; None:\n    if in_dim &lt; 1:\n        raise ValueError(f\"in_dim must be &gt;= 1, got {in_dim}.\")\n    if num_features &lt; 1:\n        raise ValueError(f\"num_features must be &gt;= 1, got {num_features}.\")\n    if scale &lt;= 0.0:\n        raise ValueError(f\"scale must be &gt; 0, got {scale}.\")\n    super().__init__()\n    gen = torch.Generator()\n    if seed is not None:\n        gen.manual_seed(seed)\n    B = torch.randn(in_dim, num_features, generator=gen) * scale\n    self.register_buffer(\"B\", B)\n    self.num_features = num_features\n</code></pre>"},{"location":"reference/anypinn/lib/encodings/#anypinn.lib.encodings.RandomFourierFeatures.forward","title":"<code>forward(x: Tensor) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/lib/encodings.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    proj = x @ self.B  # type: ignore[operator]  # (n, num_features)\n    return torch.cat([torch.cos(proj), torch.sin(proj)], dim=-1) / (self.num_features**0.5)\n</code></pre>"},{"location":"reference/anypinn/lib/utils/","title":"utils","text":""},{"location":"reference/anypinn/lib/utils/#anypinn.lib.utils","title":"<code>anypinn.lib.utils</code>","text":""},{"location":"reference/anypinn/lib/utils/#anypinn.lib.utils.T","title":"<code>T = TypeVar('T')</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/lib/utils/#anypinn.lib.utils.find","title":"<code>find(iterable: Iterable[T], predicate: Callable[[T], bool], default: T | None = None) -&gt; T | None</code>","text":"<p>Find the first element in an iterable that satisfies a predicate.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable[T]</code> <p>The iterable to search.</p> required <code>predicate</code> <code>Callable[[T], bool]</code> <p>A function that returns True for the desired element.</p> required <code>default</code> <code>T | None</code> <p>The value to return if no element is found. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>T | None</code> <p>The first matching element, or the default value.</p> Source code in <code>src/anypinn/lib/utils.py</code> <pre><code>def find(\n    iterable: Iterable[T],\n    predicate: Callable[[T], bool],\n    default: T | None = None,\n) -&gt; T | None:\n    \"\"\"\n    Find the first element in an iterable that satisfies a predicate.\n\n    Args:\n        iterable: The iterable to search.\n        predicate: A function that returns True for the desired element.\n        default: The value to return if no element is found. Defaults to None.\n\n    Returns:\n        The first matching element, or the default value.\n    \"\"\"\n    return next((x for x in iterable if predicate(x)), default)\n</code></pre>"},{"location":"reference/anypinn/lib/utils/#anypinn.lib.utils.find_or_raise","title":"<code>find_or_raise(iterable: Iterable[T], predicate: Callable[[T], bool], exception: Exception | Callable[[], Exception] | None = None) -&gt; T</code>","text":"<p>Find the first element in an iterable that satisfies a predicate, or raise an exception.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable[T]</code> <p>The iterable to search.</p> required <code>predicate</code> <code>Callable[[T], bool]</code> <p>A function that returns True for the desired element.</p> required <code>exception</code> <code>Exception | Callable[[], Exception] | None</code> <p>The exception to raise if no element is found.        Can be an Exception instance, a callable returning an Exception,        or None (raises ValueError).</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>The first matching element.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no element is found and no specific exception is provided.</p> <code>Exception</code> <p>The provided exception if no element is found.</p> Source code in <code>src/anypinn/lib/utils.py</code> <pre><code>def find_or_raise(\n    iterable: Iterable[T],\n    predicate: Callable[[T], bool],\n    exception: Exception | Callable[[], Exception] | None = None,\n) -&gt; T:\n    \"\"\"\n    Find the first element in an iterable that satisfies a predicate, or raise an exception.\n\n    Args:\n        iterable: The iterable to search.\n        predicate: A function that returns True for the desired element.\n        exception: The exception to raise if no element is found.\n                   Can be an Exception instance, a callable returning an Exception,\n                   or None (raises ValueError).\n\n    Returns:\n        The first matching element.\n\n    Raises:\n        ValueError: If no element is found and no specific exception is provided.\n        Exception: The provided exception if no element is found.\n    \"\"\"\n    found = find(iterable, predicate)\n    if found is not None:\n        return found\n\n    if exception is None:\n        raise ValueError(\"Element not found\")\n    if isinstance(exception, Exception):\n        raise exception\n    raise exception()\n</code></pre>"},{"location":"reference/anypinn/lib/utils/#anypinn.lib.utils.get_tensorboard_logger","title":"<code>get_tensorboard_logger(trainer: Trainer, default: TensorBoardLogger | None = None) -&gt; TensorBoardLogger | None</code>","text":"<p>Retrieve the TensorBoardLogger from the trainer.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The PyTorch Lightning Trainer instance.</p> required <code>default</code> <code>TensorBoardLogger | None</code> <p>Default value if not found.</p> <code>None</code> <p>Returns:</p> Type Description <code>TensorBoardLogger | None</code> <p>The TensorBoardLogger or the default value.</p> Source code in <code>src/anypinn/lib/utils.py</code> <pre><code>def get_tensorboard_logger(\n    trainer: Trainer,\n    default: TensorBoardLogger | None = None,\n) -&gt; TensorBoardLogger | None:\n    \"\"\"\n    Retrieve the TensorBoardLogger from the trainer.\n\n    Args:\n        trainer: The PyTorch Lightning Trainer instance.\n        default: Default value if not found.\n\n    Returns:\n        The TensorBoardLogger or the default value.\n    \"\"\"\n    return cast(\n        TensorBoardLogger | None,\n        find(\n            trainer.loggers,\n            lambda l: isinstance(l, TensorBoardLogger),\n            default,\n        ),\n    )\n</code></pre>"},{"location":"reference/anypinn/lib/utils/#anypinn.lib.utils.get_tensorboard_logger_or_raise","title":"<code>get_tensorboard_logger_or_raise(trainer: Trainer) -&gt; TensorBoardLogger</code>","text":"<p>Retrieve the TensorBoardLogger from the trainer, or raise if not present.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The PyTorch Lightning Trainer instance.</p> required <p>Returns:</p> Type Description <code>TensorBoardLogger</code> <p>The TensorBoardLogger.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no TensorBoardLogger is attached to the trainer.</p> Source code in <code>src/anypinn/lib/utils.py</code> <pre><code>def get_tensorboard_logger_or_raise(trainer: Trainer) -&gt; TensorBoardLogger:\n    \"\"\"\n    Retrieve the TensorBoardLogger from the trainer, or raise if not present.\n\n    Args:\n        trainer: The PyTorch Lightning Trainer instance.\n\n    Returns:\n        The TensorBoardLogger.\n\n    Raises:\n        ValueError: If no TensorBoardLogger is attached to the trainer.\n    \"\"\"\n    return cast(\n        TensorBoardLogger,\n        find_or_raise(\n            trainer.loggers,\n            lambda l: isinstance(l, TensorBoardLogger),\n            ValueError(\"TensorBoard logger not found\"),\n        ),\n    )\n</code></pre>"},{"location":"reference/anypinn/lightning/","title":"lightning","text":""},{"location":"reference/anypinn/lightning/#anypinn.lightning","title":"<code>anypinn.lightning</code>","text":"<p>Lightning integration for PINN training.</p>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.__all__","title":"<code>__all__ = ['AdaptiveCollocationCallback', 'FormattedProgressBar', 'PINNModule', 'PredictionsWriter', 'SMMAStopping']</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/lightning/#anypinn.lightning.AdaptiveCollocationCallback","title":"<code>AdaptiveCollocationCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Refreshes the collocation pool every N epochs using AdaptiveSampler.</p> <p>Requires the DataModule to be configured with <code>collocation_sampler=\"adaptive\"</code> and a <code>ResidualScorer</code> passed to <code>PINNDataModule(residual_scorer=...)</code>.</p> <p>Because the scorer typically closes over the <code>Problem</code>, it automatically uses the model's current weights on every call \u2014 no additional injection step needed.</p> <p>Parameters:</p> Name Type Description Default <code>every_n_epochs</code> <code>int</code> <p>How often (in epochs) to resample. Default: 1.</p> <code>1</code> Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>class AdaptiveCollocationCallback(Callback):\n    \"\"\"Refreshes the collocation pool every N epochs using AdaptiveSampler.\n\n    Requires the DataModule to be configured with ``collocation_sampler=\"adaptive\"``\n    and a ``ResidualScorer`` passed to ``PINNDataModule(residual_scorer=...)``.\n\n    Because the scorer typically closes over the ``Problem``, it automatically uses\n    the model's current weights on every call \u2014 no additional injection step needed.\n\n    Args:\n        every_n_epochs: How often (in epochs) to resample. Default: 1.\n    \"\"\"\n\n    def __init__(self, every_n_epochs: int = 1) -&gt; None:\n        super().__init__()\n        if every_n_epochs &lt; 1:\n            raise ValueError(f\"every_n_epochs must be &gt;= 1, got {every_n_epochs}.\")\n        self._every_n = every_n_epochs\n\n    @override\n    def on_fit_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n        \"\"\"Validate at fit start that the DataModule uses AdaptiveSampler.\"\"\"\n        dm = trainer.datamodule  # type: ignore[attr-defined]\n        if not isinstance(getattr(dm, \"_sampler\", None), AdaptiveSampler):\n            raise TypeError(\n                \"AdaptiveCollocationCallback requires the DataModule to use \"\n                \"collocation_sampler='adaptive'. Got: \"\n                f\"{type(getattr(dm, '_sampler', None)).__name__}.\"\n            )\n\n    @override\n    def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n        \"\"\"Resample collocation points using the current model weights.\"\"\"\n        if (trainer.current_epoch + 1) % self._every_n != 0:\n            return\n        dm = trainer.datamodule  # type: ignore[attr-defined]\n        n = dm.hp.training_data.collocations\n        new_coll = dm._sampler.sample(n, dm._domain)\n        dm.pinn_ds.x_coll = new_coll\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.AdaptiveCollocationCallback.__init__","title":"<code>__init__(every_n_epochs: int = 1) -&gt; None</code>","text":"Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>def __init__(self, every_n_epochs: int = 1) -&gt; None:\n    super().__init__()\n    if every_n_epochs &lt; 1:\n        raise ValueError(f\"every_n_epochs must be &gt;= 1, got {every_n_epochs}.\")\n    self._every_n = every_n_epochs\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.AdaptiveCollocationCallback.on_fit_start","title":"<code>on_fit_start(trainer: Trainer, pl_module: LightningModule) -&gt; None</code>","text":"<p>Validate at fit start that the DataModule uses AdaptiveSampler.</p> Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>@override\ndef on_fit_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n    \"\"\"Validate at fit start that the DataModule uses AdaptiveSampler.\"\"\"\n    dm = trainer.datamodule  # type: ignore[attr-defined]\n    if not isinstance(getattr(dm, \"_sampler\", None), AdaptiveSampler):\n        raise TypeError(\n            \"AdaptiveCollocationCallback requires the DataModule to use \"\n            \"collocation_sampler='adaptive'. Got: \"\n            f\"{type(getattr(dm, '_sampler', None)).__name__}.\"\n        )\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.AdaptiveCollocationCallback.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer: Trainer, pl_module: LightningModule) -&gt; None</code>","text":"<p>Resample collocation points using the current model weights.</p> Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>@override\ndef on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n    \"\"\"Resample collocation points using the current model weights.\"\"\"\n    if (trainer.current_epoch + 1) % self._every_n != 0:\n        return\n    dm = trainer.datamodule  # type: ignore[attr-defined]\n    n = dm.hp.training_data.collocations\n    new_coll = dm._sampler.sample(n, dm._domain)\n    dm.pinn_ds.x_coll = new_coll\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.FormattedProgressBar","title":"<code>FormattedProgressBar</code>","text":"<p>               Bases: <code>TQDMProgressBar</code></p> <p>Custom progress bar for training that formats metrics for better readability.</p> <p>This class extends the TQDMProgressBar to provide custom formatting for training metrics, particularly for the total loss and beta values.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>FormatFn</code> <p>Function to format the metric values.</p> required Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>class FormattedProgressBar(TQDMProgressBar):\n    \"\"\"\n    Custom progress bar for training that formats metrics for better readability.\n\n    This class extends the TQDMProgressBar to provide custom formatting for\n    training metrics, particularly for the total loss and beta values.\n\n    Args:\n        format: Function to format the metric values.\n    \"\"\"\n\n    def __init__(self, *args: Any, format: FormatFn, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n        self.format = format\n\n    @override\n    def get_metrics(self, *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n        \"\"\"\n        Format metrics for display in the progress bar.\n\n        Returns:\n            Dictionary of formatted metrics with:\n            - Total loss in scientific notation\n            - Beta value with 4 decimal places\n            - Other metrics as provided by the parent class\n        \"\"\"\n        items = super().get_metrics(*args, **kwargs)\n        items.pop(\"v_num\", None)\n        for key, value in items.items():\n            items[key] = self.format(key, value)\n\n        return items\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.FormattedProgressBar.format","title":"<code>format = format</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/#anypinn.lightning.FormattedProgressBar.__init__","title":"<code>__init__(*args: Any, format: FormatFn, **kwargs: Any)</code>","text":"Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>def __init__(self, *args: Any, format: FormatFn, **kwargs: Any):\n    super().__init__(*args, **kwargs)\n    self.format = format\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.FormattedProgressBar.get_metrics","title":"<code>get_metrics(*args: Any, **kwargs: Any) -&gt; dict[str, Any]</code>","text":"<p>Format metrics for display in the progress bar.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of formatted metrics with:</p> <code>dict[str, Any]</code> <ul> <li>Total loss in scientific notation</li> </ul> <code>dict[str, Any]</code> <ul> <li>Beta value with 4 decimal places</li> </ul> <code>dict[str, Any]</code> <ul> <li>Other metrics as provided by the parent class</li> </ul> Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>@override\ndef get_metrics(self, *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"\n    Format metrics for display in the progress bar.\n\n    Returns:\n        Dictionary of formatted metrics with:\n        - Total loss in scientific notation\n        - Beta value with 4 decimal places\n        - Other metrics as provided by the parent class\n    \"\"\"\n    items = super().get_metrics(*args, **kwargs)\n    items.pop(\"v_num\", None)\n    for key, value in items.items():\n        items[key] = self.format(key, value)\n\n    return items\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.PINNModule","title":"<code>PINNModule</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Generic PINN Lightning module. Expects external Problem + Sampler + optimizer config.</p> <p>Parameters:</p> Name Type Description Default <code>problem</code> <code>Problem</code> <p>The PINN problem definition (constraints, fields, etc.).</p> required <code>hp</code> <code>PINNHyperparameters</code> <p>Hyperparameters for training.</p> required Source code in <code>src/anypinn/lightning/module.py</code> <pre><code>class PINNModule(pl.LightningModule):\n    \"\"\"\n    Generic PINN Lightning module.\n    Expects external Problem + Sampler + optimizer config.\n\n    Args:\n        problem: The PINN problem definition (constraints, fields, etc.).\n        hp: Hyperparameters for training.\n    \"\"\"\n\n    def __init__(\n        self,\n        problem: Problem,\n        hp: PINNHyperparameters,\n    ):\n        super().__init__()\n        self.save_hyperparameters(ignore=[\"problem\"])\n\n        self.problem = problem\n        self.hp = hp\n\n        def _log(key: str, value: Tensor, progress_bar: bool = False) -&gt; None:\n            self.log(\n                key,\n                value,\n                on_step=False,\n                on_epoch=True,\n                prog_bar=progress_bar,\n                batch_size=hp.training_data.batch_size,\n            )\n\n        self._log = cast(LogFn, _log)\n\n    @override\n    def on_fit_start(self) -&gt; None:\n        \"\"\"\n        Called when fit begins. Resolves validation sources using loaded data.\n        \"\"\"\n        self.problem.inject_context(self.trainer.datamodule.context)  # type: ignore\n\n    @override\n    def on_predict_start(self) -&gt; None:\n        \"\"\"\n        Called when predict begins. Resolves validation sources using loaded data.\n        \"\"\"\n        self.problem.inject_context(self.trainer.datamodule.context)  # type: ignore\n\n    @override\n    def training_step(self, batch: TrainingBatch, batch_idx: int) -&gt; Tensor:\n        \"\"\"\n        Performs a single training step.\n        Calculates total loss from the problem.\n        \"\"\"\n        return self.problem.training_loss(batch, self._log)\n\n    @override\n    def predict_step(self, batch: PredictionBatch, batch_idx: int) -&gt; Predictions:\n        \"\"\"\n        Performs a prediction step.\n        \"\"\"\n        x_data, y_data = batch\n\n        (data_batch, predictions) = self.problem.predict((x_data, y_data))\n        true_values = self.problem.true_values(x_data)\n\n        return (data_batch, predictions, true_values)\n\n    @override\n    def configure_optimizers(self) -&gt; OptimizerLRScheduler:\n        \"\"\"\n        Configures the optimizer and learning rate scheduler.\n        \"\"\"\n        opt_cfg = self.hp.optimizer\n        if isinstance(opt_cfg, LBFGSConfig):\n            opt = torch.optim.LBFGS(\n                self.parameters(),\n                lr=opt_cfg.lr,\n                max_iter=opt_cfg.max_iter,\n                max_eval=opt_cfg.max_eval,\n                history_size=opt_cfg.history_size,\n                line_search_fn=opt_cfg.line_search_fn,\n            )\n        elif isinstance(opt_cfg, AdamConfig):\n            opt = torch.optim.Adam(\n                self.parameters(),\n                lr=opt_cfg.lr,\n                betas=opt_cfg.betas,\n                weight_decay=opt_cfg.weight_decay,\n            )\n        else:\n            opt = torch.optim.Adam(self.parameters(), lr=self.hp.lr)\n\n        sch_cfg = self.hp.scheduler\n        if not sch_cfg:\n            return opt\n\n        if isinstance(sch_cfg, CosineAnnealingConfig):\n            sch = torch.optim.lr_scheduler.CosineAnnealingLR(\n                opt,\n                T_max=sch_cfg.T_max,\n                eta_min=sch_cfg.eta_min,\n            )\n            return {\n                \"optimizer\": opt,\n                \"lr_scheduler\": {\n                    \"name\": \"lr\",\n                    \"scheduler\": sch,\n                    \"interval\": \"epoch\",\n                    \"frequency\": 1,\n                },\n            }\n\n        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            opt,\n            mode=sch_cfg.mode,\n            factor=sch_cfg.factor,\n            patience=sch_cfg.patience,\n            threshold=sch_cfg.threshold,\n            min_lr=sch_cfg.min_lr,\n        )\n\n        return {\n            \"optimizer\": opt,\n            \"lr_scheduler\": {\n                \"name\": \"lr\",\n                \"scheduler\": sch,\n                \"monitor\": LOSS_KEY,\n                \"interval\": \"epoch\",\n                \"frequency\": 1,\n            },\n        }\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.PINNModule.hp","title":"<code>hp = hp</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/#anypinn.lightning.PINNModule.problem","title":"<code>problem = problem</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/#anypinn.lightning.PINNModule.__init__","title":"<code>__init__(problem: Problem, hp: PINNHyperparameters)</code>","text":"Source code in <code>src/anypinn/lightning/module.py</code> <pre><code>def __init__(\n    self,\n    problem: Problem,\n    hp: PINNHyperparameters,\n):\n    super().__init__()\n    self.save_hyperparameters(ignore=[\"problem\"])\n\n    self.problem = problem\n    self.hp = hp\n\n    def _log(key: str, value: Tensor, progress_bar: bool = False) -&gt; None:\n        self.log(\n            key,\n            value,\n            on_step=False,\n            on_epoch=True,\n            prog_bar=progress_bar,\n            batch_size=hp.training_data.batch_size,\n        )\n\n    self._log = cast(LogFn, _log)\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.PINNModule.configure_optimizers","title":"<code>configure_optimizers() -&gt; OptimizerLRScheduler</code>","text":"<p>Configures the optimizer and learning rate scheduler.</p> Source code in <code>src/anypinn/lightning/module.py</code> <pre><code>@override\ndef configure_optimizers(self) -&gt; OptimizerLRScheduler:\n    \"\"\"\n    Configures the optimizer and learning rate scheduler.\n    \"\"\"\n    opt_cfg = self.hp.optimizer\n    if isinstance(opt_cfg, LBFGSConfig):\n        opt = torch.optim.LBFGS(\n            self.parameters(),\n            lr=opt_cfg.lr,\n            max_iter=opt_cfg.max_iter,\n            max_eval=opt_cfg.max_eval,\n            history_size=opt_cfg.history_size,\n            line_search_fn=opt_cfg.line_search_fn,\n        )\n    elif isinstance(opt_cfg, AdamConfig):\n        opt = torch.optim.Adam(\n            self.parameters(),\n            lr=opt_cfg.lr,\n            betas=opt_cfg.betas,\n            weight_decay=opt_cfg.weight_decay,\n        )\n    else:\n        opt = torch.optim.Adam(self.parameters(), lr=self.hp.lr)\n\n    sch_cfg = self.hp.scheduler\n    if not sch_cfg:\n        return opt\n\n    if isinstance(sch_cfg, CosineAnnealingConfig):\n        sch = torch.optim.lr_scheduler.CosineAnnealingLR(\n            opt,\n            T_max=sch_cfg.T_max,\n            eta_min=sch_cfg.eta_min,\n        )\n        return {\n            \"optimizer\": opt,\n            \"lr_scheduler\": {\n                \"name\": \"lr\",\n                \"scheduler\": sch,\n                \"interval\": \"epoch\",\n                \"frequency\": 1,\n            },\n        }\n\n    sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        opt,\n        mode=sch_cfg.mode,\n        factor=sch_cfg.factor,\n        patience=sch_cfg.patience,\n        threshold=sch_cfg.threshold,\n        min_lr=sch_cfg.min_lr,\n    )\n\n    return {\n        \"optimizer\": opt,\n        \"lr_scheduler\": {\n            \"name\": \"lr\",\n            \"scheduler\": sch,\n            \"monitor\": LOSS_KEY,\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        },\n    }\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.PINNModule.on_fit_start","title":"<code>on_fit_start() -&gt; None</code>","text":"<p>Called when fit begins. Resolves validation sources using loaded data.</p> Source code in <code>src/anypinn/lightning/module.py</code> <pre><code>@override\ndef on_fit_start(self) -&gt; None:\n    \"\"\"\n    Called when fit begins. Resolves validation sources using loaded data.\n    \"\"\"\n    self.problem.inject_context(self.trainer.datamodule.context)  # type: ignore\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.PINNModule.on_predict_start","title":"<code>on_predict_start() -&gt; None</code>","text":"<p>Called when predict begins. Resolves validation sources using loaded data.</p> Source code in <code>src/anypinn/lightning/module.py</code> <pre><code>@override\ndef on_predict_start(self) -&gt; None:\n    \"\"\"\n    Called when predict begins. Resolves validation sources using loaded data.\n    \"\"\"\n    self.problem.inject_context(self.trainer.datamodule.context)  # type: ignore\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.PINNModule.predict_step","title":"<code>predict_step(batch: PredictionBatch, batch_idx: int) -&gt; Predictions</code>","text":"<p>Performs a prediction step.</p> Source code in <code>src/anypinn/lightning/module.py</code> <pre><code>@override\ndef predict_step(self, batch: PredictionBatch, batch_idx: int) -&gt; Predictions:\n    \"\"\"\n    Performs a prediction step.\n    \"\"\"\n    x_data, y_data = batch\n\n    (data_batch, predictions) = self.problem.predict((x_data, y_data))\n    true_values = self.problem.true_values(x_data)\n\n    return (data_batch, predictions, true_values)\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.PINNModule.training_step","title":"<code>training_step(batch: TrainingBatch, batch_idx: int) -&gt; Tensor</code>","text":"<p>Performs a single training step. Calculates total loss from the problem.</p> Source code in <code>src/anypinn/lightning/module.py</code> <pre><code>@override\ndef training_step(self, batch: TrainingBatch, batch_idx: int) -&gt; Tensor:\n    \"\"\"\n    Performs a single training step.\n    Calculates total loss from the problem.\n    \"\"\"\n    return self.problem.training_loss(batch, self._log)\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.PredictionsWriter","title":"<code>PredictionsWriter</code>","text":"<p>               Bases: <code>BasePredictionWriter</code></p> <p>Callback to write predictions to disk at the end of an epoch.</p> <p>Parameters:</p> Name Type Description Default <code>predictions_path</code> <code>Path | None</code> <p>Path to save the predictions tensor/object.</p> <code>None</code> <code>batch_indices_path</code> <code>Path | None</code> <p>Path to save the batch indices.</p> <code>None</code> <code>on_prediction</code> <code>HookFn | None</code> <p>Optional hook function called when predictions are ready.</p> <code>None</code> <code>write_interval</code> <code>Literal['batch', 'epoch', 'batch_and_epoch']</code> <p>Interval to write predictions (\"batch\", \"epoch\", \"batch_and_epoch\").</p> <code>'epoch'</code> Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>class PredictionsWriter(BasePredictionWriter):\n    \"\"\"\n    Callback to write predictions to disk at the end of an epoch.\n\n    Args:\n        predictions_path: Path to save the predictions tensor/object.\n        batch_indices_path: Path to save the batch indices.\n        on_prediction: Optional hook function called when predictions are ready.\n        write_interval: Interval to write predictions (\"batch\", \"epoch\", \"batch_and_epoch\").\n    \"\"\"\n\n    def __init__(\n        self,\n        predictions_path: Path | None = None,\n        batch_indices_path: Path | None = None,\n        on_prediction: HookFn | None = None,\n        write_interval: Literal[\"batch\", \"epoch\", \"batch_and_epoch\"] = \"epoch\",\n    ):\n        super().__init__(write_interval)\n        self.predictions_path = predictions_path\n        self.batch_indices_path = batch_indices_path\n        self.on_prediction = on_prediction\n\n    @override\n    def write_on_epoch_end(\n        self,\n        trainer: Trainer,\n        pl_module: LightningModule,\n        predictions: Sequence[Predictions],\n        batch_indices: Sequence[Any],\n    ) -&gt; None:\n        \"\"\"\n        Writes predictions to disk or calls the hook at the end of the epoch.\n        \"\"\"\n        if self.on_prediction is not None:\n            self.on_prediction(trainer, pl_module, predictions, batch_indices)\n\n        if self.predictions_path is not None:\n            torch.save(predictions, self.predictions_path)\n\n        if self.batch_indices_path is not None:\n            torch.save(batch_indices, self.batch_indices_path)\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.PredictionsWriter.batch_indices_path","title":"<code>batch_indices_path = batch_indices_path</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/#anypinn.lightning.PredictionsWriter.on_prediction","title":"<code>on_prediction = on_prediction</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/#anypinn.lightning.PredictionsWriter.predictions_path","title":"<code>predictions_path = predictions_path</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/#anypinn.lightning.PredictionsWriter.__init__","title":"<code>__init__(predictions_path: Path | None = None, batch_indices_path: Path | None = None, on_prediction: HookFn | None = None, write_interval: Literal['batch', 'epoch', 'batch_and_epoch'] = 'epoch')</code>","text":"Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>def __init__(\n    self,\n    predictions_path: Path | None = None,\n    batch_indices_path: Path | None = None,\n    on_prediction: HookFn | None = None,\n    write_interval: Literal[\"batch\", \"epoch\", \"batch_and_epoch\"] = \"epoch\",\n):\n    super().__init__(write_interval)\n    self.predictions_path = predictions_path\n    self.batch_indices_path = batch_indices_path\n    self.on_prediction = on_prediction\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.PredictionsWriter.write_on_epoch_end","title":"<code>write_on_epoch_end(trainer: Trainer, pl_module: LightningModule, predictions: Sequence[Predictions], batch_indices: Sequence[Any]) -&gt; None</code>","text":"<p>Writes predictions to disk or calls the hook at the end of the epoch.</p> Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>@override\ndef write_on_epoch_end(\n    self,\n    trainer: Trainer,\n    pl_module: LightningModule,\n    predictions: Sequence[Predictions],\n    batch_indices: Sequence[Any],\n) -&gt; None:\n    \"\"\"\n    Writes predictions to disk or calls the hook at the end of the epoch.\n    \"\"\"\n    if self.on_prediction is not None:\n        self.on_prediction(trainer, pl_module, predictions, batch_indices)\n\n    if self.predictions_path is not None:\n        torch.save(predictions, self.predictions_path)\n\n    if self.batch_indices_path is not None:\n        torch.save(batch_indices, self.batch_indices_path)\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.SMMAStopping","title":"<code>SMMAStopping</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Early stopping callback based on the Smoothed Moving Average (SMMA) of the loss. Stops training if the relative improvement of the SMMA drops below a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SMMAStoppingConfig</code> <p>Configuration for SMMA stopping (window, threshold, lookback).</p> required <code>loss_key</code> <code>str</code> <p>The metric key to monitor (e.g., 'loss').</p> required <code>log_key</code> <code>str</code> <p>Key to log the computed SMMA value.</p> <code>SMMA_KEY</code> Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>class SMMAStopping(Callback):\n    \"\"\"\n    Early stopping callback based on the Smoothed Moving Average (SMMA) of the loss.\n    Stops training if the relative improvement of the SMMA drops below a threshold.\n\n    Args:\n        config: Configuration for SMMA stopping (window, threshold, lookback).\n        loss_key: The metric key to monitor (e.g., 'loss').\n        log_key: Key to log the computed SMMA value.\n    \"\"\"\n\n    def __init__(self, config: SMMAStoppingConfig, loss_key: str, log_key: str = SMMA_KEY):\n        super().__init__()\n        self.config = config\n        self.loss_key = loss_key\n        self.log_key = log_key\n        self.loss_buffer: list[float] = []\n        self.smma_buffer: deque[float] = deque(maxlen=self.config.lookback)\n\n    @override\n    def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n        \"\"\"\n        Called when the train epoch ends. Updates SMMA and checks stopping condition.\n        \"\"\"\n        # phase 0: get the loss\n        loss_t = trainer.callback_metrics.get(self.loss_key)\n        if loss_t is None:\n            return\n\n        loss = loss_t.item()\n        n = self.config.window\n\n        # phase 1: collect first `window` losses\n        if len(self.loss_buffer) &lt;= n:\n            self.loss_buffer.append(loss)\n            return\n\n        # phase 1.5: compute the first average\n        if len(self.smma_buffer) == 0:\n            first_smma = sum(self.loss_buffer) / n\n            self.smma_buffer.append(first_smma)\n            return\n\n        # phase 2: compute the first `lookback` Smoothed Moving Average (SMMA)\n        smma = self.smma_buffer[-1]\n        smma = ((n - 1) * smma + loss) / n\n        self.smma_buffer.append(smma)\n\n        pl_module.log(self.log_key, smma)\n        if len(self.smma_buffer) &lt; self.config.lookback:\n            return\n\n        # phase 3: compute the improvement between the current and the `lookback` SMMA\n        smma_lookback = self.smma_buffer[0]\n        improvement = (smma_lookback - smma) / smma_lookback\n\n        if 0 &lt; improvement &lt; self.config.threshold:\n            trainer.should_stop = True\n            print(\n                f\"\\nStopping training: SMMA improvement over {self.config.lookback} \"\n                f\"epochs ({improvement:.2%}) below threshold ({self.config.threshold:.2%})\"\n            )\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.SMMAStopping.config","title":"<code>config = config</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/#anypinn.lightning.SMMAStopping.log_key","title":"<code>log_key = log_key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/#anypinn.lightning.SMMAStopping.loss_buffer","title":"<code>loss_buffer: list[float] = []</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/#anypinn.lightning.SMMAStopping.loss_key","title":"<code>loss_key = loss_key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/#anypinn.lightning.SMMAStopping.smma_buffer","title":"<code>smma_buffer: deque[float] = deque(maxlen=(self.config.lookback))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/#anypinn.lightning.SMMAStopping.__init__","title":"<code>__init__(config: SMMAStoppingConfig, loss_key: str, log_key: str = SMMA_KEY)</code>","text":"Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>def __init__(self, config: SMMAStoppingConfig, loss_key: str, log_key: str = SMMA_KEY):\n    super().__init__()\n    self.config = config\n    self.loss_key = loss_key\n    self.log_key = log_key\n    self.loss_buffer: list[float] = []\n    self.smma_buffer: deque[float] = deque(maxlen=self.config.lookback)\n</code></pre>"},{"location":"reference/anypinn/lightning/#anypinn.lightning.SMMAStopping.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer: Trainer, pl_module: LightningModule) -&gt; None</code>","text":"<p>Called when the train epoch ends. Updates SMMA and checks stopping condition.</p> Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>@override\ndef on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n    \"\"\"\n    Called when the train epoch ends. Updates SMMA and checks stopping condition.\n    \"\"\"\n    # phase 0: get the loss\n    loss_t = trainer.callback_metrics.get(self.loss_key)\n    if loss_t is None:\n        return\n\n    loss = loss_t.item()\n    n = self.config.window\n\n    # phase 1: collect first `window` losses\n    if len(self.loss_buffer) &lt;= n:\n        self.loss_buffer.append(loss)\n        return\n\n    # phase 1.5: compute the first average\n    if len(self.smma_buffer) == 0:\n        first_smma = sum(self.loss_buffer) / n\n        self.smma_buffer.append(first_smma)\n        return\n\n    # phase 2: compute the first `lookback` Smoothed Moving Average (SMMA)\n    smma = self.smma_buffer[-1]\n    smma = ((n - 1) * smma + loss) / n\n    self.smma_buffer.append(smma)\n\n    pl_module.log(self.log_key, smma)\n    if len(self.smma_buffer) &lt; self.config.lookback:\n        return\n\n    # phase 3: compute the improvement between the current and the `lookback` SMMA\n    smma_lookback = self.smma_buffer[0]\n    improvement = (smma_lookback - smma) / smma_lookback\n\n    if 0 &lt; improvement &lt; self.config.threshold:\n        trainer.should_stop = True\n        print(\n            f\"\\nStopping training: SMMA improvement over {self.config.lookback} \"\n            f\"epochs ({improvement:.2%}) below threshold ({self.config.threshold:.2%})\"\n        )\n</code></pre>"},{"location":"reference/anypinn/lightning/callbacks/","title":"callbacks","text":""},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks","title":"<code>anypinn.lightning.callbacks</code>","text":""},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.FormatFn","title":"<code>FormatFn: TypeAlias = Callable[[str, Metric], Metric]</code>  <code>module-attribute</code>","text":"<p>A function that formats a metric for display in the progress bar. Takes the key and value of the metric, and returns the formatted metric.</p>"},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.HookFn","title":"<code>HookFn: TypeAlias = Callable[[Trainer, LightningModule, Sequence[Predictions], Sequence[Any]], None]</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.Metric","title":"<code>Metric: TypeAlias = int | str | float | dict[str, float]</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.SMMA_KEY","title":"<code>SMMA_KEY = 'loss/smma'</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.AdaptiveCollocationCallback","title":"<code>AdaptiveCollocationCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Refreshes the collocation pool every N epochs using AdaptiveSampler.</p> <p>Requires the DataModule to be configured with <code>collocation_sampler=\"adaptive\"</code> and a <code>ResidualScorer</code> passed to <code>PINNDataModule(residual_scorer=...)</code>.</p> <p>Because the scorer typically closes over the <code>Problem</code>, it automatically uses the model's current weights on every call \u2014 no additional injection step needed.</p> <p>Parameters:</p> Name Type Description Default <code>every_n_epochs</code> <code>int</code> <p>How often (in epochs) to resample. Default: 1.</p> <code>1</code> Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>class AdaptiveCollocationCallback(Callback):\n    \"\"\"Refreshes the collocation pool every N epochs using AdaptiveSampler.\n\n    Requires the DataModule to be configured with ``collocation_sampler=\"adaptive\"``\n    and a ``ResidualScorer`` passed to ``PINNDataModule(residual_scorer=...)``.\n\n    Because the scorer typically closes over the ``Problem``, it automatically uses\n    the model's current weights on every call \u2014 no additional injection step needed.\n\n    Args:\n        every_n_epochs: How often (in epochs) to resample. Default: 1.\n    \"\"\"\n\n    def __init__(self, every_n_epochs: int = 1) -&gt; None:\n        super().__init__()\n        if every_n_epochs &lt; 1:\n            raise ValueError(f\"every_n_epochs must be &gt;= 1, got {every_n_epochs}.\")\n        self._every_n = every_n_epochs\n\n    @override\n    def on_fit_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n        \"\"\"Validate at fit start that the DataModule uses AdaptiveSampler.\"\"\"\n        dm = trainer.datamodule  # type: ignore[attr-defined]\n        if not isinstance(getattr(dm, \"_sampler\", None), AdaptiveSampler):\n            raise TypeError(\n                \"AdaptiveCollocationCallback requires the DataModule to use \"\n                \"collocation_sampler='adaptive'. Got: \"\n                f\"{type(getattr(dm, '_sampler', None)).__name__}.\"\n            )\n\n    @override\n    def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n        \"\"\"Resample collocation points using the current model weights.\"\"\"\n        if (trainer.current_epoch + 1) % self._every_n != 0:\n            return\n        dm = trainer.datamodule  # type: ignore[attr-defined]\n        n = dm.hp.training_data.collocations\n        new_coll = dm._sampler.sample(n, dm._domain)\n        dm.pinn_ds.x_coll = new_coll\n</code></pre>"},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.AdaptiveCollocationCallback.__init__","title":"<code>__init__(every_n_epochs: int = 1) -&gt; None</code>","text":"Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>def __init__(self, every_n_epochs: int = 1) -&gt; None:\n    super().__init__()\n    if every_n_epochs &lt; 1:\n        raise ValueError(f\"every_n_epochs must be &gt;= 1, got {every_n_epochs}.\")\n    self._every_n = every_n_epochs\n</code></pre>"},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.AdaptiveCollocationCallback.on_fit_start","title":"<code>on_fit_start(trainer: Trainer, pl_module: LightningModule) -&gt; None</code>","text":"<p>Validate at fit start that the DataModule uses AdaptiveSampler.</p> Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>@override\ndef on_fit_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n    \"\"\"Validate at fit start that the DataModule uses AdaptiveSampler.\"\"\"\n    dm = trainer.datamodule  # type: ignore[attr-defined]\n    if not isinstance(getattr(dm, \"_sampler\", None), AdaptiveSampler):\n        raise TypeError(\n            \"AdaptiveCollocationCallback requires the DataModule to use \"\n            \"collocation_sampler='adaptive'. Got: \"\n            f\"{type(getattr(dm, '_sampler', None)).__name__}.\"\n        )\n</code></pre>"},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.AdaptiveCollocationCallback.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer: Trainer, pl_module: LightningModule) -&gt; None</code>","text":"<p>Resample collocation points using the current model weights.</p> Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>@override\ndef on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n    \"\"\"Resample collocation points using the current model weights.\"\"\"\n    if (trainer.current_epoch + 1) % self._every_n != 0:\n        return\n    dm = trainer.datamodule  # type: ignore[attr-defined]\n    n = dm.hp.training_data.collocations\n    new_coll = dm._sampler.sample(n, dm._domain)\n    dm.pinn_ds.x_coll = new_coll\n</code></pre>"},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.DataScaling","title":"<code>DataScaling</code>","text":"<p>               Bases: <code>DataCallback</code></p> <p>Callback to transform the data and collocation points.</p> <p>Scales x to [0, 1] and applies per-series scaling factors to y.</p> <p>Parameters:</p> Name Type Description Default <code>y_scale</code> <code>float | Sequence[float]</code> <p>Scaling factor(s) for y data. Can be: - A single float: applied to all series - A sequence of floats: one per series (length must match number of series)</p> required Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>class DataScaling(DataCallback):\n    \"\"\"\n    Callback to transform the data and collocation points.\n\n    Scales x to [0, 1] and applies per-series scaling factors to y.\n\n    Args:\n        y_scale: Scaling factor(s) for y data. Can be:\n            - A single float: applied to all series\n            - A sequence of floats: one per series (length must match number of series)\n    \"\"\"\n\n    def __init__(self, y_scale: float | Sequence[float]):\n        self._y_scale_input = y_scale\n\n    @override\n    def transform_data(self, data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]:\n        x, y = data\n\n        x_min, x_max = x.min(), x.max()\n        self.x_scale = x_max - x_min\n        x = (x - x_min) / self.x_scale\n\n        coll_min, coll_max = coll.min(), coll.max()\n        coll = (coll - coll_min) / (coll_max - coll_min)\n\n        n_series = y.shape[1]\n\n        if isinstance(self._y_scale_input, (int, float)):\n            scale_list = [float(self._y_scale_input)] * n_series\n        else:\n            scale_list = list(self._y_scale_input)\n            if len(scale_list) != n_series:\n                raise ValueError(\n                    f\"y_scale has {len(scale_list)} elements but data has {n_series} series\"\n                )\n\n        self.y_scale = torch.tensor(scale_list, dtype=y.dtype, device=y.device)\n\n        # Reshape scale for broadcasting against (n, k, 1)\n        scale_tensor = self.y_scale.view(1, -1, 1)\n\n        return (x, y * scale_tensor), coll\n\n    @override\n    def on_after_setup(self, dm: PINNDataModule) -&gt; None:\n        \"\"\"Called after setup is complete.\"\"\"\n\n        for k in dm.validation:\n            orig_fn = dm.validation[k]\n            dm.validation[k] = (lambda fn, scale: lambda x: fn(x * scale))(orig_fn, self.x_scale)\n        return None\n</code></pre>"},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.DataScaling.__init__","title":"<code>__init__(y_scale: float | Sequence[float])</code>","text":"Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>def __init__(self, y_scale: float | Sequence[float]):\n    self._y_scale_input = y_scale\n</code></pre>"},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.DataScaling.on_after_setup","title":"<code>on_after_setup(dm: PINNDataModule) -&gt; None</code>","text":"<p>Called after setup is complete.</p> Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>@override\ndef on_after_setup(self, dm: PINNDataModule) -&gt; None:\n    \"\"\"Called after setup is complete.\"\"\"\n\n    for k in dm.validation:\n        orig_fn = dm.validation[k]\n        dm.validation[k] = (lambda fn, scale: lambda x: fn(x * scale))(orig_fn, self.x_scale)\n    return None\n</code></pre>"},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.DataScaling.transform_data","title":"<code>transform_data(data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]</code>","text":"Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>@override\ndef transform_data(self, data: DataBatch, coll: Tensor) -&gt; tuple[DataBatch, Tensor]:\n    x, y = data\n\n    x_min, x_max = x.min(), x.max()\n    self.x_scale = x_max - x_min\n    x = (x - x_min) / self.x_scale\n\n    coll_min, coll_max = coll.min(), coll.max()\n    coll = (coll - coll_min) / (coll_max - coll_min)\n\n    n_series = y.shape[1]\n\n    if isinstance(self._y_scale_input, (int, float)):\n        scale_list = [float(self._y_scale_input)] * n_series\n    else:\n        scale_list = list(self._y_scale_input)\n        if len(scale_list) != n_series:\n            raise ValueError(\n                f\"y_scale has {len(scale_list)} elements but data has {n_series} series\"\n            )\n\n    self.y_scale = torch.tensor(scale_list, dtype=y.dtype, device=y.device)\n\n    # Reshape scale for broadcasting against (n, k, 1)\n    scale_tensor = self.y_scale.view(1, -1, 1)\n\n    return (x, y * scale_tensor), coll\n</code></pre>"},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.FormattedProgressBar","title":"<code>FormattedProgressBar</code>","text":"<p>               Bases: <code>TQDMProgressBar</code></p> <p>Custom progress bar for training that formats metrics for better readability.</p> <p>This class extends the TQDMProgressBar to provide custom formatting for training metrics, particularly for the total loss and beta values.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>FormatFn</code> <p>Function to format the metric values.</p> required Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>class FormattedProgressBar(TQDMProgressBar):\n    \"\"\"\n    Custom progress bar for training that formats metrics for better readability.\n\n    This class extends the TQDMProgressBar to provide custom formatting for\n    training metrics, particularly for the total loss and beta values.\n\n    Args:\n        format: Function to format the metric values.\n    \"\"\"\n\n    def __init__(self, *args: Any, format: FormatFn, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n        self.format = format\n\n    @override\n    def get_metrics(self, *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n        \"\"\"\n        Format metrics for display in the progress bar.\n\n        Returns:\n            Dictionary of formatted metrics with:\n            - Total loss in scientific notation\n            - Beta value with 4 decimal places\n            - Other metrics as provided by the parent class\n        \"\"\"\n        items = super().get_metrics(*args, **kwargs)\n        items.pop(\"v_num\", None)\n        for key, value in items.items():\n            items[key] = self.format(key, value)\n\n        return items\n</code></pre>"},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.FormattedProgressBar.format","title":"<code>format = format</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.FormattedProgressBar.__init__","title":"<code>__init__(*args: Any, format: FormatFn, **kwargs: Any)</code>","text":"Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>def __init__(self, *args: Any, format: FormatFn, **kwargs: Any):\n    super().__init__(*args, **kwargs)\n    self.format = format\n</code></pre>"},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.FormattedProgressBar.get_metrics","title":"<code>get_metrics(*args: Any, **kwargs: Any) -&gt; dict[str, Any]</code>","text":"<p>Format metrics for display in the progress bar.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of formatted metrics with:</p> <code>dict[str, Any]</code> <ul> <li>Total loss in scientific notation</li> </ul> <code>dict[str, Any]</code> <ul> <li>Beta value with 4 decimal places</li> </ul> <code>dict[str, Any]</code> <ul> <li>Other metrics as provided by the parent class</li> </ul> Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>@override\ndef get_metrics(self, *args: Any, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"\n    Format metrics for display in the progress bar.\n\n    Returns:\n        Dictionary of formatted metrics with:\n        - Total loss in scientific notation\n        - Beta value with 4 decimal places\n        - Other metrics as provided by the parent class\n    \"\"\"\n    items = super().get_metrics(*args, **kwargs)\n    items.pop(\"v_num\", None)\n    for key, value in items.items():\n        items[key] = self.format(key, value)\n\n    return items\n</code></pre>"},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.PredictionsWriter","title":"<code>PredictionsWriter</code>","text":"<p>               Bases: <code>BasePredictionWriter</code></p> <p>Callback to write predictions to disk at the end of an epoch.</p> <p>Parameters:</p> Name Type Description Default <code>predictions_path</code> <code>Path | None</code> <p>Path to save the predictions tensor/object.</p> <code>None</code> <code>batch_indices_path</code> <code>Path | None</code> <p>Path to save the batch indices.</p> <code>None</code> <code>on_prediction</code> <code>HookFn | None</code> <p>Optional hook function called when predictions are ready.</p> <code>None</code> <code>write_interval</code> <code>Literal['batch', 'epoch', 'batch_and_epoch']</code> <p>Interval to write predictions (\"batch\", \"epoch\", \"batch_and_epoch\").</p> <code>'epoch'</code> Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>class PredictionsWriter(BasePredictionWriter):\n    \"\"\"\n    Callback to write predictions to disk at the end of an epoch.\n\n    Args:\n        predictions_path: Path to save the predictions tensor/object.\n        batch_indices_path: Path to save the batch indices.\n        on_prediction: Optional hook function called when predictions are ready.\n        write_interval: Interval to write predictions (\"batch\", \"epoch\", \"batch_and_epoch\").\n    \"\"\"\n\n    def __init__(\n        self,\n        predictions_path: Path | None = None,\n        batch_indices_path: Path | None = None,\n        on_prediction: HookFn | None = None,\n        write_interval: Literal[\"batch\", \"epoch\", \"batch_and_epoch\"] = \"epoch\",\n    ):\n        super().__init__(write_interval)\n        self.predictions_path = predictions_path\n        self.batch_indices_path = batch_indices_path\n        self.on_prediction = on_prediction\n\n    @override\n    def write_on_epoch_end(\n        self,\n        trainer: Trainer,\n        pl_module: LightningModule,\n        predictions: Sequence[Predictions],\n        batch_indices: Sequence[Any],\n    ) -&gt; None:\n        \"\"\"\n        Writes predictions to disk or calls the hook at the end of the epoch.\n        \"\"\"\n        if self.on_prediction is not None:\n            self.on_prediction(trainer, pl_module, predictions, batch_indices)\n\n        if self.predictions_path is not None:\n            torch.save(predictions, self.predictions_path)\n\n        if self.batch_indices_path is not None:\n            torch.save(batch_indices, self.batch_indices_path)\n</code></pre>"},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.PredictionsWriter.batch_indices_path","title":"<code>batch_indices_path = batch_indices_path</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.PredictionsWriter.on_prediction","title":"<code>on_prediction = on_prediction</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.PredictionsWriter.predictions_path","title":"<code>predictions_path = predictions_path</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.PredictionsWriter.__init__","title":"<code>__init__(predictions_path: Path | None = None, batch_indices_path: Path | None = None, on_prediction: HookFn | None = None, write_interval: Literal['batch', 'epoch', 'batch_and_epoch'] = 'epoch')</code>","text":"Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>def __init__(\n    self,\n    predictions_path: Path | None = None,\n    batch_indices_path: Path | None = None,\n    on_prediction: HookFn | None = None,\n    write_interval: Literal[\"batch\", \"epoch\", \"batch_and_epoch\"] = \"epoch\",\n):\n    super().__init__(write_interval)\n    self.predictions_path = predictions_path\n    self.batch_indices_path = batch_indices_path\n    self.on_prediction = on_prediction\n</code></pre>"},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.PredictionsWriter.write_on_epoch_end","title":"<code>write_on_epoch_end(trainer: Trainer, pl_module: LightningModule, predictions: Sequence[Predictions], batch_indices: Sequence[Any]) -&gt; None</code>","text":"<p>Writes predictions to disk or calls the hook at the end of the epoch.</p> Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>@override\ndef write_on_epoch_end(\n    self,\n    trainer: Trainer,\n    pl_module: LightningModule,\n    predictions: Sequence[Predictions],\n    batch_indices: Sequence[Any],\n) -&gt; None:\n    \"\"\"\n    Writes predictions to disk or calls the hook at the end of the epoch.\n    \"\"\"\n    if self.on_prediction is not None:\n        self.on_prediction(trainer, pl_module, predictions, batch_indices)\n\n    if self.predictions_path is not None:\n        torch.save(predictions, self.predictions_path)\n\n    if self.batch_indices_path is not None:\n        torch.save(batch_indices, self.batch_indices_path)\n</code></pre>"},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.SMMAStopping","title":"<code>SMMAStopping</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Early stopping callback based on the Smoothed Moving Average (SMMA) of the loss. Stops training if the relative improvement of the SMMA drops below a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SMMAStoppingConfig</code> <p>Configuration for SMMA stopping (window, threshold, lookback).</p> required <code>loss_key</code> <code>str</code> <p>The metric key to monitor (e.g., 'loss').</p> required <code>log_key</code> <code>str</code> <p>Key to log the computed SMMA value.</p> <code>SMMA_KEY</code> Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>class SMMAStopping(Callback):\n    \"\"\"\n    Early stopping callback based on the Smoothed Moving Average (SMMA) of the loss.\n    Stops training if the relative improvement of the SMMA drops below a threshold.\n\n    Args:\n        config: Configuration for SMMA stopping (window, threshold, lookback).\n        loss_key: The metric key to monitor (e.g., 'loss').\n        log_key: Key to log the computed SMMA value.\n    \"\"\"\n\n    def __init__(self, config: SMMAStoppingConfig, loss_key: str, log_key: str = SMMA_KEY):\n        super().__init__()\n        self.config = config\n        self.loss_key = loss_key\n        self.log_key = log_key\n        self.loss_buffer: list[float] = []\n        self.smma_buffer: deque[float] = deque(maxlen=self.config.lookback)\n\n    @override\n    def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n        \"\"\"\n        Called when the train epoch ends. Updates SMMA and checks stopping condition.\n        \"\"\"\n        # phase 0: get the loss\n        loss_t = trainer.callback_metrics.get(self.loss_key)\n        if loss_t is None:\n            return\n\n        loss = loss_t.item()\n        n = self.config.window\n\n        # phase 1: collect first `window` losses\n        if len(self.loss_buffer) &lt;= n:\n            self.loss_buffer.append(loss)\n            return\n\n        # phase 1.5: compute the first average\n        if len(self.smma_buffer) == 0:\n            first_smma = sum(self.loss_buffer) / n\n            self.smma_buffer.append(first_smma)\n            return\n\n        # phase 2: compute the first `lookback` Smoothed Moving Average (SMMA)\n        smma = self.smma_buffer[-1]\n        smma = ((n - 1) * smma + loss) / n\n        self.smma_buffer.append(smma)\n\n        pl_module.log(self.log_key, smma)\n        if len(self.smma_buffer) &lt; self.config.lookback:\n            return\n\n        # phase 3: compute the improvement between the current and the `lookback` SMMA\n        smma_lookback = self.smma_buffer[0]\n        improvement = (smma_lookback - smma) / smma_lookback\n\n        if 0 &lt; improvement &lt; self.config.threshold:\n            trainer.should_stop = True\n            print(\n                f\"\\nStopping training: SMMA improvement over {self.config.lookback} \"\n                f\"epochs ({improvement:.2%}) below threshold ({self.config.threshold:.2%})\"\n            )\n</code></pre>"},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.SMMAStopping.config","title":"<code>config = config</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.SMMAStopping.log_key","title":"<code>log_key = log_key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.SMMAStopping.loss_buffer","title":"<code>loss_buffer: list[float] = []</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.SMMAStopping.loss_key","title":"<code>loss_key = loss_key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.SMMAStopping.smma_buffer","title":"<code>smma_buffer: deque[float] = deque(maxlen=(self.config.lookback))</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.SMMAStopping.__init__","title":"<code>__init__(config: SMMAStoppingConfig, loss_key: str, log_key: str = SMMA_KEY)</code>","text":"Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>def __init__(self, config: SMMAStoppingConfig, loss_key: str, log_key: str = SMMA_KEY):\n    super().__init__()\n    self.config = config\n    self.loss_key = loss_key\n    self.log_key = log_key\n    self.loss_buffer: list[float] = []\n    self.smma_buffer: deque[float] = deque(maxlen=self.config.lookback)\n</code></pre>"},{"location":"reference/anypinn/lightning/callbacks/#anypinn.lightning.callbacks.SMMAStopping.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer: Trainer, pl_module: LightningModule) -&gt; None</code>","text":"<p>Called when the train epoch ends. Updates SMMA and checks stopping condition.</p> Source code in <code>src/anypinn/lightning/callbacks.py</code> <pre><code>@override\ndef on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n    \"\"\"\n    Called when the train epoch ends. Updates SMMA and checks stopping condition.\n    \"\"\"\n    # phase 0: get the loss\n    loss_t = trainer.callback_metrics.get(self.loss_key)\n    if loss_t is None:\n        return\n\n    loss = loss_t.item()\n    n = self.config.window\n\n    # phase 1: collect first `window` losses\n    if len(self.loss_buffer) &lt;= n:\n        self.loss_buffer.append(loss)\n        return\n\n    # phase 1.5: compute the first average\n    if len(self.smma_buffer) == 0:\n        first_smma = sum(self.loss_buffer) / n\n        self.smma_buffer.append(first_smma)\n        return\n\n    # phase 2: compute the first `lookback` Smoothed Moving Average (SMMA)\n    smma = self.smma_buffer[-1]\n    smma = ((n - 1) * smma + loss) / n\n    self.smma_buffer.append(smma)\n\n    pl_module.log(self.log_key, smma)\n    if len(self.smma_buffer) &lt; self.config.lookback:\n        return\n\n    # phase 3: compute the improvement between the current and the `lookback` SMMA\n    smma_lookback = self.smma_buffer[0]\n    improvement = (smma_lookback - smma) / smma_lookback\n\n    if 0 &lt; improvement &lt; self.config.threshold:\n        trainer.should_stop = True\n        print(\n            f\"\\nStopping training: SMMA improvement over {self.config.lookback} \"\n            f\"epochs ({improvement:.2%}) below threshold ({self.config.threshold:.2%})\"\n        )\n</code></pre>"},{"location":"reference/anypinn/lightning/module/","title":"module","text":""},{"location":"reference/anypinn/lightning/module/#anypinn.lightning.module","title":"<code>anypinn.lightning.module</code>","text":""},{"location":"reference/anypinn/lightning/module/#anypinn.lightning.module.PINNModule","title":"<code>PINNModule</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Generic PINN Lightning module. Expects external Problem + Sampler + optimizer config.</p> <p>Parameters:</p> Name Type Description Default <code>problem</code> <code>Problem</code> <p>The PINN problem definition (constraints, fields, etc.).</p> required <code>hp</code> <code>PINNHyperparameters</code> <p>Hyperparameters for training.</p> required Source code in <code>src/anypinn/lightning/module.py</code> <pre><code>class PINNModule(pl.LightningModule):\n    \"\"\"\n    Generic PINN Lightning module.\n    Expects external Problem + Sampler + optimizer config.\n\n    Args:\n        problem: The PINN problem definition (constraints, fields, etc.).\n        hp: Hyperparameters for training.\n    \"\"\"\n\n    def __init__(\n        self,\n        problem: Problem,\n        hp: PINNHyperparameters,\n    ):\n        super().__init__()\n        self.save_hyperparameters(ignore=[\"problem\"])\n\n        self.problem = problem\n        self.hp = hp\n\n        def _log(key: str, value: Tensor, progress_bar: bool = False) -&gt; None:\n            self.log(\n                key,\n                value,\n                on_step=False,\n                on_epoch=True,\n                prog_bar=progress_bar,\n                batch_size=hp.training_data.batch_size,\n            )\n\n        self._log = cast(LogFn, _log)\n\n    @override\n    def on_fit_start(self) -&gt; None:\n        \"\"\"\n        Called when fit begins. Resolves validation sources using loaded data.\n        \"\"\"\n        self.problem.inject_context(self.trainer.datamodule.context)  # type: ignore\n\n    @override\n    def on_predict_start(self) -&gt; None:\n        \"\"\"\n        Called when predict begins. Resolves validation sources using loaded data.\n        \"\"\"\n        self.problem.inject_context(self.trainer.datamodule.context)  # type: ignore\n\n    @override\n    def training_step(self, batch: TrainingBatch, batch_idx: int) -&gt; Tensor:\n        \"\"\"\n        Performs a single training step.\n        Calculates total loss from the problem.\n        \"\"\"\n        return self.problem.training_loss(batch, self._log)\n\n    @override\n    def predict_step(self, batch: PredictionBatch, batch_idx: int) -&gt; Predictions:\n        \"\"\"\n        Performs a prediction step.\n        \"\"\"\n        x_data, y_data = batch\n\n        (data_batch, predictions) = self.problem.predict((x_data, y_data))\n        true_values = self.problem.true_values(x_data)\n\n        return (data_batch, predictions, true_values)\n\n    @override\n    def configure_optimizers(self) -&gt; OptimizerLRScheduler:\n        \"\"\"\n        Configures the optimizer and learning rate scheduler.\n        \"\"\"\n        opt_cfg = self.hp.optimizer\n        if isinstance(opt_cfg, LBFGSConfig):\n            opt = torch.optim.LBFGS(\n                self.parameters(),\n                lr=opt_cfg.lr,\n                max_iter=opt_cfg.max_iter,\n                max_eval=opt_cfg.max_eval,\n                history_size=opt_cfg.history_size,\n                line_search_fn=opt_cfg.line_search_fn,\n            )\n        elif isinstance(opt_cfg, AdamConfig):\n            opt = torch.optim.Adam(\n                self.parameters(),\n                lr=opt_cfg.lr,\n                betas=opt_cfg.betas,\n                weight_decay=opt_cfg.weight_decay,\n            )\n        else:\n            opt = torch.optim.Adam(self.parameters(), lr=self.hp.lr)\n\n        sch_cfg = self.hp.scheduler\n        if not sch_cfg:\n            return opt\n\n        if isinstance(sch_cfg, CosineAnnealingConfig):\n            sch = torch.optim.lr_scheduler.CosineAnnealingLR(\n                opt,\n                T_max=sch_cfg.T_max,\n                eta_min=sch_cfg.eta_min,\n            )\n            return {\n                \"optimizer\": opt,\n                \"lr_scheduler\": {\n                    \"name\": \"lr\",\n                    \"scheduler\": sch,\n                    \"interval\": \"epoch\",\n                    \"frequency\": 1,\n                },\n            }\n\n        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            opt,\n            mode=sch_cfg.mode,\n            factor=sch_cfg.factor,\n            patience=sch_cfg.patience,\n            threshold=sch_cfg.threshold,\n            min_lr=sch_cfg.min_lr,\n        )\n\n        return {\n            \"optimizer\": opt,\n            \"lr_scheduler\": {\n                \"name\": \"lr\",\n                \"scheduler\": sch,\n                \"monitor\": LOSS_KEY,\n                \"interval\": \"epoch\",\n                \"frequency\": 1,\n            },\n        }\n</code></pre>"},{"location":"reference/anypinn/lightning/module/#anypinn.lightning.module.PINNModule.hp","title":"<code>hp = hp</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/module/#anypinn.lightning.module.PINNModule.problem","title":"<code>problem = problem</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/lightning/module/#anypinn.lightning.module.PINNModule.__init__","title":"<code>__init__(problem: Problem, hp: PINNHyperparameters)</code>","text":"Source code in <code>src/anypinn/lightning/module.py</code> <pre><code>def __init__(\n    self,\n    problem: Problem,\n    hp: PINNHyperparameters,\n):\n    super().__init__()\n    self.save_hyperparameters(ignore=[\"problem\"])\n\n    self.problem = problem\n    self.hp = hp\n\n    def _log(key: str, value: Tensor, progress_bar: bool = False) -&gt; None:\n        self.log(\n            key,\n            value,\n            on_step=False,\n            on_epoch=True,\n            prog_bar=progress_bar,\n            batch_size=hp.training_data.batch_size,\n        )\n\n    self._log = cast(LogFn, _log)\n</code></pre>"},{"location":"reference/anypinn/lightning/module/#anypinn.lightning.module.PINNModule.configure_optimizers","title":"<code>configure_optimizers() -&gt; OptimizerLRScheduler</code>","text":"<p>Configures the optimizer and learning rate scheduler.</p> Source code in <code>src/anypinn/lightning/module.py</code> <pre><code>@override\ndef configure_optimizers(self) -&gt; OptimizerLRScheduler:\n    \"\"\"\n    Configures the optimizer and learning rate scheduler.\n    \"\"\"\n    opt_cfg = self.hp.optimizer\n    if isinstance(opt_cfg, LBFGSConfig):\n        opt = torch.optim.LBFGS(\n            self.parameters(),\n            lr=opt_cfg.lr,\n            max_iter=opt_cfg.max_iter,\n            max_eval=opt_cfg.max_eval,\n            history_size=opt_cfg.history_size,\n            line_search_fn=opt_cfg.line_search_fn,\n        )\n    elif isinstance(opt_cfg, AdamConfig):\n        opt = torch.optim.Adam(\n            self.parameters(),\n            lr=opt_cfg.lr,\n            betas=opt_cfg.betas,\n            weight_decay=opt_cfg.weight_decay,\n        )\n    else:\n        opt = torch.optim.Adam(self.parameters(), lr=self.hp.lr)\n\n    sch_cfg = self.hp.scheduler\n    if not sch_cfg:\n        return opt\n\n    if isinstance(sch_cfg, CosineAnnealingConfig):\n        sch = torch.optim.lr_scheduler.CosineAnnealingLR(\n            opt,\n            T_max=sch_cfg.T_max,\n            eta_min=sch_cfg.eta_min,\n        )\n        return {\n            \"optimizer\": opt,\n            \"lr_scheduler\": {\n                \"name\": \"lr\",\n                \"scheduler\": sch,\n                \"interval\": \"epoch\",\n                \"frequency\": 1,\n            },\n        }\n\n    sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        opt,\n        mode=sch_cfg.mode,\n        factor=sch_cfg.factor,\n        patience=sch_cfg.patience,\n        threshold=sch_cfg.threshold,\n        min_lr=sch_cfg.min_lr,\n    )\n\n    return {\n        \"optimizer\": opt,\n        \"lr_scheduler\": {\n            \"name\": \"lr\",\n            \"scheduler\": sch,\n            \"monitor\": LOSS_KEY,\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        },\n    }\n</code></pre>"},{"location":"reference/anypinn/lightning/module/#anypinn.lightning.module.PINNModule.on_fit_start","title":"<code>on_fit_start() -&gt; None</code>","text":"<p>Called when fit begins. Resolves validation sources using loaded data.</p> Source code in <code>src/anypinn/lightning/module.py</code> <pre><code>@override\ndef on_fit_start(self) -&gt; None:\n    \"\"\"\n    Called when fit begins. Resolves validation sources using loaded data.\n    \"\"\"\n    self.problem.inject_context(self.trainer.datamodule.context)  # type: ignore\n</code></pre>"},{"location":"reference/anypinn/lightning/module/#anypinn.lightning.module.PINNModule.on_predict_start","title":"<code>on_predict_start() -&gt; None</code>","text":"<p>Called when predict begins. Resolves validation sources using loaded data.</p> Source code in <code>src/anypinn/lightning/module.py</code> <pre><code>@override\ndef on_predict_start(self) -&gt; None:\n    \"\"\"\n    Called when predict begins. Resolves validation sources using loaded data.\n    \"\"\"\n    self.problem.inject_context(self.trainer.datamodule.context)  # type: ignore\n</code></pre>"},{"location":"reference/anypinn/lightning/module/#anypinn.lightning.module.PINNModule.predict_step","title":"<code>predict_step(batch: PredictionBatch, batch_idx: int) -&gt; Predictions</code>","text":"<p>Performs a prediction step.</p> Source code in <code>src/anypinn/lightning/module.py</code> <pre><code>@override\ndef predict_step(self, batch: PredictionBatch, batch_idx: int) -&gt; Predictions:\n    \"\"\"\n    Performs a prediction step.\n    \"\"\"\n    x_data, y_data = batch\n\n    (data_batch, predictions) = self.problem.predict((x_data, y_data))\n    true_values = self.problem.true_values(x_data)\n\n    return (data_batch, predictions, true_values)\n</code></pre>"},{"location":"reference/anypinn/lightning/module/#anypinn.lightning.module.PINNModule.training_step","title":"<code>training_step(batch: TrainingBatch, batch_idx: int) -&gt; Tensor</code>","text":"<p>Performs a single training step. Calculates total loss from the problem.</p> Source code in <code>src/anypinn/lightning/module.py</code> <pre><code>@override\ndef training_step(self, batch: TrainingBatch, batch_idx: int) -&gt; Tensor:\n    \"\"\"\n    Performs a single training step.\n    Calculates total loss from the problem.\n    \"\"\"\n    return self.problem.training_loss(batch, self._log)\n</code></pre>"},{"location":"reference/anypinn/problems/","title":"problems","text":""},{"location":"reference/anypinn/problems/#anypinn.problems","title":"<code>anypinn.problems</code>","text":"<p>Problem templates and implementations.</p>"},{"location":"reference/anypinn/problems/#anypinn.problems.BCValueFn","title":"<code>BCValueFn: TypeAlias = Callable[[Tensor], Tensor]</code>  <code>module-attribute</code>","text":"<p>A callable that maps boundary coordinates (n_pts, d) \u2192 target values (n_pts, out_dim).</p>"},{"location":"reference/anypinn/problems/#anypinn.problems.PDEResidualFn","title":"<code>PDEResidualFn: TypeAlias = Callable[[Tensor, FieldsRegistry, ParamsRegistry], Tensor]</code>  <code>module-attribute</code>","text":"<p>A callable (x, fields, params) \u2192 residual tensor, expected to be zero at the solution.</p>"},{"location":"reference/anypinn/problems/#anypinn.problems.PredictDataFn","title":"<code>PredictDataFn: TypeAlias = Callable[[Tensor, FieldsRegistry, ParamsRegistry], Tensor]</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.__all__","title":"<code>__all__ = ['BCValueFn', 'BoundaryCondition', 'DataConstraint', 'DirichletBCConstraint', 'ICConstraint', 'NeumannBCConstraint', 'ODECallable', 'ODEHyperparameters', 'ODEInverseProblem', 'ODEProperties', 'PDEResidualConstraint', 'PDEResidualFn', 'PeriodicBCConstraint', 'PredictDataFn', 'ResidualsConstraint']</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.BoundaryCondition","title":"<code>BoundaryCondition</code>","text":"<p>Pairs a boundary region sampler with a prescribed value function.</p> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Callable[[int], Tensor]</code> <p>Callable <code>(n_pts: int) -&gt; Tensor</code> of shape <code>(n_pts, d)</code>. Called each training step to produce fresh boundary sample points.</p> required <code>value</code> <code>BCValueFn</code> <p>Callable <code>Tensor -&gt; Tensor</code> giving the target value or normal derivative at boundary coordinates.</p> required <code>n_pts</code> <code>int</code> <p>Number of boundary points sampled per training step.</p> <code>100</code> Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>class BoundaryCondition:\n    \"\"\"\n    Pairs a boundary region sampler with a prescribed value function.\n\n    Args:\n        sampler: Callable ``(n_pts: int) -&gt; Tensor`` of shape ``(n_pts, d)``.\n            Called each training step to produce fresh boundary sample points.\n        value: Callable ``Tensor -&gt; Tensor`` giving the target value or normal\n            derivative at boundary coordinates.\n        n_pts: Number of boundary points sampled per training step.\n    \"\"\"\n\n    def __init__(\n        self,\n        sampler: Callable[[int], Tensor],\n        value: BCValueFn,\n        n_pts: int = 100,\n    ):\n        self.sampler = sampler\n        self.value = value\n        self.n_pts = n_pts\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.BoundaryCondition.n_pts","title":"<code>n_pts = n_pts</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.BoundaryCondition.sampler","title":"<code>sampler = sampler</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.BoundaryCondition.value","title":"<code>value = value</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.BoundaryCondition.__init__","title":"<code>__init__(sampler: Callable[[int], Tensor], value: BCValueFn, n_pts: int = 100)</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>def __init__(\n    self,\n    sampler: Callable[[int], Tensor],\n    value: BCValueFn,\n    n_pts: int = 100,\n):\n    self.sampler = sampler\n    self.value = value\n    self.n_pts = n_pts\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.DataConstraint","title":"<code>DataConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Constraint enforcing fit to observed data. Minimizes \\(\\lVert \\hat{{y}} - y \\rVert^2\\).</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>FieldsRegistry</code> <p>Fields registry.</p> required <code>params</code> <code>ParamsRegistry</code> <p>Parameters registry.</p> required <code>predict_data</code> <code>PredictDataFn</code> <p>Function to predict data values from fields.</p> required <code>weight</code> <code>float</code> <p>Weight for this loss term.</p> <code>1.0</code> Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>class DataConstraint(Constraint):\n    \"\"\"\n    Constraint enforcing fit to observed data.\n    Minimizes $\\\\lVert \\\\hat{{y}} - y \\\\rVert^2$.\n\n    Args:\n        fields: Fields registry.\n        params: Parameters registry.\n        predict_data: Function to predict data values from fields.\n        weight: Weight for this loss term.\n    \"\"\"\n\n    def __init__(\n        self,\n        fields: FieldsRegistry,\n        params: ParamsRegistry,\n        predict_data: PredictDataFn,\n        weight: float = 1.0,\n    ):\n        self.fields = fields\n        self.params = params\n        self.predict_data = predict_data\n        self.weight = weight\n\n    @override\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        (x_data, y_data), _ = batch\n\n        y_data_pred = self.predict_data(x_data, self.fields, self.params)\n\n        loss: Tensor = criterion(y_data_pred, y_data)\n        loss = self.weight * loss\n\n        if log is not None:\n            log(\"loss/data\", loss)\n\n        return loss\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.DataConstraint.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.DataConstraint.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.DataConstraint.predict_data","title":"<code>predict_data = predict_data</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.DataConstraint.weight","title":"<code>weight = weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.DataConstraint.__init__","title":"<code>__init__(fields: FieldsRegistry, params: ParamsRegistry, predict_data: PredictDataFn, weight: float = 1.0)</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>def __init__(\n    self,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n    predict_data: PredictDataFn,\n    weight: float = 1.0,\n):\n    self.fields = fields\n    self.params = params\n    self.predict_data = predict_data\n    self.weight = weight\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.DataConstraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>@override\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    (x_data, y_data), _ = batch\n\n    y_data_pred = self.predict_data(x_data, self.fields, self.params)\n\n    loss: Tensor = criterion(y_data_pred, y_data)\n    loss = self.weight * loss\n\n    if log is not None:\n        log(\"loss/data\", loss)\n\n    return loss\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.DirichletBCConstraint","title":"<code>DirichletBCConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Enforces the Dirichlet boundary condition: u(x_bc) = g(x_bc). Minimizes <code>weight * criterion(u(x_bc), g(x_bc))</code>.</p> <p>Parameters:</p> Name Type Description Default <code>bc</code> <code>BoundaryCondition</code> <p>Boundary condition (sampler + target value function).</p> required <code>field</code> <code>Field</code> <p>The neural field to enforce the condition on.</p> required <code>log_key</code> <code>str</code> <p>Key used when logging the loss value.</p> <code>'loss/bc_dirichlet'</code> <code>weight</code> <code>float</code> <p>Loss term weight.</p> <code>1.0</code> Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>class DirichletBCConstraint(Constraint):\n    \"\"\"\n    Enforces the Dirichlet boundary condition: u(x_bc) = g(x_bc).\n    Minimizes ``weight * criterion(u(x_bc), g(x_bc))``.\n\n    Args:\n        bc: Boundary condition (sampler + target value function).\n        field: The neural field to enforce the condition on.\n        log_key: Key used when logging the loss value.\n        weight: Loss term weight.\n    \"\"\"\n\n    def __init__(\n        self,\n        bc: BoundaryCondition,\n        field: Field,\n        log_key: str = \"loss/bc_dirichlet\",\n        weight: float = 1.0,\n    ):\n        self.bc = bc\n        self.field = field\n        self.log_key = log_key\n        self.weight = weight\n\n    @override\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        device = next(self.field.parameters()).device\n        x_bc = self.bc.sampler(self.bc.n_pts).to(device)\n        u_pred = self.field(x_bc)\n        g = self.bc.value(x_bc).to(device)\n        loss: Tensor = self.weight * criterion(u_pred, g)\n        if log is not None:\n            log(self.log_key, loss)\n        return loss\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.DirichletBCConstraint.bc","title":"<code>bc = bc</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.DirichletBCConstraint.field","title":"<code>field = field</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.DirichletBCConstraint.log_key","title":"<code>log_key = log_key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.DirichletBCConstraint.weight","title":"<code>weight = weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.DirichletBCConstraint.__init__","title":"<code>__init__(bc: BoundaryCondition, field: Field, log_key: str = 'loss/bc_dirichlet', weight: float = 1.0)</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>def __init__(\n    self,\n    bc: BoundaryCondition,\n    field: Field,\n    log_key: str = \"loss/bc_dirichlet\",\n    weight: float = 1.0,\n):\n    self.bc = bc\n    self.field = field\n    self.log_key = log_key\n    self.weight = weight\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.DirichletBCConstraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>@override\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    device = next(self.field.parameters()).device\n    x_bc = self.bc.sampler(self.bc.n_pts).to(device)\n    u_pred = self.field(x_bc)\n    g = self.bc.value(x_bc).to(device)\n    loss: Tensor = self.weight * criterion(u_pred, g)\n    if log is not None:\n        log(self.log_key, loss)\n    return loss\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.ICConstraint","title":"<code>ICConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Constraint enforcing Initial Conditions (IC). Minimizes \\(\\lVert y(t_0) - Y_0 \\rVert^2\\).</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>FieldsRegistry</code> <p>Fields registry.</p> required <code>weight</code> <code>float</code> <p>Weight for this loss term.</p> <code>1.0</code> Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>class ICConstraint(Constraint):\n    \"\"\"\n    Constraint enforcing Initial Conditions (IC).\n    Minimizes $\\\\lVert y(t_0) - Y_0 \\\\rVert^2$.\n\n    Args:\n        fields: Fields registry.\n        weight: Weight for this loss term.\n    \"\"\"\n\n    def __init__(\n        self,\n        props: ODEProperties,\n        fields: FieldsRegistry,\n        weight: float = 1.0,\n    ):\n        if len(fields) != len(props.y0):\n            raise ValueError(\n                f\"Number of fields ({len(fields)}) must match number of initial conditions \"\n                f\"in y0 ({len(props.y0)}). Field keys: {list(fields)}.\"\n            )\n\n        self.Y0 = props.y0.clone().reshape(-1, 1, 1)\n        self.dY0 = [dy.clone().reshape(-1, 1, 1) for dy in props.dy0]\n        self.order = props.order\n        self.fields = fields\n        self.weight = weight\n\n    @override\n    def inject_context(self, context: InferredContext) -&gt; None:\n        \"\"\"\n        Inject the context into the constraint.\n        \"\"\"\n        self.t0 = torch.tensor(context.domain.x0, dtype=torch.float32).reshape(1, 1)\n\n    @override\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        device = batch[1].device\n\n        if self.t0.device != device:\n            self.t0 = self.t0.to(device)\n            self.Y0 = self.Y0.to(device)\n            self.dY0 = [d.to(device) for d in self.dY0]\n\n        n_fields = len(self.fields)\n\n        if self.order == 1:\n            # Fast path: no requires_grad needed, identical to original behaviour\n            Y0_preds = torch.stack([f(self.t0) for f in self.fields.values()])\n            loss: Tensor = self.weight * criterion(Y0_preds, self.Y0)\n        else:\n            x0 = self.t0.detach().requires_grad_(True)\n            preds = [f(x0) for f in self.fields.values()]\n            Y0_preds = torch.stack(preds)\n            total = criterion(Y0_preds, self.Y0)\n            # Enforce derivative ICs by chaining from previous level\n            currents = list(preds)\n            for k in range(self.order - 1):\n                next_level = [diff_grad(currents[i], x0) for i in range(n_fields)]\n                dY0_k_pred = torch.stack(next_level)  # (n_fields, 1, 1)\n                total = total + criterion(dY0_k_pred, self.dY0[k])\n                currents = next_level\n            loss = self.weight * total\n\n        if log is not None:\n            log(\"loss/ic\", loss)\n\n        return loss\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.ICConstraint.Y0","title":"<code>Y0 = props.y0.clone().reshape(-1, 1, 1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ICConstraint.dY0","title":"<code>dY0 = [(dy.clone().reshape(-1, 1, 1)) for dy in (props.dy0)]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ICConstraint.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ICConstraint.order","title":"<code>order = props.order</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ICConstraint.weight","title":"<code>weight = weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ICConstraint.__init__","title":"<code>__init__(props: ODEProperties, fields: FieldsRegistry, weight: float = 1.0)</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>def __init__(\n    self,\n    props: ODEProperties,\n    fields: FieldsRegistry,\n    weight: float = 1.0,\n):\n    if len(fields) != len(props.y0):\n        raise ValueError(\n            f\"Number of fields ({len(fields)}) must match number of initial conditions \"\n            f\"in y0 ({len(props.y0)}). Field keys: {list(fields)}.\"\n        )\n\n    self.Y0 = props.y0.clone().reshape(-1, 1, 1)\n    self.dY0 = [dy.clone().reshape(-1, 1, 1) for dy in props.dy0]\n    self.order = props.order\n    self.fields = fields\n    self.weight = weight\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.ICConstraint.inject_context","title":"<code>inject_context(context: InferredContext) -&gt; None</code>","text":"<p>Inject the context into the constraint.</p> Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>@override\ndef inject_context(self, context: InferredContext) -&gt; None:\n    \"\"\"\n    Inject the context into the constraint.\n    \"\"\"\n    self.t0 = torch.tensor(context.domain.x0, dtype=torch.float32).reshape(1, 1)\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.ICConstraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>@override\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    device = batch[1].device\n\n    if self.t0.device != device:\n        self.t0 = self.t0.to(device)\n        self.Y0 = self.Y0.to(device)\n        self.dY0 = [d.to(device) for d in self.dY0]\n\n    n_fields = len(self.fields)\n\n    if self.order == 1:\n        # Fast path: no requires_grad needed, identical to original behaviour\n        Y0_preds = torch.stack([f(self.t0) for f in self.fields.values()])\n        loss: Tensor = self.weight * criterion(Y0_preds, self.Y0)\n    else:\n        x0 = self.t0.detach().requires_grad_(True)\n        preds = [f(x0) for f in self.fields.values()]\n        Y0_preds = torch.stack(preds)\n        total = criterion(Y0_preds, self.Y0)\n        # Enforce derivative ICs by chaining from previous level\n        currents = list(preds)\n        for k in range(self.order - 1):\n            next_level = [diff_grad(currents[i], x0) for i in range(n_fields)]\n            dY0_k_pred = torch.stack(next_level)  # (n_fields, 1, 1)\n            total = total + criterion(dY0_k_pred, self.dY0[k])\n            currents = next_level\n        loss = self.weight * total\n\n    if log is not None:\n        log(\"loss/ic\", loss)\n\n    return loss\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.NeumannBCConstraint","title":"<code>NeumannBCConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Enforces the Neumann boundary condition: \\(\\partial u / \\partial n (x_{bc}) = h(x_{bc})\\).</p> <p>For a rectangular domain face whose outward normal is axis-aligned with dimension <code>normal_dim</code>, we have \\(\\partial u / \\partial n = \\partial u / \\partial x_{\\mathrm{normal\\_dim}}\\). Minimizes <code>weight * criterion(du_dn(x_bc), h(x_bc))</code>.</p> <p>Parameters:</p> Name Type Description Default <code>bc</code> <code>BoundaryCondition</code> <p>Boundary condition (sampler + target normal-derivative function).</p> required <code>field</code> <code>Field</code> <p>The neural field to enforce the condition on.</p> required <code>normal_dim</code> <code>int</code> <p>Index of the spatial dimension the boundary normal points along.</p> required <code>log_key</code> <code>str</code> <p>Key used when logging the loss value.</p> <code>'loss/bc_neumann'</code> <code>weight</code> <code>float</code> <p>Loss term weight.</p> <code>1.0</code> Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>class NeumannBCConstraint(Constraint):\n    \"\"\"\n    Enforces the Neumann boundary condition:\n    $\\\\partial u / \\\\partial n (x_{bc}) = h(x_{bc})$.\n\n    For a rectangular domain face whose outward normal is axis-aligned with\n    dimension ``normal_dim``, we have\n    $\\\\partial u / \\\\partial n = \\\\partial u / \\\\partial x_{\\\\mathrm{normal\\\\_dim}}$.\n    Minimizes\n    ``weight * criterion(du_dn(x_bc), h(x_bc))``.\n\n    Args:\n        bc: Boundary condition (sampler + target normal-derivative function).\n        field: The neural field to enforce the condition on.\n        normal_dim: Index of the spatial dimension the boundary normal points along.\n        log_key: Key used when logging the loss value.\n        weight: Loss term weight.\n    \"\"\"\n\n    def __init__(\n        self,\n        bc: BoundaryCondition,\n        field: Field,\n        normal_dim: int,\n        log_key: str = \"loss/bc_neumann\",\n        weight: float = 1.0,\n    ):\n        self.bc = bc\n        self.field = field\n        self.normal_dim = normal_dim\n        self.log_key = log_key\n        self.weight = weight\n\n    @override\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        device = next(self.field.parameters()).device\n        x_bc = self.bc.sampler(self.bc.n_pts).to(device).detach().requires_grad_(True)\n        u_pred = self.field(x_bc)\n        du_dn = diff_partial(u_pred, x_bc, dim=self.normal_dim)\n        h = self.bc.value(x_bc.detach()).to(device)\n        loss: Tensor = self.weight * criterion(du_dn, h)\n        if log is not None:\n            log(self.log_key, loss)\n        return loss\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.NeumannBCConstraint.bc","title":"<code>bc = bc</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.NeumannBCConstraint.field","title":"<code>field = field</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.NeumannBCConstraint.log_key","title":"<code>log_key = log_key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.NeumannBCConstraint.normal_dim","title":"<code>normal_dim = normal_dim</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.NeumannBCConstraint.weight","title":"<code>weight = weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.NeumannBCConstraint.__init__","title":"<code>__init__(bc: BoundaryCondition, field: Field, normal_dim: int, log_key: str = 'loss/bc_neumann', weight: float = 1.0)</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>def __init__(\n    self,\n    bc: BoundaryCondition,\n    field: Field,\n    normal_dim: int,\n    log_key: str = \"loss/bc_neumann\",\n    weight: float = 1.0,\n):\n    self.bc = bc\n    self.field = field\n    self.normal_dim = normal_dim\n    self.log_key = log_key\n    self.weight = weight\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.NeumannBCConstraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>@override\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    device = next(self.field.parameters()).device\n    x_bc = self.bc.sampler(self.bc.n_pts).to(device).detach().requires_grad_(True)\n    u_pred = self.field(x_bc)\n    du_dn = diff_partial(u_pred, x_bc, dim=self.normal_dim)\n    h = self.bc.value(x_bc.detach()).to(device)\n    loss: Tensor = self.weight * criterion(du_dn, h)\n    if log is not None:\n        log(self.log_key, loss)\n    return loss\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.ODECallable","title":"<code>ODECallable</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for ODE right-hand side callables.</p> <p>First-order callables (<code>ODEProperties.order == 1</code>) receive three positional arguments and must match this Protocol exactly::</p> <pre><code>def my_ode(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor: ...\n</code></pre> <p>Higher-order callables (<code>order &gt;= 2</code>) receive a fourth positional argument <code>derivs: list[Tensor]</code>, where <code>derivs[k]</code> is the <code>(k+1)</code>-th derivative of all fields stacked as <code>(n_fields, m, 1)</code>::</p> <pre><code>def my_ode(x: Tensor, y: Tensor, args: ArgsRegistry,\n           derivs: list[Tensor] = []) -&gt; Tensor: ...\n</code></pre> <p>The Protocol is intentionally kept to three arguments so that existing first-order callables remain valid <code>ODECallable</code> implementations. <code>ResidualsConstraint</code> uses <code>_ODECallableN</code> internally to call higher-order functions with the correct signature.</p> Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>class ODECallable(Protocol):\n    \"\"\"\n    Protocol for ODE right-hand side callables.\n\n    **First-order** callables (``ODEProperties.order == 1``) receive three\n    positional arguments and must match this Protocol exactly::\n\n        def my_ode(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor: ...\n\n    **Higher-order** callables (``order &gt;= 2``) receive a fourth positional\n    argument ``derivs: list[Tensor]``, where ``derivs[k]`` is the\n    ``(k+1)``-th derivative of all fields stacked as ``(n_fields, m, 1)``::\n\n        def my_ode(x: Tensor, y: Tensor, args: ArgsRegistry,\n                   derivs: list[Tensor] = []) -&gt; Tensor: ...\n\n    The Protocol is intentionally kept to three arguments so that existing\n    first-order callables remain valid ``ODECallable`` implementations.\n    ``ResidualsConstraint`` uses ``_ODECallableN`` internally to call\n    higher-order functions with the correct signature.\n    \"\"\"\n\n    def __call__(self, x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor: ...\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.ODECallable.__call__","title":"<code>__call__(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>def __call__(self, x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor: ...\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.ODEHyperparameters","title":"<code>ODEHyperparameters</code>  <code>dataclass</code>","text":"<p>               Bases: <code>PINNHyperparameters</code></p> <p>Hyperparameters for ODE inverse problems.</p> Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>@dataclass(kw_only=True)\nclass ODEHyperparameters(PINNHyperparameters):\n    \"\"\"\n    Hyperparameters for ODE inverse problems.\n    \"\"\"\n\n    pde_weight: float = 1.0\n    ic_weight: float = 1.0\n    data_weight: float = 1.0\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.ODEHyperparameters.data_weight","title":"<code>data_weight: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ODEHyperparameters.ic_weight","title":"<code>ic_weight: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ODEHyperparameters.pde_weight","title":"<code>pde_weight: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ODEHyperparameters.__init__","title":"<code>__init__(*, lr: float, training_data: IngestionConfig | GenerationConfig, fields_config: MLPConfig, params_config: MLPConfig | ScalarConfig, max_epochs: int | None = None, gradient_clip_val: float | None = None, criterion: Criteria = 'mse', optimizer: AdamConfig | LBFGSConfig | None = None, scheduler: ReduceLROnPlateauConfig | CosineAnnealingConfig | None = None, early_stopping: EarlyStoppingConfig | None = None, smma_stopping: SMMAStoppingConfig | None = None, pde_weight: float = 1.0, ic_weight: float = 1.0, data_weight: float = 1.0) -&gt; None</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ODEInverseProblem","title":"<code>ODEInverseProblem</code>","text":"<p>               Bases: <code>Problem</code></p> <p>Generic ODE Inverse Problem. Composes Residuals + IC + Data constraints with MSELoss.</p> Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>class ODEInverseProblem(Problem):\n    \"\"\"\n    Generic ODE Inverse Problem.\n    Composes Residuals + IC + Data constraints with MSELoss.\n    \"\"\"\n\n    def __init__(\n        self,\n        props: ODEProperties,\n        hp: ODEHyperparameters,\n        fields: FieldsRegistry,\n        params: ParamsRegistry,\n        predict_data: PredictDataFn,\n    ) -&gt; None:\n        constraints: list[Constraint] = [\n            ResidualsConstraint(\n                props=props,\n                fields=fields,\n                params=params,\n                weight=hp.pde_weight,\n            ),\n            ICConstraint(\n                props=props,\n                fields=fields,\n                weight=hp.ic_weight,\n            ),\n            DataConstraint(\n                fields=fields,\n                params=params,\n                predict_data=predict_data,\n                weight=hp.data_weight,\n            ),\n        ]\n\n        criterion = build_criterion(hp.criterion)\n\n        super().__init__(\n            constraints=constraints,\n            criterion=criterion,\n            fields=fields,\n            params=params,\n        )\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.ODEInverseProblem.__init__","title":"<code>__init__(props: ODEProperties, hp: ODEHyperparameters, fields: FieldsRegistry, params: ParamsRegistry, predict_data: PredictDataFn) -&gt; None</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>def __init__(\n    self,\n    props: ODEProperties,\n    hp: ODEHyperparameters,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n    predict_data: PredictDataFn,\n) -&gt; None:\n    constraints: list[Constraint] = [\n        ResidualsConstraint(\n            props=props,\n            fields=fields,\n            params=params,\n            weight=hp.pde_weight,\n        ),\n        ICConstraint(\n            props=props,\n            fields=fields,\n            weight=hp.ic_weight,\n        ),\n        DataConstraint(\n            fields=fields,\n            params=params,\n            predict_data=predict_data,\n            weight=hp.data_weight,\n        ),\n    ]\n\n    criterion = build_criterion(hp.criterion)\n\n    super().__init__(\n        constraints=constraints,\n        criterion=criterion,\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.ODEProperties","title":"<code>ODEProperties</code>  <code>dataclass</code>","text":"<p>Properties defining an Ordinary Differential Equation problem.</p> <p>Attributes:</p> Name Type Description <code>ode</code> <code>ODECallable</code> <p>The ODE function (callable).</p> <code>args</code> <code>ArgsRegistry</code> <p>Arguments/Parameters for the ODE.</p> <code>y0</code> <code>Tensor</code> <p>Initial conditions.</p> <code>order</code> <code>int</code> <p>Order of the ODE (default 1). For order=n, the ODE callable receives derivs as its last argument: derivs[k] is the (k+1)-th derivative.</p> <code>dy0</code> <code>list[Tensor]</code> <p>Initial conditions for lower-order derivatives, length = order-1. dy0[k] is the IC for the (k+1)-th derivative, shape (n_fields,).</p> <code>expected_args</code> <code>frozenset[str] | None</code> <p>Optional set of arg keys the ODE function accesses. When provided, validated against the merged args+params at construction time.</p> Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>@dataclass\nclass ODEProperties:\n    \"\"\"\n    Properties defining an Ordinary Differential Equation problem.\n\n    Attributes:\n        ode: The ODE function (callable).\n        args: Arguments/Parameters for the ODE.\n        y0: Initial conditions.\n        order: Order of the ODE (default 1). For order=n, the ODE callable receives\n            derivs as its last argument: derivs[k] is the (k+1)-th derivative.\n        dy0: Initial conditions for lower-order derivatives, length = order-1.\n            dy0[k] is the IC for the (k+1)-th derivative, shape (n_fields,).\n        expected_args: Optional set of arg keys the ODE function accesses.\n            When provided, validated against the merged args+params at construction time.\n    \"\"\"\n\n    ode: ODECallable\n    args: ArgsRegistry\n    y0: Tensor\n    order: int = 1\n    dy0: list[Tensor] = dc_field(default_factory=list)\n    expected_args: frozenset[str] | None = None\n\n    def __post_init__(self) -&gt; None:\n        if self.order &lt; 1:\n            raise ValueError(f\"order must be &gt;= 1, got {self.order}\")\n        if len(self.dy0) != self.order - 1:\n            raise ValueError(f\"dy0 must have length order-1={self.order - 1}, got {len(self.dy0)}\")\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.ODEProperties.args","title":"<code>args: ArgsRegistry</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ODEProperties.dy0","title":"<code>dy0: list[Tensor] = dc_field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ODEProperties.expected_args","title":"<code>expected_args: frozenset[str] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ODEProperties.ode","title":"<code>ode: ODECallable</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ODEProperties.order","title":"<code>order: int = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ODEProperties.y0","title":"<code>y0: Tensor</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ODEProperties.__init__","title":"<code>__init__(ode: ODECallable, args: ArgsRegistry, y0: Tensor, order: int = 1, dy0: list[Tensor] = list(), expected_args: frozenset[str] | None = None) -&gt; None</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ODEProperties.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.order &lt; 1:\n        raise ValueError(f\"order must be &gt;= 1, got {self.order}\")\n    if len(self.dy0) != self.order - 1:\n        raise ValueError(f\"dy0 must have length order-1={self.order - 1}, got {len(self.dy0)}\")\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.PDEResidualConstraint","title":"<code>PDEResidualConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Enforces a PDE interior residual: <code>residual_fn(x, fields, params) \u2248 0</code>. Minimizes <code>weight * criterion(residual_fn(x_coll, fields, params), 0)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>FieldsRegistry</code> <p>Registry of neural fields the residual function operates on. Pass only the subset needed \u2014 other fields in the Problem are ignored.</p> required <code>params</code> <code>ParamsRegistry</code> <p>Registry of parameters the residual function uses.</p> required <code>residual_fn</code> <code>PDEResidualFn</code> <p>Callable (x, fields, params) \u2192 Tensor of residuals. Should use <code>anypinn.lib.diff</code> operators for derivatives. The returned tensor is compared against zeros.</p> required <code>log_key</code> <code>str</code> <p>Key used when logging the loss value.</p> <code>'loss/pde_residual'</code> <code>weight</code> <code>float</code> <p>Loss term weight.</p> <code>1.0</code> Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>class PDEResidualConstraint(Constraint):\n    \"\"\"\n    Enforces a PDE interior residual: ``residual_fn(x, fields, params) \u2248 0``.\n    Minimizes ``weight * criterion(residual_fn(x_coll, fields, params), 0)``.\n\n    Args:\n        fields: Registry of neural fields the residual function operates on.\n            Pass only the subset needed \u2014 other fields in the Problem are ignored.\n        params: Registry of parameters the residual function uses.\n        residual_fn: Callable (x, fields, params) \u2192 Tensor of residuals.\n            Should use ``anypinn.lib.diff`` operators for derivatives.\n            The returned tensor is compared against zeros.\n        log_key: Key used when logging the loss value.\n        weight: Loss term weight.\n    \"\"\"\n\n    def __init__(\n        self,\n        fields: FieldsRegistry,\n        params: ParamsRegistry,\n        residual_fn: PDEResidualFn,\n        log_key: str = \"loss/pde_residual\",\n        weight: float = 1.0,\n    ):\n        self.fields = fields\n        self.params = params\n        self.residual_fn = residual_fn\n        self.log_key = log_key\n        self.weight = weight\n\n    @override\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        _, x_coll = batch\n        x_coll = x_coll.detach().requires_grad_(True)\n        residual = self.residual_fn(x_coll, self.fields, self.params)\n        loss: Tensor = self.weight * criterion(residual, torch.zeros_like(residual))\n        if log is not None:\n            log(self.log_key, loss)\n        return loss\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.PDEResidualConstraint.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.PDEResidualConstraint.log_key","title":"<code>log_key = log_key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.PDEResidualConstraint.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.PDEResidualConstraint.residual_fn","title":"<code>residual_fn = residual_fn</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.PDEResidualConstraint.weight","title":"<code>weight = weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.PDEResidualConstraint.__init__","title":"<code>__init__(fields: FieldsRegistry, params: ParamsRegistry, residual_fn: PDEResidualFn, log_key: str = 'loss/pde_residual', weight: float = 1.0)</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>def __init__(\n    self,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n    residual_fn: PDEResidualFn,\n    log_key: str = \"loss/pde_residual\",\n    weight: float = 1.0,\n):\n    self.fields = fields\n    self.params = params\n    self.residual_fn = residual_fn\n    self.log_key = log_key\n    self.weight = weight\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.PDEResidualConstraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>@override\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    _, x_coll = batch\n    x_coll = x_coll.detach().requires_grad_(True)\n    residual = self.residual_fn(x_coll, self.fields, self.params)\n    loss: Tensor = self.weight * criterion(residual, torch.zeros_like(residual))\n    if log is not None:\n        log(self.log_key, loss)\n    return loss\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.PeriodicBCConstraint","title":"<code>PeriodicBCConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Enforces periodic boundary conditions: <code>u(x_left, t) = u(x_right, t)</code> and <code>\u2202u/\u2202x(x_left, t) = \u2202u/\u2202x(x_right, t)</code>.</p> <p>The two boundary samplers must produce paired points \u2014 identical coordinates in every dimension except the periodic one \u2014 so that the value- and derivative-matching losses are meaningful.</p> <p>Minimizes <code>weight * [criterion(u_left, u_right) + criterion(du_left, du_right)]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>bc_left</code> <code>BoundaryCondition</code> <p>Left boundary sampler (sampler + dummy value function).</p> required <code>bc_right</code> <code>BoundaryCondition</code> <p>Right boundary sampler (sampler + dummy value function).</p> required <code>field</code> <code>Field</code> <p>The neural field to enforce the condition on.</p> required <code>match_dim</code> <code>int</code> <p>Spatial dimension index for the derivative matching.</p> <code>0</code> <code>log_key</code> <code>str</code> <p>Key used when logging the loss value.</p> <code>'loss/bc_periodic'</code> <code>weight</code> <code>float</code> <p>Loss term weight.</p> <code>1.0</code> Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>class PeriodicBCConstraint(Constraint):\n    \"\"\"\n    Enforces periodic boundary conditions:\n    ``u(x_left, t) = u(x_right, t)`` and\n    ``\u2202u/\u2202x(x_left, t) = \u2202u/\u2202x(x_right, t)``.\n\n    The two boundary samplers must produce **paired** points \u2014 identical\n    coordinates in every dimension except the periodic one \u2014 so that\n    the value- and derivative-matching losses are meaningful.\n\n    Minimizes\n    ``weight * [criterion(u_left, u_right) + criterion(du_left, du_right)]``.\n\n    Args:\n        bc_left: Left boundary sampler (sampler + dummy value function).\n        bc_right: Right boundary sampler (sampler + dummy value function).\n        field: The neural field to enforce the condition on.\n        match_dim: Spatial dimension index for the derivative matching.\n        log_key: Key used when logging the loss value.\n        weight: Loss term weight.\n    \"\"\"\n\n    def __init__(\n        self,\n        bc_left: BoundaryCondition,\n        bc_right: BoundaryCondition,\n        field: Field,\n        match_dim: int = 0,\n        log_key: str = \"loss/bc_periodic\",\n        weight: float = 1.0,\n    ):\n        self.bc_left = bc_left\n        self.bc_right = bc_right\n        self.field = field\n        self.match_dim = match_dim\n        self.log_key = log_key\n        self.weight = weight\n\n    @override\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        device = next(self.field.parameters()).device\n        n_pts = self.bc_left.n_pts\n\n        x_left = self.bc_left.sampler(n_pts).to(device).detach().requires_grad_(True)\n        x_right = self.bc_right.sampler(n_pts).to(device).detach().requires_grad_(True)\n\n        u_left = self.field(x_left)\n        u_right = self.field(x_right)\n\n        # Value matching: u(x_left, t) = u(x_right, t)\n        loss_val: Tensor = criterion(u_left, u_right)\n\n        # Derivative matching: du/dx(x_left, t) = du/dx(x_right, t)\n        du_left = diff_partial(u_left, x_left, dim=self.match_dim)\n        du_right = diff_partial(u_right, x_right, dim=self.match_dim)\n        loss_deriv: Tensor = criterion(du_left, du_right)\n\n        loss: Tensor = self.weight * (loss_val + loss_deriv)\n        if log is not None:\n            log(self.log_key, loss)\n        return loss\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.PeriodicBCConstraint.bc_left","title":"<code>bc_left = bc_left</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.PeriodicBCConstraint.bc_right","title":"<code>bc_right = bc_right</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.PeriodicBCConstraint.field","title":"<code>field = field</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.PeriodicBCConstraint.log_key","title":"<code>log_key = log_key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.PeriodicBCConstraint.match_dim","title":"<code>match_dim = match_dim</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.PeriodicBCConstraint.weight","title":"<code>weight = weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.PeriodicBCConstraint.__init__","title":"<code>__init__(bc_left: BoundaryCondition, bc_right: BoundaryCondition, field: Field, match_dim: int = 0, log_key: str = 'loss/bc_periodic', weight: float = 1.0)</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>def __init__(\n    self,\n    bc_left: BoundaryCondition,\n    bc_right: BoundaryCondition,\n    field: Field,\n    match_dim: int = 0,\n    log_key: str = \"loss/bc_periodic\",\n    weight: float = 1.0,\n):\n    self.bc_left = bc_left\n    self.bc_right = bc_right\n    self.field = field\n    self.match_dim = match_dim\n    self.log_key = log_key\n    self.weight = weight\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.PeriodicBCConstraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>@override\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    device = next(self.field.parameters()).device\n    n_pts = self.bc_left.n_pts\n\n    x_left = self.bc_left.sampler(n_pts).to(device).detach().requires_grad_(True)\n    x_right = self.bc_right.sampler(n_pts).to(device).detach().requires_grad_(True)\n\n    u_left = self.field(x_left)\n    u_right = self.field(x_right)\n\n    # Value matching: u(x_left, t) = u(x_right, t)\n    loss_val: Tensor = criterion(u_left, u_right)\n\n    # Derivative matching: du/dx(x_left, t) = du/dx(x_right, t)\n    du_left = diff_partial(u_left, x_left, dim=self.match_dim)\n    du_right = diff_partial(u_right, x_right, dim=self.match_dim)\n    loss_deriv: Tensor = criterion(du_left, du_right)\n\n    loss: Tensor = self.weight * (loss_val + loss_deriv)\n    if log is not None:\n        log(self.log_key, loss)\n    return loss\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.ResidualsConstraint","title":"<code>ResidualsConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Constraint enforcing the ODE residuals. Minimizes \\(\\lVert \\partial y / \\partial t - f(t, y) \\rVert^2\\).</p> <p>Parameters:</p> Name Type Description Default <code>props</code> <code>ODEProperties</code> <p>ODE properties.</p> required <code>fields</code> <code>FieldsRegistry</code> <p>List of fields.</p> required <code>params</code> <code>ParamsRegistry</code> <p>List of parameters.</p> required <code>weight</code> <code>float</code> <p>Weight for this loss term.</p> <code>1.0</code> Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>class ResidualsConstraint(Constraint):\n    \"\"\"\n    Constraint enforcing the ODE residuals.\n    Minimizes $\\\\lVert \\\\partial y / \\\\partial t - f(t, y) \\\\rVert^2$.\n\n    Args:\n        props: ODE properties.\n        fields: List of fields.\n        params: List of parameters.\n        weight: Weight for this loss term.\n    \"\"\"\n\n    def __init__(\n        self,\n        props: ODEProperties,\n        fields: FieldsRegistry,\n        params: ParamsRegistry,\n        weight: float = 1.0,\n    ):\n        if len(fields) != len(props.y0):\n            raise ValueError(\n                f\"Number of fields ({len(fields)}) must match number of initial conditions \"\n                f\"in y0 ({len(props.y0)}). Field keys: {list(fields)}.\"\n            )\n\n        merged_args: dict[str, object] = {**props.args, **params}\n        if props.expected_args is not None:\n            missing = props.expected_args - merged_args.keys()\n            if missing:\n                raise ValueError(\n                    f\"ODE function expects args {sorted(missing)!r} but they are not in \"\n                    f\"props.args or params. Available keys: {sorted(merged_args.keys())!r}.\"\n                )\n\n        self.fields = fields\n        self.weight = weight\n        self.order = props.order\n\n        self.ode = props.ode\n\n        # add the trainable params as args\n        self.args = props.args.copy()\n        self.args.update(params)\n\n    @override\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        _, x_coll = batch\n\n        n_fields = len(self.fields)\n        x_copies = [x_coll.detach().clone().requires_grad_(True) for _ in range(n_fields)]\n        preds = [f(x_copies[i]) for i, f in enumerate(self.fields.values())]\n        y = torch.stack(preds)\n\n        # Build all derivative levels by chaining (each level differentiates the previous)\n        # deriv_levels[k][i] = (k+1)-th derivative of field i\n        deriv_levels: list[list[Tensor]] = []\n        currents = list(preds)\n        for _ in range(self.order):\n            next_level = [diff_grad(currents[i], x_copies[i]) for i in range(n_fields)]\n            deriv_levels.append(next_level)\n            currents = next_level\n\n        # derivs[k] = (k+1)-th derivative stacked across fields, passed to the ODE callable\n        derivs = [torch.stack(deriv_levels[k]) for k in range(self.order - 1)]\n        # The order-th derivative is the LHS to compare against f_out\n        high_deriv = torch.stack(deriv_levels[self.order - 1])\n\n        if self.order == 1:\n            f_out = self.ode(x_coll, y, self.args)\n        else:\n            f_out = cast(_ODECallableN, self.ode)(x_coll, y, self.args, derivs)\n\n        loss: Tensor = self.weight * criterion(high_deriv, f_out)\n\n        if log is not None:\n            log(\"loss/res\", loss)\n\n        return loss\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.ResidualsConstraint.args","title":"<code>args = props.args.copy()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ResidualsConstraint.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ResidualsConstraint.ode","title":"<code>ode = props.ode</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ResidualsConstraint.order","title":"<code>order = props.order</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ResidualsConstraint.weight","title":"<code>weight = weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/#anypinn.problems.ResidualsConstraint.__init__","title":"<code>__init__(props: ODEProperties, fields: FieldsRegistry, params: ParamsRegistry, weight: float = 1.0)</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>def __init__(\n    self,\n    props: ODEProperties,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n    weight: float = 1.0,\n):\n    if len(fields) != len(props.y0):\n        raise ValueError(\n            f\"Number of fields ({len(fields)}) must match number of initial conditions \"\n            f\"in y0 ({len(props.y0)}). Field keys: {list(fields)}.\"\n        )\n\n    merged_args: dict[str, object] = {**props.args, **params}\n    if props.expected_args is not None:\n        missing = props.expected_args - merged_args.keys()\n        if missing:\n            raise ValueError(\n                f\"ODE function expects args {sorted(missing)!r} but they are not in \"\n                f\"props.args or params. Available keys: {sorted(merged_args.keys())!r}.\"\n            )\n\n    self.fields = fields\n    self.weight = weight\n    self.order = props.order\n\n    self.ode = props.ode\n\n    # add the trainable params as args\n    self.args = props.args.copy()\n    self.args.update(params)\n</code></pre>"},{"location":"reference/anypinn/problems/#anypinn.problems.ResidualsConstraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>@override\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    _, x_coll = batch\n\n    n_fields = len(self.fields)\n    x_copies = [x_coll.detach().clone().requires_grad_(True) for _ in range(n_fields)]\n    preds = [f(x_copies[i]) for i, f in enumerate(self.fields.values())]\n    y = torch.stack(preds)\n\n    # Build all derivative levels by chaining (each level differentiates the previous)\n    # deriv_levels[k][i] = (k+1)-th derivative of field i\n    deriv_levels: list[list[Tensor]] = []\n    currents = list(preds)\n    for _ in range(self.order):\n        next_level = [diff_grad(currents[i], x_copies[i]) for i in range(n_fields)]\n        deriv_levels.append(next_level)\n        currents = next_level\n\n    # derivs[k] = (k+1)-th derivative stacked across fields, passed to the ODE callable\n    derivs = [torch.stack(deriv_levels[k]) for k in range(self.order - 1)]\n    # The order-th derivative is the LHS to compare against f_out\n    high_deriv = torch.stack(deriv_levels[self.order - 1])\n\n    if self.order == 1:\n        f_out = self.ode(x_coll, y, self.args)\n    else:\n        f_out = cast(_ODECallableN, self.ode)(x_coll, y, self.args, derivs)\n\n    loss: Tensor = self.weight * criterion(high_deriv, f_out)\n\n    if log is not None:\n        log(\"loss/res\", loss)\n\n    return loss\n</code></pre>"},{"location":"reference/anypinn/problems/ode/","title":"ode","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode","title":"<code>anypinn.problems.ode</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.PredictDataFn","title":"<code>PredictDataFn: TypeAlias = Callable[[Tensor, FieldsRegistry, ParamsRegistry], Tensor]</code>  <code>module-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.DataConstraint","title":"<code>DataConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Constraint enforcing fit to observed data. Minimizes \\(\\lVert \\hat{{y}} - y \\rVert^2\\).</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>FieldsRegistry</code> <p>Fields registry.</p> required <code>params</code> <code>ParamsRegistry</code> <p>Parameters registry.</p> required <code>predict_data</code> <code>PredictDataFn</code> <p>Function to predict data values from fields.</p> required <code>weight</code> <code>float</code> <p>Weight for this loss term.</p> <code>1.0</code> Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>class DataConstraint(Constraint):\n    \"\"\"\n    Constraint enforcing fit to observed data.\n    Minimizes $\\\\lVert \\\\hat{{y}} - y \\\\rVert^2$.\n\n    Args:\n        fields: Fields registry.\n        params: Parameters registry.\n        predict_data: Function to predict data values from fields.\n        weight: Weight for this loss term.\n    \"\"\"\n\n    def __init__(\n        self,\n        fields: FieldsRegistry,\n        params: ParamsRegistry,\n        predict_data: PredictDataFn,\n        weight: float = 1.0,\n    ):\n        self.fields = fields\n        self.params = params\n        self.predict_data = predict_data\n        self.weight = weight\n\n    @override\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        (x_data, y_data), _ = batch\n\n        y_data_pred = self.predict_data(x_data, self.fields, self.params)\n\n        loss: Tensor = criterion(y_data_pred, y_data)\n        loss = self.weight * loss\n\n        if log is not None:\n            log(\"loss/data\", loss)\n\n        return loss\n</code></pre>"},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.DataConstraint.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.DataConstraint.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.DataConstraint.predict_data","title":"<code>predict_data = predict_data</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.DataConstraint.weight","title":"<code>weight = weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.DataConstraint.__init__","title":"<code>__init__(fields: FieldsRegistry, params: ParamsRegistry, predict_data: PredictDataFn, weight: float = 1.0)</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>def __init__(\n    self,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n    predict_data: PredictDataFn,\n    weight: float = 1.0,\n):\n    self.fields = fields\n    self.params = params\n    self.predict_data = predict_data\n    self.weight = weight\n</code></pre>"},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.DataConstraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>@override\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    (x_data, y_data), _ = batch\n\n    y_data_pred = self.predict_data(x_data, self.fields, self.params)\n\n    loss: Tensor = criterion(y_data_pred, y_data)\n    loss = self.weight * loss\n\n    if log is not None:\n        log(\"loss/data\", loss)\n\n    return loss\n</code></pre>"},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ICConstraint","title":"<code>ICConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Constraint enforcing Initial Conditions (IC). Minimizes \\(\\lVert y(t_0) - Y_0 \\rVert^2\\).</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>FieldsRegistry</code> <p>Fields registry.</p> required <code>weight</code> <code>float</code> <p>Weight for this loss term.</p> <code>1.0</code> Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>class ICConstraint(Constraint):\n    \"\"\"\n    Constraint enforcing Initial Conditions (IC).\n    Minimizes $\\\\lVert y(t_0) - Y_0 \\\\rVert^2$.\n\n    Args:\n        fields: Fields registry.\n        weight: Weight for this loss term.\n    \"\"\"\n\n    def __init__(\n        self,\n        props: ODEProperties,\n        fields: FieldsRegistry,\n        weight: float = 1.0,\n    ):\n        if len(fields) != len(props.y0):\n            raise ValueError(\n                f\"Number of fields ({len(fields)}) must match number of initial conditions \"\n                f\"in y0 ({len(props.y0)}). Field keys: {list(fields)}.\"\n            )\n\n        self.Y0 = props.y0.clone().reshape(-1, 1, 1)\n        self.dY0 = [dy.clone().reshape(-1, 1, 1) for dy in props.dy0]\n        self.order = props.order\n        self.fields = fields\n        self.weight = weight\n\n    @override\n    def inject_context(self, context: InferredContext) -&gt; None:\n        \"\"\"\n        Inject the context into the constraint.\n        \"\"\"\n        self.t0 = torch.tensor(context.domain.x0, dtype=torch.float32).reshape(1, 1)\n\n    @override\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        device = batch[1].device\n\n        if self.t0.device != device:\n            self.t0 = self.t0.to(device)\n            self.Y0 = self.Y0.to(device)\n            self.dY0 = [d.to(device) for d in self.dY0]\n\n        n_fields = len(self.fields)\n\n        if self.order == 1:\n            # Fast path: no requires_grad needed, identical to original behaviour\n            Y0_preds = torch.stack([f(self.t0) for f in self.fields.values()])\n            loss: Tensor = self.weight * criterion(Y0_preds, self.Y0)\n        else:\n            x0 = self.t0.detach().requires_grad_(True)\n            preds = [f(x0) for f in self.fields.values()]\n            Y0_preds = torch.stack(preds)\n            total = criterion(Y0_preds, self.Y0)\n            # Enforce derivative ICs by chaining from previous level\n            currents = list(preds)\n            for k in range(self.order - 1):\n                next_level = [diff_grad(currents[i], x0) for i in range(n_fields)]\n                dY0_k_pred = torch.stack(next_level)  # (n_fields, 1, 1)\n                total = total + criterion(dY0_k_pred, self.dY0[k])\n                currents = next_level\n            loss = self.weight * total\n\n        if log is not None:\n            log(\"loss/ic\", loss)\n\n        return loss\n</code></pre>"},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ICConstraint.Y0","title":"<code>Y0 = props.y0.clone().reshape(-1, 1, 1)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ICConstraint.dY0","title":"<code>dY0 = [(dy.clone().reshape(-1, 1, 1)) for dy in (props.dy0)]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ICConstraint.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ICConstraint.order","title":"<code>order = props.order</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ICConstraint.weight","title":"<code>weight = weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ICConstraint.__init__","title":"<code>__init__(props: ODEProperties, fields: FieldsRegistry, weight: float = 1.0)</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>def __init__(\n    self,\n    props: ODEProperties,\n    fields: FieldsRegistry,\n    weight: float = 1.0,\n):\n    if len(fields) != len(props.y0):\n        raise ValueError(\n            f\"Number of fields ({len(fields)}) must match number of initial conditions \"\n            f\"in y0 ({len(props.y0)}). Field keys: {list(fields)}.\"\n        )\n\n    self.Y0 = props.y0.clone().reshape(-1, 1, 1)\n    self.dY0 = [dy.clone().reshape(-1, 1, 1) for dy in props.dy0]\n    self.order = props.order\n    self.fields = fields\n    self.weight = weight\n</code></pre>"},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ICConstraint.inject_context","title":"<code>inject_context(context: InferredContext) -&gt; None</code>","text":"<p>Inject the context into the constraint.</p> Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>@override\ndef inject_context(self, context: InferredContext) -&gt; None:\n    \"\"\"\n    Inject the context into the constraint.\n    \"\"\"\n    self.t0 = torch.tensor(context.domain.x0, dtype=torch.float32).reshape(1, 1)\n</code></pre>"},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ICConstraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>@override\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    device = batch[1].device\n\n    if self.t0.device != device:\n        self.t0 = self.t0.to(device)\n        self.Y0 = self.Y0.to(device)\n        self.dY0 = [d.to(device) for d in self.dY0]\n\n    n_fields = len(self.fields)\n\n    if self.order == 1:\n        # Fast path: no requires_grad needed, identical to original behaviour\n        Y0_preds = torch.stack([f(self.t0) for f in self.fields.values()])\n        loss: Tensor = self.weight * criterion(Y0_preds, self.Y0)\n    else:\n        x0 = self.t0.detach().requires_grad_(True)\n        preds = [f(x0) for f in self.fields.values()]\n        Y0_preds = torch.stack(preds)\n        total = criterion(Y0_preds, self.Y0)\n        # Enforce derivative ICs by chaining from previous level\n        currents = list(preds)\n        for k in range(self.order - 1):\n            next_level = [diff_grad(currents[i], x0) for i in range(n_fields)]\n            dY0_k_pred = torch.stack(next_level)  # (n_fields, 1, 1)\n            total = total + criterion(dY0_k_pred, self.dY0[k])\n            currents = next_level\n        loss = self.weight * total\n\n    if log is not None:\n        log(\"loss/ic\", loss)\n\n    return loss\n</code></pre>"},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODECallable","title":"<code>ODECallable</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for ODE right-hand side callables.</p> <p>First-order callables (<code>ODEProperties.order == 1</code>) receive three positional arguments and must match this Protocol exactly::</p> <pre><code>def my_ode(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor: ...\n</code></pre> <p>Higher-order callables (<code>order &gt;= 2</code>) receive a fourth positional argument <code>derivs: list[Tensor]</code>, where <code>derivs[k]</code> is the <code>(k+1)</code>-th derivative of all fields stacked as <code>(n_fields, m, 1)</code>::</p> <pre><code>def my_ode(x: Tensor, y: Tensor, args: ArgsRegistry,\n           derivs: list[Tensor] = []) -&gt; Tensor: ...\n</code></pre> <p>The Protocol is intentionally kept to three arguments so that existing first-order callables remain valid <code>ODECallable</code> implementations. <code>ResidualsConstraint</code> uses <code>_ODECallableN</code> internally to call higher-order functions with the correct signature.</p> Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>class ODECallable(Protocol):\n    \"\"\"\n    Protocol for ODE right-hand side callables.\n\n    **First-order** callables (``ODEProperties.order == 1``) receive three\n    positional arguments and must match this Protocol exactly::\n\n        def my_ode(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor: ...\n\n    **Higher-order** callables (``order &gt;= 2``) receive a fourth positional\n    argument ``derivs: list[Tensor]``, where ``derivs[k]`` is the\n    ``(k+1)``-th derivative of all fields stacked as ``(n_fields, m, 1)``::\n\n        def my_ode(x: Tensor, y: Tensor, args: ArgsRegistry,\n                   derivs: list[Tensor] = []) -&gt; Tensor: ...\n\n    The Protocol is intentionally kept to three arguments so that existing\n    first-order callables remain valid ``ODECallable`` implementations.\n    ``ResidualsConstraint`` uses ``_ODECallableN`` internally to call\n    higher-order functions with the correct signature.\n    \"\"\"\n\n    def __call__(self, x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor: ...\n</code></pre>"},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODECallable.__call__","title":"<code>__call__(x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>def __call__(self, x: Tensor, y: Tensor, args: ArgsRegistry) -&gt; Tensor: ...\n</code></pre>"},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODEHyperparameters","title":"<code>ODEHyperparameters</code>  <code>dataclass</code>","text":"<p>               Bases: <code>PINNHyperparameters</code></p> <p>Hyperparameters for ODE inverse problems.</p> Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>@dataclass(kw_only=True)\nclass ODEHyperparameters(PINNHyperparameters):\n    \"\"\"\n    Hyperparameters for ODE inverse problems.\n    \"\"\"\n\n    pde_weight: float = 1.0\n    ic_weight: float = 1.0\n    data_weight: float = 1.0\n</code></pre>"},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODEHyperparameters.data_weight","title":"<code>data_weight: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODEHyperparameters.ic_weight","title":"<code>ic_weight: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODEHyperparameters.pde_weight","title":"<code>pde_weight: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODEHyperparameters.__init__","title":"<code>__init__(*, lr: float, training_data: IngestionConfig | GenerationConfig, fields_config: MLPConfig, params_config: MLPConfig | ScalarConfig, max_epochs: int | None = None, gradient_clip_val: float | None = None, criterion: Criteria = 'mse', optimizer: AdamConfig | LBFGSConfig | None = None, scheduler: ReduceLROnPlateauConfig | CosineAnnealingConfig | None = None, early_stopping: EarlyStoppingConfig | None = None, smma_stopping: SMMAStoppingConfig | None = None, pde_weight: float = 1.0, ic_weight: float = 1.0, data_weight: float = 1.0) -&gt; None</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODEInverseProblem","title":"<code>ODEInverseProblem</code>","text":"<p>               Bases: <code>Problem</code></p> <p>Generic ODE Inverse Problem. Composes Residuals + IC + Data constraints with MSELoss.</p> Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>class ODEInverseProblem(Problem):\n    \"\"\"\n    Generic ODE Inverse Problem.\n    Composes Residuals + IC + Data constraints with MSELoss.\n    \"\"\"\n\n    def __init__(\n        self,\n        props: ODEProperties,\n        hp: ODEHyperparameters,\n        fields: FieldsRegistry,\n        params: ParamsRegistry,\n        predict_data: PredictDataFn,\n    ) -&gt; None:\n        constraints: list[Constraint] = [\n            ResidualsConstraint(\n                props=props,\n                fields=fields,\n                params=params,\n                weight=hp.pde_weight,\n            ),\n            ICConstraint(\n                props=props,\n                fields=fields,\n                weight=hp.ic_weight,\n            ),\n            DataConstraint(\n                fields=fields,\n                params=params,\n                predict_data=predict_data,\n                weight=hp.data_weight,\n            ),\n        ]\n\n        criterion = build_criterion(hp.criterion)\n\n        super().__init__(\n            constraints=constraints,\n            criterion=criterion,\n            fields=fields,\n            params=params,\n        )\n</code></pre>"},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODEInverseProblem.__init__","title":"<code>__init__(props: ODEProperties, hp: ODEHyperparameters, fields: FieldsRegistry, params: ParamsRegistry, predict_data: PredictDataFn) -&gt; None</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>def __init__(\n    self,\n    props: ODEProperties,\n    hp: ODEHyperparameters,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n    predict_data: PredictDataFn,\n) -&gt; None:\n    constraints: list[Constraint] = [\n        ResidualsConstraint(\n            props=props,\n            fields=fields,\n            params=params,\n            weight=hp.pde_weight,\n        ),\n        ICConstraint(\n            props=props,\n            fields=fields,\n            weight=hp.ic_weight,\n        ),\n        DataConstraint(\n            fields=fields,\n            params=params,\n            predict_data=predict_data,\n            weight=hp.data_weight,\n        ),\n    ]\n\n    criterion = build_criterion(hp.criterion)\n\n    super().__init__(\n        constraints=constraints,\n        criterion=criterion,\n        fields=fields,\n        params=params,\n    )\n</code></pre>"},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODEProperties","title":"<code>ODEProperties</code>  <code>dataclass</code>","text":"<p>Properties defining an Ordinary Differential Equation problem.</p> <p>Attributes:</p> Name Type Description <code>ode</code> <code>ODECallable</code> <p>The ODE function (callable).</p> <code>args</code> <code>ArgsRegistry</code> <p>Arguments/Parameters for the ODE.</p> <code>y0</code> <code>Tensor</code> <p>Initial conditions.</p> <code>order</code> <code>int</code> <p>Order of the ODE (default 1). For order=n, the ODE callable receives derivs as its last argument: derivs[k] is the (k+1)-th derivative.</p> <code>dy0</code> <code>list[Tensor]</code> <p>Initial conditions for lower-order derivatives, length = order-1. dy0[k] is the IC for the (k+1)-th derivative, shape (n_fields,).</p> <code>expected_args</code> <code>frozenset[str] | None</code> <p>Optional set of arg keys the ODE function accesses. When provided, validated against the merged args+params at construction time.</p> Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>@dataclass\nclass ODEProperties:\n    \"\"\"\n    Properties defining an Ordinary Differential Equation problem.\n\n    Attributes:\n        ode: The ODE function (callable).\n        args: Arguments/Parameters for the ODE.\n        y0: Initial conditions.\n        order: Order of the ODE (default 1). For order=n, the ODE callable receives\n            derivs as its last argument: derivs[k] is the (k+1)-th derivative.\n        dy0: Initial conditions for lower-order derivatives, length = order-1.\n            dy0[k] is the IC for the (k+1)-th derivative, shape (n_fields,).\n        expected_args: Optional set of arg keys the ODE function accesses.\n            When provided, validated against the merged args+params at construction time.\n    \"\"\"\n\n    ode: ODECallable\n    args: ArgsRegistry\n    y0: Tensor\n    order: int = 1\n    dy0: list[Tensor] = dc_field(default_factory=list)\n    expected_args: frozenset[str] | None = None\n\n    def __post_init__(self) -&gt; None:\n        if self.order &lt; 1:\n            raise ValueError(f\"order must be &gt;= 1, got {self.order}\")\n        if len(self.dy0) != self.order - 1:\n            raise ValueError(f\"dy0 must have length order-1={self.order - 1}, got {len(self.dy0)}\")\n</code></pre>"},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODEProperties.args","title":"<code>args: ArgsRegistry</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODEProperties.dy0","title":"<code>dy0: list[Tensor] = dc_field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODEProperties.expected_args","title":"<code>expected_args: frozenset[str] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODEProperties.ode","title":"<code>ode: ODECallable</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODEProperties.order","title":"<code>order: int = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODEProperties.y0","title":"<code>y0: Tensor</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODEProperties.__init__","title":"<code>__init__(ode: ODECallable, args: ArgsRegistry, y0: Tensor, order: int = 1, dy0: list[Tensor] = list(), expected_args: frozenset[str] | None = None) -&gt; None</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ODEProperties.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.order &lt; 1:\n        raise ValueError(f\"order must be &gt;= 1, got {self.order}\")\n    if len(self.dy0) != self.order - 1:\n        raise ValueError(f\"dy0 must have length order-1={self.order - 1}, got {len(self.dy0)}\")\n</code></pre>"},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ResidualsConstraint","title":"<code>ResidualsConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Constraint enforcing the ODE residuals. Minimizes \\(\\lVert \\partial y / \\partial t - f(t, y) \\rVert^2\\).</p> <p>Parameters:</p> Name Type Description Default <code>props</code> <code>ODEProperties</code> <p>ODE properties.</p> required <code>fields</code> <code>FieldsRegistry</code> <p>List of fields.</p> required <code>params</code> <code>ParamsRegistry</code> <p>List of parameters.</p> required <code>weight</code> <code>float</code> <p>Weight for this loss term.</p> <code>1.0</code> Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>class ResidualsConstraint(Constraint):\n    \"\"\"\n    Constraint enforcing the ODE residuals.\n    Minimizes $\\\\lVert \\\\partial y / \\\\partial t - f(t, y) \\\\rVert^2$.\n\n    Args:\n        props: ODE properties.\n        fields: List of fields.\n        params: List of parameters.\n        weight: Weight for this loss term.\n    \"\"\"\n\n    def __init__(\n        self,\n        props: ODEProperties,\n        fields: FieldsRegistry,\n        params: ParamsRegistry,\n        weight: float = 1.0,\n    ):\n        if len(fields) != len(props.y0):\n            raise ValueError(\n                f\"Number of fields ({len(fields)}) must match number of initial conditions \"\n                f\"in y0 ({len(props.y0)}). Field keys: {list(fields)}.\"\n            )\n\n        merged_args: dict[str, object] = {**props.args, **params}\n        if props.expected_args is not None:\n            missing = props.expected_args - merged_args.keys()\n            if missing:\n                raise ValueError(\n                    f\"ODE function expects args {sorted(missing)!r} but they are not in \"\n                    f\"props.args or params. Available keys: {sorted(merged_args.keys())!r}.\"\n                )\n\n        self.fields = fields\n        self.weight = weight\n        self.order = props.order\n\n        self.ode = props.ode\n\n        # add the trainable params as args\n        self.args = props.args.copy()\n        self.args.update(params)\n\n    @override\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        _, x_coll = batch\n\n        n_fields = len(self.fields)\n        x_copies = [x_coll.detach().clone().requires_grad_(True) for _ in range(n_fields)]\n        preds = [f(x_copies[i]) for i, f in enumerate(self.fields.values())]\n        y = torch.stack(preds)\n\n        # Build all derivative levels by chaining (each level differentiates the previous)\n        # deriv_levels[k][i] = (k+1)-th derivative of field i\n        deriv_levels: list[list[Tensor]] = []\n        currents = list(preds)\n        for _ in range(self.order):\n            next_level = [diff_grad(currents[i], x_copies[i]) for i in range(n_fields)]\n            deriv_levels.append(next_level)\n            currents = next_level\n\n        # derivs[k] = (k+1)-th derivative stacked across fields, passed to the ODE callable\n        derivs = [torch.stack(deriv_levels[k]) for k in range(self.order - 1)]\n        # The order-th derivative is the LHS to compare against f_out\n        high_deriv = torch.stack(deriv_levels[self.order - 1])\n\n        if self.order == 1:\n            f_out = self.ode(x_coll, y, self.args)\n        else:\n            f_out = cast(_ODECallableN, self.ode)(x_coll, y, self.args, derivs)\n\n        loss: Tensor = self.weight * criterion(high_deriv, f_out)\n\n        if log is not None:\n            log(\"loss/res\", loss)\n\n        return loss\n</code></pre>"},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ResidualsConstraint.args","title":"<code>args = props.args.copy()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ResidualsConstraint.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ResidualsConstraint.ode","title":"<code>ode = props.ode</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ResidualsConstraint.order","title":"<code>order = props.order</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ResidualsConstraint.weight","title":"<code>weight = weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ResidualsConstraint.__init__","title":"<code>__init__(props: ODEProperties, fields: FieldsRegistry, params: ParamsRegistry, weight: float = 1.0)</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>def __init__(\n    self,\n    props: ODEProperties,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n    weight: float = 1.0,\n):\n    if len(fields) != len(props.y0):\n        raise ValueError(\n            f\"Number of fields ({len(fields)}) must match number of initial conditions \"\n            f\"in y0 ({len(props.y0)}). Field keys: {list(fields)}.\"\n        )\n\n    merged_args: dict[str, object] = {**props.args, **params}\n    if props.expected_args is not None:\n        missing = props.expected_args - merged_args.keys()\n        if missing:\n            raise ValueError(\n                f\"ODE function expects args {sorted(missing)!r} but they are not in \"\n                f\"props.args or params. Available keys: {sorted(merged_args.keys())!r}.\"\n            )\n\n    self.fields = fields\n    self.weight = weight\n    self.order = props.order\n\n    self.ode = props.ode\n\n    # add the trainable params as args\n    self.args = props.args.copy()\n    self.args.update(params)\n</code></pre>"},{"location":"reference/anypinn/problems/ode/#anypinn.problems.ode.ResidualsConstraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/problems/ode.py</code> <pre><code>@override\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    _, x_coll = batch\n\n    n_fields = len(self.fields)\n    x_copies = [x_coll.detach().clone().requires_grad_(True) for _ in range(n_fields)]\n    preds = [f(x_copies[i]) for i, f in enumerate(self.fields.values())]\n    y = torch.stack(preds)\n\n    # Build all derivative levels by chaining (each level differentiates the previous)\n    # deriv_levels[k][i] = (k+1)-th derivative of field i\n    deriv_levels: list[list[Tensor]] = []\n    currents = list(preds)\n    for _ in range(self.order):\n        next_level = [diff_grad(currents[i], x_copies[i]) for i in range(n_fields)]\n        deriv_levels.append(next_level)\n        currents = next_level\n\n    # derivs[k] = (k+1)-th derivative stacked across fields, passed to the ODE callable\n    derivs = [torch.stack(deriv_levels[k]) for k in range(self.order - 1)]\n    # The order-th derivative is the LHS to compare against f_out\n    high_deriv = torch.stack(deriv_levels[self.order - 1])\n\n    if self.order == 1:\n        f_out = self.ode(x_coll, y, self.args)\n    else:\n        f_out = cast(_ODECallableN, self.ode)(x_coll, y, self.args, derivs)\n\n    loss: Tensor = self.weight * criterion(high_deriv, f_out)\n\n    if log is not None:\n        log(\"loss/res\", loss)\n\n    return loss\n</code></pre>"},{"location":"reference/anypinn/problems/pde/","title":"pde","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde","title":"<code>anypinn.problems.pde</code>","text":"<p>Boundary condition constraints for PDE problems.</p>"},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.BCValueFn","title":"<code>BCValueFn: TypeAlias = Callable[[Tensor], Tensor]</code>  <code>module-attribute</code>","text":"<p>A callable that maps boundary coordinates (n_pts, d) \u2192 target values (n_pts, out_dim).</p>"},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PDEResidualFn","title":"<code>PDEResidualFn: TypeAlias = Callable[[Tensor, FieldsRegistry, ParamsRegistry], Tensor]</code>  <code>module-attribute</code>","text":"<p>A callable (x, fields, params) \u2192 residual tensor, expected to be zero at the solution.</p>"},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.BoundaryCondition","title":"<code>BoundaryCondition</code>","text":"<p>Pairs a boundary region sampler with a prescribed value function.</p> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Callable[[int], Tensor]</code> <p>Callable <code>(n_pts: int) -&gt; Tensor</code> of shape <code>(n_pts, d)</code>. Called each training step to produce fresh boundary sample points.</p> required <code>value</code> <code>BCValueFn</code> <p>Callable <code>Tensor -&gt; Tensor</code> giving the target value or normal derivative at boundary coordinates.</p> required <code>n_pts</code> <code>int</code> <p>Number of boundary points sampled per training step.</p> <code>100</code> Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>class BoundaryCondition:\n    \"\"\"\n    Pairs a boundary region sampler with a prescribed value function.\n\n    Args:\n        sampler: Callable ``(n_pts: int) -&gt; Tensor`` of shape ``(n_pts, d)``.\n            Called each training step to produce fresh boundary sample points.\n        value: Callable ``Tensor -&gt; Tensor`` giving the target value or normal\n            derivative at boundary coordinates.\n        n_pts: Number of boundary points sampled per training step.\n    \"\"\"\n\n    def __init__(\n        self,\n        sampler: Callable[[int], Tensor],\n        value: BCValueFn,\n        n_pts: int = 100,\n    ):\n        self.sampler = sampler\n        self.value = value\n        self.n_pts = n_pts\n</code></pre>"},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.BoundaryCondition.n_pts","title":"<code>n_pts = n_pts</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.BoundaryCondition.sampler","title":"<code>sampler = sampler</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.BoundaryCondition.value","title":"<code>value = value</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.BoundaryCondition.__init__","title":"<code>__init__(sampler: Callable[[int], Tensor], value: BCValueFn, n_pts: int = 100)</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>def __init__(\n    self,\n    sampler: Callable[[int], Tensor],\n    value: BCValueFn,\n    n_pts: int = 100,\n):\n    self.sampler = sampler\n    self.value = value\n    self.n_pts = n_pts\n</code></pre>"},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.DirichletBCConstraint","title":"<code>DirichletBCConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Enforces the Dirichlet boundary condition: u(x_bc) = g(x_bc). Minimizes <code>weight * criterion(u(x_bc), g(x_bc))</code>.</p> <p>Parameters:</p> Name Type Description Default <code>bc</code> <code>BoundaryCondition</code> <p>Boundary condition (sampler + target value function).</p> required <code>field</code> <code>Field</code> <p>The neural field to enforce the condition on.</p> required <code>log_key</code> <code>str</code> <p>Key used when logging the loss value.</p> <code>'loss/bc_dirichlet'</code> <code>weight</code> <code>float</code> <p>Loss term weight.</p> <code>1.0</code> Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>class DirichletBCConstraint(Constraint):\n    \"\"\"\n    Enforces the Dirichlet boundary condition: u(x_bc) = g(x_bc).\n    Minimizes ``weight * criterion(u(x_bc), g(x_bc))``.\n\n    Args:\n        bc: Boundary condition (sampler + target value function).\n        field: The neural field to enforce the condition on.\n        log_key: Key used when logging the loss value.\n        weight: Loss term weight.\n    \"\"\"\n\n    def __init__(\n        self,\n        bc: BoundaryCondition,\n        field: Field,\n        log_key: str = \"loss/bc_dirichlet\",\n        weight: float = 1.0,\n    ):\n        self.bc = bc\n        self.field = field\n        self.log_key = log_key\n        self.weight = weight\n\n    @override\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        device = next(self.field.parameters()).device\n        x_bc = self.bc.sampler(self.bc.n_pts).to(device)\n        u_pred = self.field(x_bc)\n        g = self.bc.value(x_bc).to(device)\n        loss: Tensor = self.weight * criterion(u_pred, g)\n        if log is not None:\n            log(self.log_key, loss)\n        return loss\n</code></pre>"},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.DirichletBCConstraint.bc","title":"<code>bc = bc</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.DirichletBCConstraint.field","title":"<code>field = field</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.DirichletBCConstraint.log_key","title":"<code>log_key = log_key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.DirichletBCConstraint.weight","title":"<code>weight = weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.DirichletBCConstraint.__init__","title":"<code>__init__(bc: BoundaryCondition, field: Field, log_key: str = 'loss/bc_dirichlet', weight: float = 1.0)</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>def __init__(\n    self,\n    bc: BoundaryCondition,\n    field: Field,\n    log_key: str = \"loss/bc_dirichlet\",\n    weight: float = 1.0,\n):\n    self.bc = bc\n    self.field = field\n    self.log_key = log_key\n    self.weight = weight\n</code></pre>"},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.DirichletBCConstraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>@override\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    device = next(self.field.parameters()).device\n    x_bc = self.bc.sampler(self.bc.n_pts).to(device)\n    u_pred = self.field(x_bc)\n    g = self.bc.value(x_bc).to(device)\n    loss: Tensor = self.weight * criterion(u_pred, g)\n    if log is not None:\n        log(self.log_key, loss)\n    return loss\n</code></pre>"},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.NeumannBCConstraint","title":"<code>NeumannBCConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Enforces the Neumann boundary condition: \\(\\partial u / \\partial n (x_{bc}) = h(x_{bc})\\).</p> <p>For a rectangular domain face whose outward normal is axis-aligned with dimension <code>normal_dim</code>, we have \\(\\partial u / \\partial n = \\partial u / \\partial x_{\\mathrm{normal\\_dim}}\\). Minimizes <code>weight * criterion(du_dn(x_bc), h(x_bc))</code>.</p> <p>Parameters:</p> Name Type Description Default <code>bc</code> <code>BoundaryCondition</code> <p>Boundary condition (sampler + target normal-derivative function).</p> required <code>field</code> <code>Field</code> <p>The neural field to enforce the condition on.</p> required <code>normal_dim</code> <code>int</code> <p>Index of the spatial dimension the boundary normal points along.</p> required <code>log_key</code> <code>str</code> <p>Key used when logging the loss value.</p> <code>'loss/bc_neumann'</code> <code>weight</code> <code>float</code> <p>Loss term weight.</p> <code>1.0</code> Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>class NeumannBCConstraint(Constraint):\n    \"\"\"\n    Enforces the Neumann boundary condition:\n    $\\\\partial u / \\\\partial n (x_{bc}) = h(x_{bc})$.\n\n    For a rectangular domain face whose outward normal is axis-aligned with\n    dimension ``normal_dim``, we have\n    $\\\\partial u / \\\\partial n = \\\\partial u / \\\\partial x_{\\\\mathrm{normal\\\\_dim}}$.\n    Minimizes\n    ``weight * criterion(du_dn(x_bc), h(x_bc))``.\n\n    Args:\n        bc: Boundary condition (sampler + target normal-derivative function).\n        field: The neural field to enforce the condition on.\n        normal_dim: Index of the spatial dimension the boundary normal points along.\n        log_key: Key used when logging the loss value.\n        weight: Loss term weight.\n    \"\"\"\n\n    def __init__(\n        self,\n        bc: BoundaryCondition,\n        field: Field,\n        normal_dim: int,\n        log_key: str = \"loss/bc_neumann\",\n        weight: float = 1.0,\n    ):\n        self.bc = bc\n        self.field = field\n        self.normal_dim = normal_dim\n        self.log_key = log_key\n        self.weight = weight\n\n    @override\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        device = next(self.field.parameters()).device\n        x_bc = self.bc.sampler(self.bc.n_pts).to(device).detach().requires_grad_(True)\n        u_pred = self.field(x_bc)\n        du_dn = diff_partial(u_pred, x_bc, dim=self.normal_dim)\n        h = self.bc.value(x_bc.detach()).to(device)\n        loss: Tensor = self.weight * criterion(du_dn, h)\n        if log is not None:\n            log(self.log_key, loss)\n        return loss\n</code></pre>"},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.NeumannBCConstraint.bc","title":"<code>bc = bc</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.NeumannBCConstraint.field","title":"<code>field = field</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.NeumannBCConstraint.log_key","title":"<code>log_key = log_key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.NeumannBCConstraint.normal_dim","title":"<code>normal_dim = normal_dim</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.NeumannBCConstraint.weight","title":"<code>weight = weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.NeumannBCConstraint.__init__","title":"<code>__init__(bc: BoundaryCondition, field: Field, normal_dim: int, log_key: str = 'loss/bc_neumann', weight: float = 1.0)</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>def __init__(\n    self,\n    bc: BoundaryCondition,\n    field: Field,\n    normal_dim: int,\n    log_key: str = \"loss/bc_neumann\",\n    weight: float = 1.0,\n):\n    self.bc = bc\n    self.field = field\n    self.normal_dim = normal_dim\n    self.log_key = log_key\n    self.weight = weight\n</code></pre>"},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.NeumannBCConstraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>@override\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    device = next(self.field.parameters()).device\n    x_bc = self.bc.sampler(self.bc.n_pts).to(device).detach().requires_grad_(True)\n    u_pred = self.field(x_bc)\n    du_dn = diff_partial(u_pred, x_bc, dim=self.normal_dim)\n    h = self.bc.value(x_bc.detach()).to(device)\n    loss: Tensor = self.weight * criterion(du_dn, h)\n    if log is not None:\n        log(self.log_key, loss)\n    return loss\n</code></pre>"},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PDEResidualConstraint","title":"<code>PDEResidualConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Enforces a PDE interior residual: <code>residual_fn(x, fields, params) \u2248 0</code>. Minimizes <code>weight * criterion(residual_fn(x_coll, fields, params), 0)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>FieldsRegistry</code> <p>Registry of neural fields the residual function operates on. Pass only the subset needed \u2014 other fields in the Problem are ignored.</p> required <code>params</code> <code>ParamsRegistry</code> <p>Registry of parameters the residual function uses.</p> required <code>residual_fn</code> <code>PDEResidualFn</code> <p>Callable (x, fields, params) \u2192 Tensor of residuals. Should use <code>anypinn.lib.diff</code> operators for derivatives. The returned tensor is compared against zeros.</p> required <code>log_key</code> <code>str</code> <p>Key used when logging the loss value.</p> <code>'loss/pde_residual'</code> <code>weight</code> <code>float</code> <p>Loss term weight.</p> <code>1.0</code> Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>class PDEResidualConstraint(Constraint):\n    \"\"\"\n    Enforces a PDE interior residual: ``residual_fn(x, fields, params) \u2248 0``.\n    Minimizes ``weight * criterion(residual_fn(x_coll, fields, params), 0)``.\n\n    Args:\n        fields: Registry of neural fields the residual function operates on.\n            Pass only the subset needed \u2014 other fields in the Problem are ignored.\n        params: Registry of parameters the residual function uses.\n        residual_fn: Callable (x, fields, params) \u2192 Tensor of residuals.\n            Should use ``anypinn.lib.diff`` operators for derivatives.\n            The returned tensor is compared against zeros.\n        log_key: Key used when logging the loss value.\n        weight: Loss term weight.\n    \"\"\"\n\n    def __init__(\n        self,\n        fields: FieldsRegistry,\n        params: ParamsRegistry,\n        residual_fn: PDEResidualFn,\n        log_key: str = \"loss/pde_residual\",\n        weight: float = 1.0,\n    ):\n        self.fields = fields\n        self.params = params\n        self.residual_fn = residual_fn\n        self.log_key = log_key\n        self.weight = weight\n\n    @override\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        _, x_coll = batch\n        x_coll = x_coll.detach().requires_grad_(True)\n        residual = self.residual_fn(x_coll, self.fields, self.params)\n        loss: Tensor = self.weight * criterion(residual, torch.zeros_like(residual))\n        if log is not None:\n            log(self.log_key, loss)\n        return loss\n</code></pre>"},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PDEResidualConstraint.fields","title":"<code>fields = fields</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PDEResidualConstraint.log_key","title":"<code>log_key = log_key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PDEResidualConstraint.params","title":"<code>params = params</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PDEResidualConstraint.residual_fn","title":"<code>residual_fn = residual_fn</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PDEResidualConstraint.weight","title":"<code>weight = weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PDEResidualConstraint.__init__","title":"<code>__init__(fields: FieldsRegistry, params: ParamsRegistry, residual_fn: PDEResidualFn, log_key: str = 'loss/pde_residual', weight: float = 1.0)</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>def __init__(\n    self,\n    fields: FieldsRegistry,\n    params: ParamsRegistry,\n    residual_fn: PDEResidualFn,\n    log_key: str = \"loss/pde_residual\",\n    weight: float = 1.0,\n):\n    self.fields = fields\n    self.params = params\n    self.residual_fn = residual_fn\n    self.log_key = log_key\n    self.weight = weight\n</code></pre>"},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PDEResidualConstraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>@override\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    _, x_coll = batch\n    x_coll = x_coll.detach().requires_grad_(True)\n    residual = self.residual_fn(x_coll, self.fields, self.params)\n    loss: Tensor = self.weight * criterion(residual, torch.zeros_like(residual))\n    if log is not None:\n        log(self.log_key, loss)\n    return loss\n</code></pre>"},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PeriodicBCConstraint","title":"<code>PeriodicBCConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>Enforces periodic boundary conditions: <code>u(x_left, t) = u(x_right, t)</code> and <code>\u2202u/\u2202x(x_left, t) = \u2202u/\u2202x(x_right, t)</code>.</p> <p>The two boundary samplers must produce paired points \u2014 identical coordinates in every dimension except the periodic one \u2014 so that the value- and derivative-matching losses are meaningful.</p> <p>Minimizes <code>weight * [criterion(u_left, u_right) + criterion(du_left, du_right)]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>bc_left</code> <code>BoundaryCondition</code> <p>Left boundary sampler (sampler + dummy value function).</p> required <code>bc_right</code> <code>BoundaryCondition</code> <p>Right boundary sampler (sampler + dummy value function).</p> required <code>field</code> <code>Field</code> <p>The neural field to enforce the condition on.</p> required <code>match_dim</code> <code>int</code> <p>Spatial dimension index for the derivative matching.</p> <code>0</code> <code>log_key</code> <code>str</code> <p>Key used when logging the loss value.</p> <code>'loss/bc_periodic'</code> <code>weight</code> <code>float</code> <p>Loss term weight.</p> <code>1.0</code> Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>class PeriodicBCConstraint(Constraint):\n    \"\"\"\n    Enforces periodic boundary conditions:\n    ``u(x_left, t) = u(x_right, t)`` and\n    ``\u2202u/\u2202x(x_left, t) = \u2202u/\u2202x(x_right, t)``.\n\n    The two boundary samplers must produce **paired** points \u2014 identical\n    coordinates in every dimension except the periodic one \u2014 so that\n    the value- and derivative-matching losses are meaningful.\n\n    Minimizes\n    ``weight * [criterion(u_left, u_right) + criterion(du_left, du_right)]``.\n\n    Args:\n        bc_left: Left boundary sampler (sampler + dummy value function).\n        bc_right: Right boundary sampler (sampler + dummy value function).\n        field: The neural field to enforce the condition on.\n        match_dim: Spatial dimension index for the derivative matching.\n        log_key: Key used when logging the loss value.\n        weight: Loss term weight.\n    \"\"\"\n\n    def __init__(\n        self,\n        bc_left: BoundaryCondition,\n        bc_right: BoundaryCondition,\n        field: Field,\n        match_dim: int = 0,\n        log_key: str = \"loss/bc_periodic\",\n        weight: float = 1.0,\n    ):\n        self.bc_left = bc_left\n        self.bc_right = bc_right\n        self.field = field\n        self.match_dim = match_dim\n        self.log_key = log_key\n        self.weight = weight\n\n    @override\n    def loss(\n        self,\n        batch: TrainingBatch,\n        criterion: nn.Module,\n        log: LogFn | None = None,\n    ) -&gt; Tensor:\n        device = next(self.field.parameters()).device\n        n_pts = self.bc_left.n_pts\n\n        x_left = self.bc_left.sampler(n_pts).to(device).detach().requires_grad_(True)\n        x_right = self.bc_right.sampler(n_pts).to(device).detach().requires_grad_(True)\n\n        u_left = self.field(x_left)\n        u_right = self.field(x_right)\n\n        # Value matching: u(x_left, t) = u(x_right, t)\n        loss_val: Tensor = criterion(u_left, u_right)\n\n        # Derivative matching: du/dx(x_left, t) = du/dx(x_right, t)\n        du_left = diff_partial(u_left, x_left, dim=self.match_dim)\n        du_right = diff_partial(u_right, x_right, dim=self.match_dim)\n        loss_deriv: Tensor = criterion(du_left, du_right)\n\n        loss: Tensor = self.weight * (loss_val + loss_deriv)\n        if log is not None:\n            log(self.log_key, loss)\n        return loss\n</code></pre>"},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PeriodicBCConstraint.bc_left","title":"<code>bc_left = bc_left</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PeriodicBCConstraint.bc_right","title":"<code>bc_right = bc_right</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PeriodicBCConstraint.field","title":"<code>field = field</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PeriodicBCConstraint.log_key","title":"<code>log_key = log_key</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PeriodicBCConstraint.match_dim","title":"<code>match_dim = match_dim</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PeriodicBCConstraint.weight","title":"<code>weight = weight</code>  <code>instance-attribute</code>","text":""},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PeriodicBCConstraint.__init__","title":"<code>__init__(bc_left: BoundaryCondition, bc_right: BoundaryCondition, field: Field, match_dim: int = 0, log_key: str = 'loss/bc_periodic', weight: float = 1.0)</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>def __init__(\n    self,\n    bc_left: BoundaryCondition,\n    bc_right: BoundaryCondition,\n    field: Field,\n    match_dim: int = 0,\n    log_key: str = \"loss/bc_periodic\",\n    weight: float = 1.0,\n):\n    self.bc_left = bc_left\n    self.bc_right = bc_right\n    self.field = field\n    self.match_dim = match_dim\n    self.log_key = log_key\n    self.weight = weight\n</code></pre>"},{"location":"reference/anypinn/problems/pde/#anypinn.problems.pde.PeriodicBCConstraint.loss","title":"<code>loss(batch: TrainingBatch, criterion: nn.Module, log: LogFn | None = None) -&gt; Tensor</code>","text":"Source code in <code>src/anypinn/problems/pde.py</code> <pre><code>@override\ndef loss(\n    self,\n    batch: TrainingBatch,\n    criterion: nn.Module,\n    log: LogFn | None = None,\n) -&gt; Tensor:\n    device = next(self.field.parameters()).device\n    n_pts = self.bc_left.n_pts\n\n    x_left = self.bc_left.sampler(n_pts).to(device).detach().requires_grad_(True)\n    x_right = self.bc_right.sampler(n_pts).to(device).detach().requires_grad_(True)\n\n    u_left = self.field(x_left)\n    u_right = self.field(x_right)\n\n    # Value matching: u(x_left, t) = u(x_right, t)\n    loss_val: Tensor = criterion(u_left, u_right)\n\n    # Derivative matching: du/dx(x_left, t) = du/dx(x_right, t)\n    du_left = diff_partial(u_left, x_left, dim=self.match_dim)\n    du_right = diff_partial(u_right, x_right, dim=self.match_dim)\n    loss_deriv: Tensor = criterion(du_left, du_right)\n\n    loss: Tensor = self.weight * (loss_val + loss_deriv)\n    if log is not None:\n        log(self.log_key, loss)\n    return loss\n</code></pre>"}]}